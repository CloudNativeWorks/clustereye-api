package server

import (
	"bytes"
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"math"
	"net/http"
	"reflect"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/CloudNativeWorks/clustereye-api/internal/database"
	"github.com/CloudNativeWorks/clustereye-api/internal/logger"
	"github.com/CloudNativeWorks/clustereye-api/internal/metrics"
	pb "github.com/CloudNativeWorks/clustereye-api/pkg/agent"
	influxdb2 "github.com/influxdata/influxdb-client-go/v2"
	"github.com/influxdata/influxdb-client-go/v2/api/write"

	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/metadata"
	"google.golang.org/grpc/status"
	"google.golang.org/protobuf/types/known/anypb"
	"google.golang.org/protobuf/types/known/structpb"
	"google.golang.org/protobuf/types/known/timestamppb"
)

// PostgresInfo, PostgreSQL test sonucunu ve bilgilerini temsil eder
type PostgresInfo struct {
	Status   string `json:"status"`
	User     string `json:"user"`
	Password string `json:"password"`
	Cluster  string `json:"cluster"`
}

// AgentConnection, bir agent ile olan baÄŸlantÄ±yÄ± temsil eder
type AgentConnection struct {
	Stream pb.AgentService_ConnectServer
	Info   *pb.AgentInfo
}

// QueryResponse, bir sorgu sonucunu ve sonuÃ§ kanalÄ±nÄ± iÃ§erir
type QueryResponse struct {
	Result     string
	ResultChan chan *pb.QueryResult
}

// Server, ClusterEye sunucusunu temsil eder
type Server struct {
	pb.UnimplementedAgentServiceServer
	mu          sync.RWMutex
	agents      map[string]*AgentConnection
	queryMu     sync.RWMutex
	queryResult map[string]*QueryResponse // query_id -> QueryResponse
	db          *sql.DB                   // PostgreSQL veritabanÄ± baÄŸlantÄ±sÄ±
	companyRepo *database.CompanyRepository
	// Son ping zamanlarÄ±nÄ± tutmak iÃ§in map
	lastPingMu   sync.RWMutex
	lastPingTime map[string]time.Time
	// Job yÃ¶netimi iÃ§in yeni alanlar
	jobMu sync.RWMutex
	jobs  map[string]*pb.Job
	// InfluxDB writer
	influxWriter *metrics.InfluxDBWriter
	// Coordination duplicate prevention
	coordinationMu         sync.RWMutex
	processedCoordinations map[string]time.Time // key: process_id + old_master + new_master, value: timestamp
}

// NewServer, yeni bir sunucu nesnesi oluÅŸturur
func NewServer(db *sql.DB, influxWriter *metrics.InfluxDBWriter) *Server {
	return &Server{
		agents:                 make(map[string]*AgentConnection),
		queryResult:            make(map[string]*QueryResponse),
		db:                     db,
		companyRepo:            database.NewCompanyRepository(db),
		lastPingTime:           make(map[string]time.Time),
		jobs:                   make(map[string]*pb.Job),
		influxWriter:           influxWriter,
		processedCoordinations: make(map[string]time.Time), // Coordination tracking
	}
}

// Connect, agent'larÄ±n baÄŸlanmasÄ± iÃ§in stream aÃ§ar
func (s *Server) Connect(stream pb.AgentService_ConnectServer) error {
	var currentAgentID string
	var companyID int

	logger.Info().Msg("Yeni agent baÄŸlantÄ± isteÄŸi alÄ±ndÄ±")

	for {
		in, err := stream.Recv()
		if err != nil {
			logger.Error().Err(err).Str("agent_id", currentAgentID).Msg("Agent baÄŸlantÄ±sÄ± kapandÄ±")

			// Agent'Ä± silmek yerine, sadece Stream'i nil yap ve baÄŸlantÄ± durumunu gÃ¼ncelle
			if currentAgentID != "" {
				s.mu.Lock()
				if conn, exists := s.agents[currentAgentID]; exists && conn != nil {
					// Stream'i nil yap ama agent bilgilerini sakla
					conn.Stream = nil
				}
				s.mu.Unlock()

				// Son ping zamanÄ±nÄ± sÄ±fÄ±rla ki "disconnected" olarak gÃ¶rÃ¼nsÃ¼n
				s.lastPingMu.Lock()
				delete(s.lastPingTime, currentAgentID)
				s.lastPingMu.Unlock()

				// Agent'Ä±n veritabanÄ±ndaki durumunu gÃ¼ncelle
				s.UpdateAgentDisconnectedStatus(currentAgentID)

				logger.Info().Str("agent_id", currentAgentID).Msg("Agent baÄŸlantÄ±sÄ± kapatÄ±ldÄ±, disconnected olarak iÅŸaretlendi")
			}

			return err
		}

		switch payload := in.Payload.(type) {
		case *pb.AgentMessage_AgentInfo:
			agentInfo := payload.AgentInfo
			logger.Info().
				Str("agent_id", agentInfo.AgentId).
				Str("hostname", agentInfo.Hostname).
				Msg("Agent bilgileri alÄ±ndÄ±")

			// Agent anahtarÄ±nÄ± doÄŸrula
			company, err := s.companyRepo.ValidateAgentKey(context.Background(), agentInfo.Key)
			if err != nil {
				logger.Error().Err(err).Str("agent_id", agentInfo.AgentId).Msg("Agent kimlik doÄŸrulama hatasÄ±")
				return err
			}

			// Agent ID'yi belirle
			currentAgentID = agentInfo.AgentId
			companyID = company.ID

			// Agent'Ä± kaydet
			err = s.companyRepo.RegisterAgent(
				context.Background(),
				companyID,
				currentAgentID,
				agentInfo.Hostname,
				agentInfo.Ip,
			)

			if err != nil {
				logger.Error().Err(err).Str("agent_id", currentAgentID).Msg("Agent kaydedilemedi")
				return err
			}

			// Agent'Ä± baÄŸlantÄ± listesine ekle
			s.mu.Lock()
			s.agents[currentAgentID] = &AgentConnection{
				Stream: stream,
				Info:   agentInfo,
			}
			s.mu.Unlock()

			// Ä°lk baÄŸlantÄ±da ping zamanÄ±nÄ± baÅŸlat
			s.lastPingMu.Lock()
			s.lastPingTime[currentAgentID] = time.Now()
			s.lastPingMu.Unlock()

			logger.Info().
				Str("agent_id", currentAgentID).
				Int("total_connections", len(s.agents)).
				Msg("Agent baÄŸlantÄ± listesine eklendi")

			// BaÅŸarÄ±lÄ± kayÄ±t mesajÄ± gÃ¶nder
			stream.Send(&pb.ServerMessage{
				Payload: &pb.ServerMessage_Registration{
					Registration: &pb.RegistrationResult{
						Status:  "success",
						Message: "Agent baÅŸarÄ±yla kaydedildi",
					},
				},
			})

			logger.Info().
				Str("agent_id", currentAgentID).
				Str("company", company.CompanyName).
				Msg("Agent baÅŸarÄ±yla kaydedildi ve baÄŸlandÄ±")

		case *pb.AgentMessage_QueryResult:
			queryResult := payload.QueryResult
			logger.Debug().Str("agent_id", currentAgentID).Msg("Agent sorguya cevap verdi")

			// Ping yanÄ±tlarÄ±nÄ± kontrol et
			if strings.HasPrefix(queryResult.QueryId, "ping_") {
				// Ping yanÄ±tÄ± alÄ±ndÄ±, son ping zamanÄ±nÄ± gÃ¼ncelle
				s.lastPingMu.Lock()
				s.lastPingTime[currentAgentID] = time.Now()
				s.lastPingMu.Unlock()

				// VeritabanÄ±ndaki agent durumunu ve last_seen'i gÃ¼ncelle
				s.UpdateAgentConnectedStatus(currentAgentID)

				logger.Debug().Str("agent_id", currentAgentID).Msg("Ping yanÄ±tÄ± alÄ±ndÄ±, baÄŸlantÄ± durumu gÃ¼ncellendi")
				continue
			}

			// Sorgu sonucunu ilgili kanal Ã¼zerinden ilet
			s.queryMu.RLock()
			queryResp, ok := s.queryResult[queryResult.QueryId]
			s.queryMu.RUnlock()

			if ok && queryResp.ResultChan != nil {
				queryResp.ResultChan <- queryResult
			}
		}
	}
}

// Register, agent'Ä±n kaydÄ± iÃ§in kullanÄ±lan gRPC metodudur
func (s *Server) Register(ctx context.Context, req *pb.RegisterRequest) (*pb.RegisterResponse, error) {
	logger.Info().Msg("Register metodu Ã§aÄŸrÄ±ldÄ±")
	agentInfo := req.AgentInfo

	// Agent anahtarÄ±nÄ± doÄŸrula
	company, err := s.companyRepo.ValidateAgentKey(ctx, agentInfo.Key)
	if err != nil {
		logger.Error().Err(err).Str("agent_id", agentInfo.AgentId).Msg("Agent kimlik doÄŸrulama hatasÄ±")
		return &pb.RegisterResponse{
			Registration: &pb.RegistrationResult{
				Status:  "error",
				Message: "GeÃ§ersiz agent anahtarÄ±",
			},
		}, nil
	}

	// Agent'Ä± kaydet
	err = s.companyRepo.RegisterAgent(
		ctx,
		company.ID,
		agentInfo.AgentId,
		agentInfo.Hostname,
		agentInfo.Ip,
	)

	if err != nil {
		logger.Error().Err(err).Str("agent_id", agentInfo.AgentId).Msg("Agent kaydedilemedi")
		return &pb.RegisterResponse{
			Registration: &pb.RegistrationResult{
				Status:  "error",
				Message: "Agent kaydedilemedi",
			},
		}, nil
	}

	// PostgreSQL baÄŸlantÄ± bilgilerini kaydet
	logger.Info().
		Str("hostname", agentInfo.Hostname).
		Str("cluster", agentInfo.Platform).
		Str("postgres_user", agentInfo.PostgresUser).
		Msg("PostgreSQL bilgileri kaydediliyor")

	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		logger.Error().Err(err).Msg("VeritabanÄ± baÄŸlantÄ± hatasÄ±")
	}

	// PostgreSQL bilgilerini kaydet
	err = s.companyRepo.SavePostgresConnInfo(
		ctx,
		agentInfo.Hostname,
		agentInfo.Platform,     // Platform alanÄ±nÄ± cluster adÄ± olarak kullanÄ±yoruz
		agentInfo.PostgresUser, // Agent'dan gelen kullanÄ±cÄ± adÄ±
		agentInfo.PostgresPass, // Agent'dan gelen ÅŸifre
	)

	if err != nil {
		logger.Error().Err(err).Str("hostname", agentInfo.Hostname).Msg("PostgreSQL baÄŸlantÄ± bilgileri kaydedilemedi")
	} else {
		logger.Info().Str("hostname", agentInfo.Hostname).Msg("PostgreSQL baÄŸlantÄ± bilgileri kaydedildi")
	}

	logger.Info().
		Str("agent_id", agentInfo.AgentId).
		Str("hostname", agentInfo.Hostname).
		Str("company", company.CompanyName).
		Msg("Yeni Agent baÄŸlandÄ± ve kaydedildi")

	return &pb.RegisterResponse{
		Registration: &pb.RegistrationResult{
			Status:  "success",
			Message: "Agent baÅŸarÄ±yla kaydedildi",
		},
	}, nil
}

func (s *Server) ExecuteQuery(ctx context.Context, req *pb.QueryRequest) (*pb.QueryResponse, error) {
	// Sorgu iÅŸleme mantÄ±ÄŸÄ±
	// ...
	return &pb.QueryResponse{
		Result: &pb.QueryResult{
			QueryId: req.Query.QueryId,
			// SonuÃ§ verilerini doldurun
		},
	}, nil
}

func (s *Server) SendPostgresInfo(ctx context.Context, req *pb.PostgresInfoRequest) (*pb.PostgresInfoResponse, error) {
	logger.Info().Msg("SendPostgresInfo metodu Ã§aÄŸrÄ±ldÄ±")

	// Gelen PostgreSQL bilgilerini logla
	pgInfo := req.PostgresInfo
	
	// Agent ID'sini metrik verilerinden Ã§Ä±kar ve agent durumunu active yap
	agentID := fmt.Sprintf("agent_%s", pgInfo.Hostname)
	s.UpdateAgentConnectedStatus(agentID)
	logger.Debug().
		Str("cluster", pgInfo.ClusterName).
		Str("hostname", pgInfo.Hostname).
		Str("ip", pgInfo.Ip).
		Msg("PostgreSQL bilgileri alÄ±ndÄ±")

	// Daha detaylÄ± loglama
	logger.Debug().
		Str("cluster", pgInfo.ClusterName).
		Str("ip", pgInfo.Ip).
		Str("hostname", pgInfo.Hostname).
		Str("node_status", pgInfo.NodeStatus).
		Str("pg_version", pgInfo.PgVersion).
		Str("location", pgInfo.Location).
		Str("pgbouncer_status", pgInfo.PgBouncerStatus).
		Str("pg_service_status", pgInfo.PgServiceStatus).
		Int64("replication_lag_sec", pgInfo.ReplicationLagSec).
		Str("free_disk", pgInfo.FreeDisk).
		Int64("fd_percent", int64(pgInfo.FdPercent)).
		Str("config_path", pgInfo.ConfigPath).
		Str("data_path", pgInfo.DataPath).
		Msg("PostgreSQL detay bilgileri")

	// VeritabanÄ±na kaydetme iÅŸlemi
	// Bu kÄ±smÄ± ihtiyacÄ±nÄ±za gÃ¶re geliÅŸtirebilirsiniz
	err := s.savePostgresInfoToDatabase(ctx, pgInfo)
	if err != nil {
		logger.Error().Err(err).Str("cluster", pgInfo.ClusterName).Msg("PostgreSQL bilgileri veritabanÄ±na kaydedilemedi")
		return &pb.PostgresInfoResponse{
			Status: "error",
		}, nil
	}

	logger.Info().Str("cluster", pgInfo.ClusterName).Msg("PostgreSQL bilgileri baÅŸarÄ±yla iÅŸlendi ve kaydedildi")

	return &pb.PostgresInfoResponse{
		Status: "success",
	}, nil
}

// PostgreSQL bilgilerini veritabanÄ±na kaydetmek iÃ§in yardÄ±mcÄ± fonksiyon
func (s *Server) savePostgresInfoToDatabase(ctx context.Context, pgInfo *pb.PostgresInfo) error {
	// Ã–nce mevcut kaydÄ± kontrol et
	var existingData []byte
	var id int

	checkQuery := `
		SELECT id, jsondata FROM public.postgres_data 
		WHERE clustername = $1 
		ORDER BY id DESC LIMIT 1
	`

	err := s.db.QueryRowContext(ctx, checkQuery, pgInfo.ClusterName).Scan(&id, &existingData)

	// Yeni node verisi
	pgData := map[string]interface{}{
		"ClusterName":       pgInfo.ClusterName,
		"Location":          pgInfo.Location,
		"FDPercent":         pgInfo.FdPercent,
		"FreeDisk":          pgInfo.FreeDisk,
		"Hostname":          pgInfo.Hostname,
		"IP":                pgInfo.Ip,
		"NodeStatus":        pgInfo.NodeStatus,
		"PGBouncerStatus":   pgInfo.PgBouncerStatus,
		"PGServiceStatus":   pgInfo.PgServiceStatus,
		"PGVersion":         pgInfo.PgVersion,
		"ReplicationLagSec": pgInfo.ReplicationLagSec,
		"TotalVCPU":         pgInfo.TotalVcpu,
		"TotalMemory":       pgInfo.TotalMemory,
		"ConfigPath":        pgInfo.ConfigPath,
		"DataPath":          pgInfo.DataPath,
		// Patroni fields
		"PatroniEnabled":   pgInfo.PatroniEnabled,
		"PatroniCluster":   pgInfo.PatroniCluster,
		"PatroniRole":      pgInfo.PatroniRole,
		"PatroniState":     pgInfo.PatroniState,
		"PatroniRestAPI":   pgInfo.PatroniRestApi,
		"PatroniDetection": pgInfo.PatroniDetection,
	}

	var jsonData []byte

	// Hata kontrolÃ¼nÃ¼ dÃ¼zgÃ¼n yap
	if err == nil {
		// Mevcut kayÄ±t var, gÃ¼ncelle
		logger.Debug().
			Str("cluster", pgInfo.ClusterName).
			Int("id", id).
			Msg("PostgreSQL cluster iÃ§in mevcut kayÄ±t bulundu, gÃ¼ncelleniyor")

		var existingJSON map[string][]interface{}
		if err := json.Unmarshal(existingData, &existingJSON); err != nil {
			logger.Error().Err(err).Str("cluster", pgInfo.ClusterName).Msg("Mevcut JSON ayrÄ±ÅŸtÄ±rma hatasÄ±")
			return err
		}

		// Cluster array'ini al
		clusterData, ok := existingJSON[pgInfo.ClusterName]
		if !ok {
			// EÄŸer cluster verisi yoksa yeni oluÅŸtur
			clusterData = []interface{}{}
		}

		// Node'u bul ve gÃ¼ncelle
		nodeFound := false
		nodeChanged := false
		for i, node := range clusterData {
			nodeMap, ok := node.(map[string]interface{})
			if !ok {
				continue
			}

			// Hostname ve IP ile node eÅŸleÅŸmesi kontrol et
			if nodeMap["Hostname"] == pgInfo.Hostname && nodeMap["IP"] == pgInfo.Ip {
				// Sadece deÄŸiÅŸen alanlarÄ± gÃ¼ncelle
				nodeFound = true

				// DeÄŸiÅŸiklikleri takip et
				for key, newValue := range pgData {
					currentValue, exists := nodeMap[key]
					var hasChanged bool

					if !exists {
						// DeÄŸer mevcut deÄŸil, yeni alan ekleniyor
						hasChanged = true
						logger.Debug().
							Str("hostname", pgInfo.Hostname).
							Str("field", key).
							Msg("PostgreSQL node'da yeni alan eklendi")
					} else {
						// Mevcut deÄŸer ile yeni deÄŸeri karÅŸÄ±laÅŸtÄ±r
						// Numeric deÄŸerler iÃ§in Ã¶zel karÅŸÄ±laÅŸtÄ±rma yap
						hasChanged = !compareValues(currentValue, newValue)
						if hasChanged {
							logger.Debug().
								Str("hostname", pgInfo.Hostname).
								Str("field", key).
								Interface("old_value", currentValue).
								Interface("new_value", newValue).
								Msg("PostgreSQL node'da deÄŸiÅŸiklik tespit edildi")
						}
					}

					if hasChanged {
						nodeMap[key] = newValue
						// NodeStatus, PGServiceStatus, FreeDisk gibi Ã¶nemli alanlar deÄŸiÅŸtiyse iÅŸaretle
						if key == "NodeStatus" || key == "PGServiceStatus" || key == "FreeDisk" || key == "ReplicationLagSec" || key == "PGBouncerStatus" {
							nodeChanged = true
						}
					}
				}

				clusterData[i] = nodeMap
				break
			}
		}

		// EÄŸer node bulunamadÄ±ysa yeni ekle
		if !nodeFound {
			clusterData = append(clusterData, pgData)
			nodeChanged = true
			logger.Info().Str("hostname", pgInfo.Hostname).Msg("Yeni PostgreSQL node eklendi")
		}

		// EÄŸer Ã¶nemli bir deÄŸiÅŸiklik yoksa veritabanÄ±nÄ± gÃ¼ncelleme
		if !nodeChanged {
			logger.Debug().Str("hostname", pgInfo.Hostname).Msg("PostgreSQL node'da Ã¶nemli bir deÄŸiÅŸiklik yok, gÃ¼ncelleme yapÄ±lmadÄ±")
			return nil
		}

		existingJSON[pgInfo.ClusterName] = clusterData

		// JSON'Ä± gÃ¼ncelle
		jsonData, err = json.Marshal(existingJSON)
		if err != nil {
			logger.Error().Err(err).Str("cluster", pgInfo.ClusterName).Msg("JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
			return err
		}

		// VeritabanÄ±nÄ± gÃ¼ncelle
		updateQuery := `
			UPDATE public.postgres_data 
			SET jsondata = $1, updated_at = CURRENT_TIMESTAMP
			WHERE id = $2
		`

		_, err = s.db.ExecContext(ctx, updateQuery, jsonData, id)
		if err != nil {
			logger.Error().Err(err).Str("cluster", pgInfo.ClusterName).Msg("VeritabanÄ± gÃ¼ncelleme hatasÄ±")
			return err
		}

		logger.Info().Str("cluster", pgInfo.ClusterName).Msg("PostgreSQL node bilgileri baÅŸarÄ±yla gÃ¼ncellendi (Ã¶nemli deÄŸiÅŸiklik nedeniyle)")
	} else if err == sql.ErrNoRows {
		// KayÄ±t bulunamadÄ±, yeni kayÄ±t oluÅŸtur
		logger.Info().Str("cluster", pgInfo.ClusterName).Msg("PostgreSQL cluster iÃ§in kayÄ±t bulunamadÄ±, yeni kayÄ±t oluÅŸturuluyor")

		outerJSON := map[string][]interface{}{
			pgInfo.ClusterName: {pgData},
		}

		jsonData, err = json.Marshal(outerJSON)
		if err != nil {
			logger.Error().Err(err).Str("cluster", pgInfo.ClusterName).Msg("JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
			return err
		}

		insertQuery := `
			INSERT INTO public.postgres_data (
				jsondata, clustername, created_at, updated_at
			) VALUES ($1, $2, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
			ON CONFLICT (clustername) DO UPDATE SET
				jsondata = EXCLUDED.jsondata,
				updated_at = CURRENT_TIMESTAMP
		`

		_, err = s.db.ExecContext(ctx, insertQuery, jsonData, pgInfo.ClusterName)
		if err != nil {
			logger.Error().Err(err).Str("cluster", pgInfo.ClusterName).Msg("VeritabanÄ± ekleme hatasÄ±")
			return err
		}

		logger.Info().Str("cluster", pgInfo.ClusterName).Msg("PostgreSQL node bilgileri baÅŸarÄ±yla veritabanÄ±na kaydedildi (yeni kayÄ±t)")
	} else {
		// BaÅŸka bir veritabanÄ± hatasÄ± oluÅŸtu
		logger.Error().Err(err).Str("cluster", pgInfo.ClusterName).Msg("PostgreSQL cluster kayÄ±t kontrolÃ¼ sÄ±rasÄ±nda hata")
		return fmt.Errorf("veritabanÄ± kontrol hatasÄ±: %v", err)
	}

	return nil
}

func (s *Server) StreamQueries(stream pb.AgentService_StreamQueriesServer) error {
	// SÃ¼rekli sorgu akÄ±ÅŸÄ± mantÄ±ÄŸÄ±
	// ...
	return nil
}

func (s *Server) StreamPostgresInfo(stream pb.AgentService_StreamPostgresInfoServer) error {
	// SÃ¼rekli PostgreSQL bilgi akÄ±ÅŸÄ± mantÄ±ÄŸÄ±
	// ...
	return nil
}

// GetStatusPostgres, PostgreSQL veritabanÄ±ndan durum bilgilerini Ã§eker
func (s *Server) GetStatusPostgres(ctx context.Context, _ *structpb.Struct) (*structpb.Value, error) {
	rows, err := s.db.QueryContext(ctx, "SELECT json_agg(sub.jsondata) FROM (SELECT jsondata FROM postgres_data ORDER BY id) AS sub")
	if err != nil {
		return nil, status.Errorf(codes.Internal, "VeritabanÄ± sorgusu baÅŸarÄ±sÄ±z: %v", err)
	}
	defer rows.Close()

	var jsonData []byte
	if rows.Next() {
		err := rows.Scan(&jsonData)
		if err != nil {
			return nil, status.Errorf(codes.Internal, "Veri okuma hatasÄ±: %v", err)
		}
	}

	// JSON verisini structpb.Value'ya dÃ¶nÃ¼ÅŸtÃ¼r
	var jsonValue interface{}
	if err := json.Unmarshal(jsonData, &jsonValue); err != nil {
		return nil, status.Errorf(codes.Internal, "JSON ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
	}

	value, err := structpb.NewValue(jsonValue)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "Veri dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
	}

	return value, nil
}

// GetConnectedAgents, aktif gRPC baÄŸlantÄ±larÄ±ndaki agent'larÄ± dÃ¶ndÃ¼rÃ¼r
func (s *Server) GetConnectedAgents() []map[string]interface{} {
	s.mu.RLock()
	defer s.mu.RUnlock()

	logger.Debug().Int("active_connections", len(s.agents)).Msg("Aktif gRPC baÄŸlantÄ±larÄ±")

	// Istanbul zaman dilimini al
	loc, err := time.LoadLocation("Europe/Istanbul")
	if err != nil {
		logger.Warn().Err(err).Msg("Zaman dilimi yÃ¼klenemedi")
		loc = time.UTC
	}

	agents := make([]map[string]interface{}, 0)
	for id, conn := range s.agents {
		if conn == nil || conn.Info == nil {
			logger.Warn().Str("agent_id", id).Msg("GeÃ§ersiz agent baÄŸlantÄ±sÄ±")
			continue
		}

		// gRPC stream'in durumunu kontrol et
		status := "disconnected"
		// Son ping zamanÄ±nÄ± al
		s.lastPingMu.RLock()
		lastPing, exists := s.lastPingTime[id]
		s.lastPingMu.RUnlock()

		// VarsayÄ±lan olarak son gÃ¶rÃ¼lme zamanÄ±nÄ± son ping zamanÄ± olarak ayarla
		lastSeenTime := lastPing

		if conn.Stream != nil {
			shouldPing := false

			// Ping frekansÄ±nÄ± azalt - ENHANCE_YOUR_CALM hatasÄ±nÄ± Ã¶nlemek iÃ§in
			if !exists || time.Since(lastPing) > 60*time.Second {
				shouldPing = true
			} else {
				// Son 60 saniye iÃ§inde baÅŸarÄ±lÄ± ping varsa, baÄŸlÄ± kabul et
				status = "connected"
			}

			if shouldPing {
				// Ping mesajÄ± gÃ¶nder
				pingMsg := &pb.ServerMessage{
					Payload: &pb.ServerMessage_Query{
						Query: &pb.Query{
							QueryId: fmt.Sprintf("ping_%d", time.Now().UnixNano()),
							Command: "ping",
						},
					},
				}

				// Ping'i sadece gRPC stream Ã¼zerinden gÃ¶nder
				err := conn.Stream.Send(pingMsg)
				if err == nil {
					// Ping gÃ¶nderimi baÅŸarÄ±lÄ± olduÄŸunda da baÄŸlantÄ±yÄ± "connected" olarak iÅŸaretle
					// Agent yanÄ±t verdiÄŸinde Connect metodu lastPingTime'Ä± gÃ¼ncelleyecek
					status = "connected"
					// BaÅŸarÄ±lÄ± ping zamanÄ±nÄ± kaydet
					s.lastPingMu.Lock()
					s.lastPingTime[id] = time.Now()
					lastSeenTime = s.lastPingTime[id] // Son gÃ¶rÃ¼lme zamanÄ±nÄ± gÃ¼ncelle
					s.lastPingMu.Unlock()

					// VeritabanÄ±ndaki agent durumunu ve last_seen'i gÃ¼ncelle
					s.UpdateAgentConnectedStatus(id)

					logger.Debug().Str("agent_id", id).Msg("Ping gÃ¶nderimi baÅŸarÄ±lÄ±, baÄŸlantÄ± durumu gÃ¼ncellendi")
				} else {
					logger.Warn().Err(err).Str("agent_id", id).Msg("Agent ping hatasÄ±")
					// Stream'i nil yap ama agent'Ä± silme
					conn.Stream = nil
					// Son ping zamanÄ±nÄ± da sil
					s.lastPingMu.Lock()
					delete(s.lastPingTime, id)
					s.lastPingMu.Unlock()
					// Status zaten "disconnected" olarak ayarlandÄ±
				}
			}
		}

		// last_seen iÃ§in doÄŸru zaman bilgisini kullan
		var lastSeenStr string
		if exists {
			// EÄŸer son ping zamanÄ± varsa, onu kullan
			lastSeenStr = lastSeenTime.In(loc).Format("2006-01-02T15:04:05-07:00")
		} else {
			// EÄŸer hiÃ§ ping alÄ±nmadÄ±ysa, ÅŸu anki zamanÄ± kullan
			// Bu sadece yeni eklenen ve henÃ¼z ping almamÄ±ÅŸ agentlar iÃ§in geÃ§erli olmalÄ±
			lastSeenStr = time.Now().In(loc).Format("2006-01-02T15:04:05-07:00")
		}

		// Agent'Ä±n veritabanÄ±ndaki durumunu kontrol et
		var dbStatus string
		err := s.db.QueryRow("SELECT status FROM agents WHERE agent_id = $1", id).Scan(&dbStatus)
		if err == nil && dbStatus == "disconnected" {
			// EÄŸer veritabanÄ±nda disconnected olarak iÅŸaretlenmiÅŸse, status'u disconnected olarak ayarla
			status = "disconnected"
		}

		agent := map[string]interface{}{
			"id":         id,
			"hostname":   conn.Info.Hostname,
			"ip":         conn.Info.Ip,
			"status":     status,
			"last_seen":  lastSeenStr,
			"connection": "grpc",
		}
		agents = append(agents, agent)
	}
	return agents
}

// GetAgentStatusFromDB, veritabanÄ±ndan agent durumlarÄ±nÄ± alÄ±r
func (s *Server) GetAgentStatusFromDB(ctx context.Context) ([]map[string]interface{}, error) {
	query := `
		SELECT hostname, last_seen 
		FROM agents 
		WHERE last_seen > NOW() - INTERVAL '1 minute'
	`

	rows, err := s.db.QueryContext(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("veritabanÄ± sorgusu baÅŸarÄ±sÄ±z: %v", err)
	}
	defer rows.Close()

	agents := make([]map[string]interface{}, 0)
	for rows.Next() {
		var hostname string
		var lastSeen time.Time
		if err := rows.Scan(&hostname, &lastSeen); err != nil {
			return nil, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
		}

		agent := map[string]interface{}{
			"hostname":   hostname,
			"last_seen":  lastSeen.Format(time.RFC3339),
			"status":     "active",
			"connection": "db",
		}
		agents = append(agents, agent)
	}

	if err := rows.Err(); err != nil {
		return nil, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
	}

	return agents, nil
}

// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
func (s *Server) checkDatabaseConnection() error {
	err := s.db.Ping()
	if err != nil {
		logger.Error().Err(err).Msg("VeritabanÄ± baÄŸlantÄ± hatasÄ±")
		return err
	}
	return nil
}

// UpdateAgentDisconnectedStatus, agent'Ä±n veritabanÄ±ndaki durumunu disconnected olarak gÃ¼nceller
func (s *Server) UpdateAgentDisconnectedStatus(agentID string) error {
	query := `
		UPDATE agents 
		SET status = 'disconnected'
		WHERE agent_id = $1
	`
	_, err := s.db.Exec(query, agentID)
	if err != nil {
		logger.Error().Err(err).Str("agent_id", agentID).Msg("Agent durumu veritabanÄ±nda gÃ¼ncellenemedi")
	} else {
		logger.Info().Str("agent_id", agentID).Msg("Agent durumu veritabanÄ±nda disconnected olarak gÃ¼ncellendi")
	}
	return err
}

// UpdateAgentConnectedStatus, agent'Ä±n veritabanÄ±ndaki durumunu active olarak gÃ¼nceller ve last_seen'i ÅŸimdiki zaman yapar
func (s *Server) UpdateAgentConnectedStatus(agentID string) error {
	query := `
		UPDATE agents 
		SET status = 'active', last_seen = CURRENT_TIMESTAMP
		WHERE agent_id = $1
	`
	_, err := s.db.Exec(query, agentID)
	if err != nil {
		logger.Error().Err(err).Str("agent_id", agentID).Msg("Agent durumu veritabanÄ±nda gÃ¼ncellenemedi")
	} else {
		logger.Debug().Str("agent_id", agentID).Msg("Agent durumu veritabanÄ±nda active olarak gÃ¼ncellendi ve last_seen gÃ¼ncellendi")
	}
	return err
}

// SendQuery, belirli bir agent'a sorgu gÃ¶nderir ve cevabÄ± bekler

// SendQuery, belirli bir agent'a sorgu gÃ¶nderir ve cevabÄ± bekler

// SendQuery, belirli bir agent'a sorgu gÃ¶nderir ve cevabÄ± bekler
func (s *Server) SendQuery(ctx context.Context, agentID, queryID, command, database string) (*pb.QueryResult, error) {

	s.mu.RLock()
	defer s.mu.RUnlock()

	// Agent ID'sini kontrol et ve gerekirse dÃ¼zelt
	if !strings.HasPrefix(agentID, "agent_") {
		agentID = "agent_" + agentID
	}

	agentConn, ok := s.agents[agentID]
	if !ok {
		return nil, fmt.Errorf("agent bulunamadÄ±: %s", agentID)
	}

	// Sorgu cevabÄ± iÃ§in bir kanal oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)

	// Haritaya ekle
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Temizlik iÅŸlemi iÃ§in defer
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu agent'a gÃ¶nder
	err := agentConn.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId:  queryID,
				Command:  command,
				Database: database,
			},
		},
	})

	if err != nil {
		logger.Error().Err(err).Str("query_id", queryID).Msg("Sorgu gÃ¶nderimi baÅŸarÄ±sÄ±z")
		return nil, err
	}

	logger.Debug().Str("query_id", queryID).Msg("Sorgu gÃ¶nderildi, yanÄ±t bekleniyor")

	// CevabÄ± bekle (timeout ile)
	select {
	case result := <-resultChan:
		// Protobuf sonucunu JSON formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
		if result.Result != nil {
			// Protobuf struct'Ä± parse et
			var structValue structpb.Struct
			if err := result.Result.UnmarshalTo(&structValue); err != nil {
				logger.Error().Err(err).Str("query_id", queryID).Msg("Error unmarshaling to struct")
				return result, nil
			}

			// Struct'Ä± map'e dÃ¶nÃ¼ÅŸtÃ¼r
			resultMap := structValue.AsMap()

			// Map'i JSON'a dÃ¶nÃ¼ÅŸtÃ¼r
			jsonBytes, err := json.Marshal(resultMap)
			if err != nil {
				logger.Error().Err(err).Str("query_id", queryID).Msg("Error marshaling map to JSON")
				return result, nil
			}

			// Sonucu gÃ¼ncelle
			result.Result = &anypb.Any{
				TypeUrl: "type.googleapis.com/google.protobuf.Value",
				Value:   jsonBytes,
			}
		}
		logger.Debug().Str("query_id", queryID).Msg("Sorgu yanÄ±tÄ± alÄ±ndÄ±")
		return result, nil
	case <-ctx.Done():
		logger.Error().Err(ctx.Err()).Str("query_id", queryID).Msg("Context iptal edildi")
		return nil, ctx.Err()
	case <-time.After(60 * time.Second): // 60 saniye timeout - uzun sÃ¼ren sorgular iÃ§in
		logger.Error().Str("query_id", queryID).Msg("Sorgu zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
		return nil, fmt.Errorf("sorgu zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
	}
}

// SendSystemMetrics, agent'dan sistem metriklerini alÄ±r
func (s *Server) SendSystemMetrics(ctx context.Context, req *pb.SystemMetricsRequest) (*pb.SystemMetricsResponse, error) {
	logger.Info().Str("agent_id", req.AgentId).Msg("SendSystemMetrics baÅŸladÄ± - basitleÅŸtirilmiÅŸ yaklaÅŸÄ±m")

	// Agent ID'yi standart formata getir
	agentID := req.AgentId
	if !strings.HasPrefix(agentID, "agent_") {
		agentID = "agent_" + agentID
		logger.Debug().Str("agent_id", agentID).Msg("Agent ID dÃ¼zeltildi")
	}

	// Agent baÄŸlantÄ±sÄ±nÄ± bul
	s.mu.RLock()
	agentConn, ok := s.agents[agentID]
	s.mu.RUnlock()

	if !ok {
		logger.Error().Str("agent_id", agentID).Msg("Agent bulunamadÄ±")
		return nil, fmt.Errorf("agent bulunamadÄ±: %s", agentID)
	}

	if agentConn.Stream == nil {
		logger.Error().Str("agent_id", agentID).Msg("Agent stream baÄŸlantÄ±sÄ± yok")
		return nil, fmt.Errorf("agent stream baÄŸlantÄ±sÄ± yok: %s", agentID)
	}

	// Metrics iÃ§in bir kanal oluÅŸtur
	metricsChan := make(chan *pb.SystemMetrics, 1)
	errorChan := make(chan error, 1)

	// Unique bir query ID oluÅŸtur
	queryID := fmt.Sprintf("metrics_%d", time.Now().UnixNano())

	// Query sonucu iÃ§in bir kanal oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Ä°ÅŸlem bitince cleanup yap
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
		close(metricsChan)
		close(errorChan)
	}()

	logger.Info().Str("query_id", queryID).Msg("Metrik almak iÃ§in Ã¶zel sorgu gÃ¶nderiliyor")

	// Metrik almak iÃ§in "get_system_metrics" adÄ±nda Ã¶zel bir sorgu gÃ¶nder
	err := agentConn.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId:  queryID,
				Command:  "get_system_metrics",
				Database: "",
			},
		},
	})

	if err != nil {
		logger.Error().Err(err).Str("query_id", queryID).Msg("Metrik sorgusu gÃ¶nderilemedi")
		return nil, fmt.Errorf("metrik sorgusu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().Str("query_id", queryID).Msg("Metrik sorgusu gÃ¶nderildi, yanÄ±t bekleniyor")

	// 3 saniyelik timeout ayarla
	ctx, cancel := context.WithTimeout(ctx, 15*time.Second)
	defer cancel()

	// YanÄ±tÄ± bekle
	select {
	case result := <-resultChan:
		if result == nil {
			logger.Error().Str("query_id", queryID).Msg("BoÅŸ metrik yanÄ±tÄ± alÄ±ndÄ±")
			return nil, fmt.Errorf("boÅŸ metrik yanÄ±tÄ± alÄ±ndÄ±")
		}

		logger.Info().Str("query_id", queryID).Msg("Metrik yanÄ±tÄ± alÄ±ndÄ±")

		// Result iÃ§erisindeki Any tipini Struct'a dÃ¶nÃ¼ÅŸtÃ¼r
		var metricsStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&metricsStruct); err != nil {
			logger.Error().Err(err).Str("query_id", queryID).Msg("Metrik yapÄ±sÄ± Ã§Ã¶zÃ¼mlenemedi")
			return nil, fmt.Errorf("metrik yapÄ±sÄ± Ã§Ã¶zÃ¼mlenemedi: %v", err)
		}
		// YanÄ±tÄ± oluÅŸtur ve dÃ¶ndÃ¼r
		response := &pb.SystemMetricsResponse{
			Status: "success",
			Data:   &metricsStruct,
		}

		logger.Info().Str("query_id", queryID).Msg("Metrik yanÄ±tÄ± baÅŸarÄ±yla dÃ¶ndÃ¼rÃ¼lÃ¼yor")
		return response, nil

	case <-ctx.Done():
		logger.Error().Err(ctx.Err()).Str("query_id", queryID).Msg("Metrik yanÄ±tÄ± beklerken timeout")
		return nil, ctx.Err()
	}
}

// GetDB, veritabanÄ± baÄŸlantÄ±sÄ±nÄ± dÃ¶ndÃ¼rÃ¼r
func (s *Server) GetDB() *sql.DB {
	return s.db
}

// GetInfluxWriter, InfluxDB writer'Ä±nÄ± dÃ¶ndÃ¼rÃ¼r
func (s *Server) GetInfluxWriter() *metrics.InfluxDBWriter {
	return s.influxWriter
}

// SendMetrics, agent'dan gelen metric batch'lerini iÅŸler
func (s *Server) SendMetrics(ctx context.Context, req *pb.SendMetricsRequest) (*pb.SendMetricsResponse, error) {
	if req.Batch == nil {
		return &pb.SendMetricsResponse{
			Status:  "error",
			Message: "Batch is required",
		}, nil
	}

	batch := req.Batch

	var errors []string
	processedCount := int32(0)

	// InfluxDB'ye metrikler yazÄ±lacaksa
	if s.influxWriter != nil {
		// InfluxDB yazma iÅŸlemleri iÃ§in ayrÄ± context - uzun timeout
		influxCtx, influxCancel := context.WithTimeout(context.Background(), 60*time.Second)
		defer influxCancel()
		// PostgreSQL active queries metadata'sÄ±nÄ± kontrol et ve iÅŸle
		if activeQueriesJSON, exists := batch.Metadata["active_queries"]; exists && batch.MetricType == "postgresql_database" {
			if err := s.processActiveQueriesMetadata(influxCtx, batch.AgentId, activeQueriesJSON); err != nil {
				errorMsg := fmt.Sprintf("Active queries metadata iÅŸleme hatasÄ±: %v", err)
				errors = append(errors, errorMsg)
				logger.Error().
					Str("agent_id", batch.AgentId).
					Err(err).
					Msg("Active queries metadata iÅŸleme hatasÄ±")
			} else {
				logger.Debug().
					Str("agent_id", batch.AgentId).
					Msg("Active queries metadata baÅŸarÄ±yla iÅŸlendi")
			}
		}

		// MongoDB active operations metadata'sÄ±nÄ± kontrol et ve iÅŸle
		if activeOperationsJSON, exists := batch.Metadata["active_operations"]; exists && batch.MetricType == "mongodb_database" {
			if err := s.processActiveOperationsMetadata(influxCtx, batch.AgentId, activeOperationsJSON); err != nil {
				errorMsg := fmt.Sprintf("Active operations metadata iÅŸleme hatasÄ±: %v", err)
				errors = append(errors, errorMsg)
				logger.Error().
					Str("agent_id", batch.AgentId).
					Err(err).
					Msg("Active operations metadata iÅŸleme hatasÄ±")
			} else {
				logger.Debug().
					Str("agent_id", batch.AgentId).
					Msg("Active operations metadata baÅŸarÄ±yla iÅŸlendi")
			}
		}

		// TÃ¼m metrikler iÃ§in points oluÅŸtur ve batch olarak yaz
		if len(batch.Metrics) > 0 {
			if err := s.writeBatchMetricsToInfluxDB(influxCtx, batch); err != nil {
				// Batch yazma baÅŸarÄ±sÄ±z olursa, tekil yazma deneyeceÄŸiz
				logger.Warn().
					Str("agent_id", batch.AgentId).
					Int("metric_count", len(batch.Metrics)).
					Err(err).
					Msg("Batch metric yazma baÅŸarÄ±sÄ±z, tekil yazma deneyecek")
				
				// Her metric'i tek tek yazmayÄ± dene
				for _, metric := range batch.Metrics {
					if err := s.writeMetricToInfluxDB(influxCtx, batch, metric); err != nil {
						errorMsg := fmt.Sprintf("Metric yazma hatasÄ± (%s): %v", metric.Name, err)
						errors = append(errors, errorMsg)
						logger.Error().
							Str("metric_name", metric.Name).
							Str("agent_id", batch.AgentId).
							Err(err).
							Msg("Metric yazma hatasÄ±")
					} else {
						processedCount++
					}
				}
			} else {
				processedCount = int32(len(batch.Metrics))
				logger.Debug().
					Str("agent_id", batch.AgentId).
					Int("metric_count", len(batch.Metrics)).
					Msg("Batch metric yazma baÅŸarÄ±lÄ±")
			}
		}
	} else {
		// InfluxDB yoksa sadece log'la
		for _, metric := range batch.Metrics {
			logger.Debug().
				Str("metric_name", metric.Name).
				Float64("value", s.getMetricValueAsFloat(metric.Value)).
				Str("agent_id", batch.AgentId).
				Msg("Metric alÄ±ndÄ±")
			processedCount++
		}
	}

	// YanÄ±t oluÅŸtur
	status := "success"
	message := fmt.Sprintf("Processed %d metrics", processedCount)

	if len(errors) > 0 {
		if processedCount == 0 {
			status = "error"
			message = "All metrics failed to process"
		} else {
			status = "partial_success"
			message = fmt.Sprintf("Processed %d metrics with %d errors", processedCount, len(errors))
		}
	}

	return &pb.SendMetricsResponse{
		Status:         status,
		Message:        message,
		ProcessedCount: processedCount,
		Errors:         errors,
	}, nil
}

// processActiveQueriesMetadata, metadata'daki active queries JSON'Ä±nÄ± iÅŸler ve InfluxDB'ye yazar
func (s *Server) processActiveQueriesMetadata(ctx context.Context, agentID, activeQueriesJSON string) error {
	// JSON'Ä± parse et
	var queries []map[string]interface{}
	if err := json.Unmarshal([]byte(activeQueriesJSON), &queries); err != nil {
		return fmt.Errorf("active queries JSON parse hatasÄ±: %v", err)
	}

	logger.Debug().
		Str("agent_id", agentID).
		Int("query_count", len(queries)).
		Str("json_preview", activeQueriesJSON[:min(200, len(activeQueriesJSON))]).
		Msg("Active/completed queries metadata parse edildi")

		// Her query iÃ§in InfluxDB point'i oluÅŸtur
	activeCount := 0
	completedCount := 0

	for i, queryData := range queries {
		// Agent'dan gelen timestamp'Ä± kullan, yoksa current time
		timestamp := time.Now()
		if timestampStr, ok := queryData["timestamp"].(string); ok && timestampStr != "" {
			if parsedTime, err := time.Parse(time.RFC3339Nano, timestampStr); err == nil {
				timestamp = parsedTime
			}
		}
		// Debug: field'larÄ± ve tiplerini logla
		logger.Debug().
			Str("agent_id", agentID).
			Int("query_index", i).
			Interface("query_data", queryData).
			Msg("Query data debug")

		// PID tracking ile gelen yeni alanlarÄ± iÅŸle
		pid := queryData["pid"]
		state, _ := queryData["state"].(string)

		// Query completion detection iÃ§in state'i kontrol et
		if state == "completed" {
			completedCount++
			logger.Debug().
				Str("agent_id", agentID).
				Interface("pid", pid).
				Interface("duration", queryData["duration_seconds"]).
				Msg("Completed query detected")

			// Completed query'leri ayrÄ± measurement'a yaz
			if err := s.writeCompletedQueryToInfluxDB(ctx, agentID, queryData, timestamp); err != nil {
				logger.Error().
					Str("agent_id", agentID).
					Err(err).
					Msg("Completed query InfluxDB yazma hatasÄ±")
				return err
			}
		} else {
			activeCount++
			// Active query'leri mevcut ÅŸekilde iÅŸle
			if err := s.writeActiveQueryToInfluxDB(ctx, agentID, queryData, timestamp); err != nil {
				logger.Error().
					Str("agent_id", agentID).
					Err(err).
					Msg("Active query InfluxDB yazma hatasÄ±")
				return err
			}
		}
	}

	logger.Debug().
		Str("agent_id", agentID).
		Int("active_queries", activeCount).
		Int("completed_queries", completedCount).
		Msg("Query processing completed")

	return nil
}

// processActiveOperationsMetadata, metadata'daki active operations JSON'Ä±nÄ± iÅŸler ve InfluxDB'ye yazar
func (s *Server) processActiveOperationsMetadata(ctx context.Context, agentID, activeOperationsJSON string) error {
	// JSON'Ä± parse et
	var operations []map[string]interface{}
	if err := json.Unmarshal([]byte(activeOperationsJSON), &operations); err != nil {
		return fmt.Errorf("active operations JSON parse hatasÄ±: %v", err)
	}

	logger.Debug().
		Str("agent_id", agentID).
		Int("operation_count", len(operations)).
		Str("json_preview", activeOperationsJSON[:min(200, len(activeOperationsJSON))]).
		Msg("Active operations metadata parse edildi")

	// Her operation iÃ§in InfluxDB point'i oluÅŸtur
	activeCount := 0

	for i, operationData := range operations {
		// Agent'dan gelen timestamp'Ä± kullan, yoksa current time
		timestamp := time.Now()
		if timestampStr, ok := operationData["timestamp"].(string); ok && timestampStr != "" {
			if parsedTime, err := time.Parse(time.RFC3339Nano, timestampStr); err == nil {
				timestamp = parsedTime
			}
		}

		// Debug: field'larÄ± ve tiplerini logla
		logger.Debug().
			Str("agent_id", agentID).
			Int("operation_index", i).
			Interface("operation_data", operationData).
			Msg("Operation data debug")

		activeCount++
		// Active operation'larÄ± InfluxDB'ye yaz
		if err := s.writeActiveOperationToInfluxDB(ctx, agentID, operationData, timestamp); err != nil {
			logger.Error().
				Str("agent_id", agentID).
				Err(err).
				Msg("Active operation InfluxDB yazma hatasÄ±")
			return err
		}
	}

	logger.Debug().
		Str("agent_id", agentID).
		Int("active_operations", activeCount).
		Msg("Operation processing completed")

	return nil
}

// writeActiveQueryToInfluxDB, tek bir active query'yi InfluxDB'ye yazar
// TTL: 10 dakika (otomatik expire)
func (s *Server) writeActiveQueryToInfluxDB(ctx context.Context, agentID string, queryData map[string]interface{}, timestamp time.Time) error {
	// Tags'leri oluÅŸtur
	tags := map[string]string{
		"agent_id": agentID,
	}

	// Query-specific tags ekle
	if dbName, ok := queryData["database_name"].(string); ok {
		tags["database"] = dbName
	}
	if username, ok := queryData["username"].(string); ok {
		tags["username"] = username
	}
	if appName, ok := queryData["application_name"].(string); ok {
		tags["application"] = appName
	}
	if state, ok := queryData["state"].(string); ok {
		tags["state"] = state
	}
	if clientAddr, ok := queryData["client_addr"].(string); ok && clientAddr != "" {
		tags["client_addr"] = clientAddr
	}
	if waitEventType, ok := queryData["wait_event_type"].(string); ok && waitEventType != "" {
		tags["wait_event_type"] = waitEventType
	}
	if waitEvent, ok := queryData["wait_event"].(string); ok && waitEvent != "" {
		tags["wait_event"] = waitEvent
	}

	// Fields'leri oluÅŸtur
	fields := make(map[string]interface{})

	// PID field'Ä± ekle
	if pid, ok := queryData["pid"]; ok {
		switch v := pid.(type) {
		case float64:
			fields["pid"] = int64(v)
		case float32:
			fields["pid"] = int64(v)
		case int:
			fields["pid"] = int64(v)
		case int64:
			fields["pid"] = v
		case string:
			if parsed, err := strconv.ParseInt(v, 10, 64); err == nil {
				fields["pid"] = parsed
			}
		}
	}

	// Duration field'Ä± - debug ile birlikte
	if duration, ok := queryData["duration_seconds"]; ok {
		logger.Debug().
			Str("agent_id", agentID).
			Interface("duration_raw", duration).
			Str("duration_type", fmt.Sprintf("%T", duration)).
			Msg("Duration field debug")

		// FarklÄ± tip dÃ¶nÃ¼ÅŸÃ¼mlerini dene
		switch v := duration.(type) {
		case float64:
			fields["duration"] = v
		case float32:
			fields["duration"] = float64(v)
		case int:
			fields["duration"] = float64(v)
		case int64:
			fields["duration"] = float64(v)
		case string:
			if parsed, err := strconv.ParseFloat(v, 64); err == nil {
				fields["duration"] = parsed
			}
		}
	}

	// Query text field'Ä± - debug ile birlikte
	if queryText, ok := queryData["query"].(string); ok {
		fields["text"] = queryText
		logger.Debug().
			Str("agent_id", agentID).
			Str("query_text", queryText).
			Msg("Query text field debug")
	} else {
		logger.Debug().
			Str("agent_id", agentID).
			Interface("query_raw", queryData["query"]).
			Str("query_type", fmt.Sprintf("%T", queryData["query"])).
			Msg("Query text field missing or wrong type")
	}

	// Query start time field'Ä± ekle
	if queryStartStr, ok := queryData["query_start"].(string); ok && queryStartStr != "" {
		if queryStart, err := time.Parse(time.RFC3339Nano, queryStartStr); err == nil {
			fields["query_start"] = queryStart
		}
	}

	logger.Debug().
		Str("agent_id", agentID).
		Interface("tags", tags).
		Interface("fields", fields).
		Msg("Final active query InfluxDB write data")

	// InfluxDB'ye yaz
	return s.influxWriter.WriteMetric(ctx, "postgresql_active_query", tags, fields, timestamp)
}

// writeCompletedQueryToInfluxDB, tamamlanmÄ±ÅŸ query'leri InfluxDB'ye yazar
// TTL: Yok (kalÄ±cÄ± history)
func (s *Server) writeCompletedQueryToInfluxDB(ctx context.Context, agentID string, queryData map[string]interface{}, timestamp time.Time) error {
	// Tags'leri oluÅŸtur
	tags := map[string]string{
		"agent_id":    agentID,
		"query_state": "completed",
	}

	// Query-specific tags ekle
	if dbName, ok := queryData["database_name"].(string); ok {
		tags["database"] = dbName
	}
	if username, ok := queryData["username"].(string); ok {
		tags["username"] = username
	}
	if appName, ok := queryData["application_name"].(string); ok {
		tags["application"] = appName
	}
	if clientAddr, ok := queryData["client_addr"].(string); ok && clientAddr != "" {
		tags["client_addr"] = clientAddr
	}
	if waitEventType, ok := queryData["wait_event_type"].(string); ok && waitEventType != "" {
		tags["wait_event_type"] = waitEventType
	}
	if waitEvent, ok := queryData["wait_event"].(string); ok && waitEvent != "" {
		tags["wait_event"] = waitEvent
	}

	// Fields'leri oluÅŸtur
	fields := make(map[string]interface{})

	// Query text field'Ä±
	if queryText, ok := queryData["query"].(string); ok {
		fields["query_text"] = queryText
	}

	// Duration field'Ä±
	if duration, ok := queryData["duration_seconds"]; ok {
		switch v := duration.(type) {
		case float64:
			fields["duration_seconds"] = v
		case float32:
			fields["duration_seconds"] = float64(v)
		case int:
			fields["duration_seconds"] = float64(v)
		case int64:
			fields["duration_seconds"] = float64(v)
		case string:
			if parsed, err := strconv.ParseFloat(v, 64); err == nil {
				fields["duration_seconds"] = parsed
			}
		}
	}

	// PID field'Ä±
	if pid, ok := queryData["pid"]; ok {
		switch v := pid.(type) {
		case float64:
			fields["pid"] = int64(v)
		case float32:
			fields["pid"] = int64(v)
		case int:
			fields["pid"] = int64(v)
		case int64:
			fields["pid"] = v
		case string:
			if parsed, err := strconv.ParseInt(v, 10, 64); err == nil {
				fields["pid"] = parsed
			}
		}
	}

	// Completion time - agent'dan gelen completion_time'Ä± kullan
	if completionTimeStr, ok := queryData["completion_time"].(string); ok && completionTimeStr != "" {
		if completionTime, err := time.Parse(time.RFC3339Nano, completionTimeStr); err == nil {
			fields["completion_time"] = completionTime
		} else {
			// Parse edilemezse current time kullan
			fields["completion_time"] = timestamp
		}
	} else {
		// completion_time yoksa current time kullan
		fields["completion_time"] = timestamp
	}

	// Query start time field'Ä± ekle
	if queryStartStr, ok := queryData["query_start"].(string); ok && queryStartStr != "" {
		if queryStart, err := time.Parse(time.RFC3339Nano, queryStartStr); err == nil {
			fields["query_start"] = queryStart
		}
	}

	logger.Debug().
		Str("agent_id", agentID).
		Interface("tags", tags).
		Interface("fields", fields).
		Msg("Final completed query InfluxDB write data")

	// InfluxDB'ye yaz
	return s.influxWriter.WriteMetric(ctx, "postgresql_query_history", tags, fields, timestamp)
}

// writeActiveOperationToInfluxDB, tek bir active operation'Ä± InfluxDB'ye yazar
// TTL: 10 dakika (otomatik expire)
func (s *Server) writeActiveOperationToInfluxDB(ctx context.Context, agentID string, operationData map[string]interface{}, timestamp time.Time) error {
	// Tags'leri oluÅŸtur
	tags := map[string]string{
		"agent_id": agentID,
	}

	// Operation-specific tags ekle
	if dbName, ok := operationData["database"].(string); ok {
		tags["database"] = dbName
	}
	if collection, ok := operationData["collection"].(string); ok {
		tags["collection"] = collection
	}
	if opType, ok := operationData["op_type"].(string); ok {
		tags["op_type"] = opType
	}
	if namespace, ok := operationData["namespace"].(string); ok {
		tags["namespace"] = namespace
	}
	if client, ok := operationData["client"].(string); ok && client != "" {
		tags["client"] = client
	}
	if lockType, ok := operationData["lock_type"].(string); ok && lockType != "" {
		tags["lock_type"] = lockType
	}

	// Fields'leri oluÅŸtur
	fields := make(map[string]interface{})

	// Operation ID field'Ä± ekle
	if operationID, ok := operationData["operation_id"].(string); ok {
		fields["operation_id"] = operationID
	}

	// Connection ID field'Ä±
	if connID, ok := operationData["connection_id"]; ok {
		switch v := connID.(type) {
		case float64:
			fields["connection_id"] = int64(v)
		case float32:
			fields["connection_id"] = int64(v)
		case int:
			fields["connection_id"] = int64(v)
		case int64:
			fields["connection_id"] = v
		case string:
			if parsed, err := strconv.ParseInt(v, 10, 64); err == nil {
				fields["connection_id"] = parsed
			}
		}
	}

	// Duration field'Ä±
	if duration, ok := operationData["duration_seconds"]; ok {
		logger.Debug().
			Str("agent_id", agentID).
			Interface("duration_raw", duration).
			Msg("Processing MongoDB operation duration")

		switch v := duration.(type) {
		case float64:
			fields["duration"] = v
		case float32:
			fields["duration"] = float64(v)
		case int:
			fields["duration"] = float64(v)
		case int64:
			fields["duration"] = float64(v)
		case string:
			if parsed, err := strconv.ParseFloat(v, 64); err == nil {
				fields["duration"] = parsed
			}
		}
	}

	// Command field'Ä±
	if command, ok := operationData["command"].(string); ok {
		fields["command"] = command
	}

	// Command details field'Ä± - JSON olarak sakla
	if commandDetails, ok := operationData["command_details"]; ok {
		if commandDetailsJSON, err := json.Marshal(commandDetails); err == nil {
			fields["command_details"] = string(commandDetailsJSON)
		} else {
			logger.Debug().
				Str("agent_id", agentID).
				Err(err).
				Interface("command_details", commandDetails).
				Msg("Command details JSON marshal hatasÄ±")
		}
	}

	// ðŸ†• Yeni truncated command alanlarÄ±
	if commandTruncated, ok := operationData["command_truncated"].(bool); ok {
		fields["command_truncated"] = commandTruncated
	}
	if commandType, ok := operationData["command_type"].(string); ok {
		fields["command_type"] = commandType
	}
	if truncatedContent, ok := operationData["truncated_content"]; ok {
		if truncatedContentJSON, err := json.Marshal(truncatedContent); err == nil {
			fields["truncated_content"] = string(truncatedContentJSON)
		} else {
			logger.Debug().
				Str("agent_id", agentID).
				Err(err).
				Interface("truncated_content", truncatedContent).
				Msg("Truncated content JSON marshal hatasÄ±")
		}
	}
	if inferredCommandType, ok := operationData["inferred_command_type"].(string); ok {
		fields["inferred_command_type"] = inferredCommandType
	}
	if planSummary, ok := operationData["plan_summary"].(string); ok {
		fields["plan_summary"] = planSummary
	}
	if note, ok := operationData["note"].(string); ok {
		fields["note"] = note
	}

	// ðŸ†• Client details field'Ä± - JSON olarak sakla
	if clientDetails, ok := operationData["client_details"]; ok {
		if clientDetailsJSON, err := json.Marshal(clientDetails); err == nil {
			fields["client_details"] = string(clientDetailsJSON)
		} else {
			logger.Debug().
				Str("agent_id", agentID).
				Err(err).
				Interface("client_details", clientDetails).
				Msg("Client details JSON marshal hatasÄ±")
		}
	}

	// Thread ID field'Ä±
	if threadID, ok := operationData["thread_id"].(string); ok {
		fields["thread_id"] = threadID
	}

	// Waiting for lock field'Ä±
	if waitingForLock, ok := operationData["waiting_for_lock"].(bool); ok {
		fields["waiting_for_lock"] = waitingForLock
	}

	logger.Debug().
		Str("agent_id", agentID).
		Interface("tags", tags).
		Interface("fields", fields).
		Msg("Final active operation InfluxDB write data")

	// InfluxDB'ye yaz - TTL ile (10 dakika)
	return s.influxWriter.WriteMetric(ctx, "mongodb_active_operation", tags, fields, timestamp)
}

// writeMetricToInfluxDB tek bir metric'i InfluxDB'ye yazar
func (s *Server) writeMetricToInfluxDB(ctx context.Context, batch *pb.MetricBatch, metric *pb.Metric) error {
	// Metric deÄŸerini float64'e Ã§evir
	value := s.getMetricValueAsFloat(metric.Value)

	// Tags'leri map'e Ã§evir
	tags := make(map[string]string)
	tags["agent_id"] = batch.AgentId
	tags["metric_type"] = batch.MetricType

	for _, tag := range metric.Tags {
		tags[tag.Key] = tag.Value
	}

	// Fields'leri oluÅŸtur
	fieldName := s.extractFieldName(metric.Name)

	fields := map[string]interface{}{
		fieldName: value,
	}

	if metric.Unit != "" {
		fields[fieldName+"_unit"] = metric.Unit
	}
	if metric.Description != "" {
		fields[fieldName+"_description"] = metric.Description
	}

	// Timestamp'i time.Time'a Ã§evir
	timestamp := time.Unix(0, metric.Timestamp)

	// Measurement adÄ±nÄ± metric adÄ±ndan Ã§Ä±kar
	measurement := s.extractMeasurementName(metric.Name)

	// InfluxDB'ye yaz
	return s.influxWriter.WriteMetric(ctx, measurement, tags, fields, timestamp)
}

// writeBatchMetricsToInfluxDB batch'teki tÃ¼m metrikler iÃ§in tek seferde InfluxDB'ye yazar
func (s *Server) writeBatchMetricsToInfluxDB(ctx context.Context, batch *pb.MetricBatch) error {
	if s.influxWriter == nil {
		return nil
	}

	// TÃ¼m metrikler iÃ§in points oluÅŸtur
	var points []*write.Point
	
	for _, metric := range batch.Metrics {
		// Tags'leri map'e Ã§evir
		tags := make(map[string]string)
		tags["agent_id"] = batch.AgentId
		tags["metric_type"] = batch.MetricType
		
		for _, tag := range metric.Tags {
			tags[tag.Key] = tag.Value
		}
		
		// String deÄŸerleri tag olarak ekle, numeric deÄŸerleri field olarak
		fieldName := s.extractFieldName(metric.Name)
		fields := make(map[string]interface{})
		
		if metric.Value.GetStringValue() != "" {
			// String deÄŸerleri tag olarak ekle ve dummy field ekle
			tags[fieldName] = metric.Value.GetStringValue()
			fields["value"] = 1 // InfluxDB requires at least one field
		} else {
			// Numeric deÄŸerleri field olarak ekle
			value := s.getMetricValueAsFloat(metric.Value)
			fields[fieldName] = value
		}
		
		if metric.Unit != "" {
			fields[fieldName+"_unit"] = metric.Unit
		}
		if metric.Description != "" {
			fields[fieldName+"_description"] = metric.Description
		}
		
		// Timestamp'i time.Time'a Ã§evir
		timestamp := time.Unix(0, metric.Timestamp)
		
		// Measurement adÄ±nÄ± metric adÄ±ndan Ã§Ä±kar
		measurement := s.extractMeasurementName(metric.Name)
		
		// Point oluÅŸtur
		point := influxdb2.NewPointWithMeasurement(measurement)
		
		// Tags ekle
		for k, v := range tags {
			point = point.AddTag(k, v)
		}
		
		// Fields ekle
		for k, v := range fields {
			point = point.AddField(k, v)
		}
		
		// Timestamp ayarla
		point = point.SetTime(timestamp)
		
		points = append(points, point)
	}
	
	// Batch olarak InfluxDB'ye yaz
	return s.influxWriter.WriteBatchPoints(ctx, points...)
}

// getMetricValueAsFloat MetricValue'yu float64'e Ã§evirir
func (s *Server) getMetricValueAsFloat(value *pb.MetricValue) float64 {
	if value == nil {
		return 0
	}

	switch v := value.Value.(type) {
	case *pb.MetricValue_DoubleValue:
		return v.DoubleValue
	case *pb.MetricValue_IntValue:
		return float64(v.IntValue)
	case *pb.MetricValue_BoolValue:
		if v.BoolValue {
			return 1
		}
		return 0
	case *pb.MetricValue_StringValue:
		// String deÄŸerleri parse etmeye Ã§alÄ±ÅŸ
		if parsed, err := strconv.ParseFloat(v.StringValue, 64); err == nil {
			return parsed
		}
		return 0
	default:
		return 0
	}
}

// extractFieldName metric adÄ±ndan field adÄ±nÄ± Ã§Ä±karÄ±r
func (s *Server) extractFieldName(metricName string) string {
	parts := strings.Split(metricName, ".")
	if len(parts) >= 3 {
		return parts[2]
	}
	if len(parts) >= 2 {
		return parts[len(parts)-1]
	}
	return "value"
}

// extractMeasurementName metric adÄ±ndan measurement adÄ±nÄ± Ã§Ä±karÄ±r
func (s *Server) extractMeasurementName(metricName string) string {
	// "mongodb.operations.insert" -> "mongodb_operations"
	// "postgresql.connections.active" -> "postgresql_connections"
	// "mssql.performance.cpu" -> "mssql_performance"
	// "system.cpu.usage" -> "system_cpu"

	parts := strings.Split(metricName, ".")
	if len(parts) >= 2 {
		return strings.Join(parts[:2], "_")
	}

	// Fallback: ilk part'Ä± kullan
	if len(parts) > 0 {
		return parts[0]
	}

	return "unknown"
}

// CollectMetrics, agent'a metric toplama talebi gÃ¶nderir
func (s *Server) CollectMetrics(ctx context.Context, req *pb.CollectMetricsRequest) (*pb.CollectMetricsResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Strs("metric_types", req.MetricTypes).
		Msg("Metric toplama talebi")

	// Bu metod ÅŸu anda sadece acknowledgment dÃ¶ndÃ¼rÃ¼yor
	// Gelecekte agent'a aktif olarak metric toplama talebi gÃ¶ndermek iÃ§in kullanÄ±labilir

	return &pb.CollectMetricsResponse{
		Status:  "success",
		Message: "Metric collection request acknowledged",
	}, nil
}

// ReportAlarm, agent'lardan gelen alarm bildirimlerini iÅŸler
func (s *Server) ReportAlarm(ctx context.Context, req *pb.ReportAlarmRequest) (*pb.ReportAlarmResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Int("alarm_count", len(req.Events)).
		Msg("ReportAlarm metodu Ã§aÄŸrÄ±ldÄ±")

	// Agent ID doÄŸrula
	s.mu.RLock()
	_, agentExists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !agentExists {
		logger.Warn().Str("agent_id", req.AgentId).Msg("Bilinmeyen agent'dan alarm bildirimi")
		// Bilinmeyen agent olsa da iÅŸlemeye devam ediyoruz
	}

	// Gelen her alarmÄ± iÅŸle
	for _, event := range req.Events {
		logger.Info().
			Str("event_id", event.Id).
			Str("status", event.Status).
			Str("metric_name", event.MetricName).
			Str("metric_value", event.MetricValue).
			Str("severity", event.Severity).
			Msg("Alarm iÅŸleniyor")

		// Alarm verilerini veritabanÄ±na kaydet
		err := s.saveAlarmToDatabase(ctx, event)
		if err != nil {
			logger.Error().Err(err).Str("event_id", event.Id).Msg("Alarm veritabanÄ±na kaydedilemedi")
			// Devam et, bir alarmÄ±n kaydedilememesi diÄŸerlerini etkilememeli
		}

		// Bildirimi gÃ¶nder (Slack, Email vb.)
		err = s.sendAlarmNotification(ctx, event)
		if err != nil {
			logger.Error().Err(err).Str("event_id", event.Id).Msg("Alarm bildirimi gÃ¶nderilemedi")
			// Devam et, bir bildirimin gÃ¶nderilememesi diÄŸerlerini etkilememeli
		}
	}

	return &pb.ReportAlarmResponse{
		Status: "success",
	}, nil
}

// saveAlarmToDatabase, alarm olayÄ±nÄ± veritabanÄ±na kaydeder
func (s *Server) saveAlarmToDatabase(ctx context.Context, event *pb.AlarmEvent) error {
	logger.Debug().Str("alarm_id", event.AlarmId).Msg("Starting saveAlarmToDatabase")

	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		logger.Error().Err(err).Str("alarm_id", event.AlarmId).Msg("Database connection check failed")
		return fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}
	logger.Debug().Str("alarm_id", event.AlarmId).Msg("Database connection check passed")

	// SQL sorgusu hazÄ±rla
	query := `
		INSERT INTO alarms (
			alarm_id, 
			event_id, 
			agent_id, 
			status, 
			metric_name, 
			metric_value, 
			message, 
			severity,
			created_at,
			database
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
	`

	// Zaman damgasÄ±nÄ± parse et
	timestamp, err := time.Parse(time.RFC3339, event.Timestamp)
	if err != nil {
		logger.Warn().
			Str("timestamp", event.Timestamp).
			Err(err).
			Str("alarm_id", event.AlarmId).
			Msg("Failed to parse timestamp, using current time")
		timestamp = time.Now() // Parse edilemezse ÅŸu anki zamanÄ± kullan
	} else {
		logger.Debug().
			Time("timestamp", timestamp).
			Str("alarm_id", event.AlarmId).
			Msg("Successfully parsed timestamp")
	}

	// UTC zamanÄ±nÄ± TÃ¼rkiye saatine (UTC+3) Ã§evir
	turkeyLoc, err := time.LoadLocation("Europe/Istanbul")
	if err != nil {
		logger.Warn().Err(err).Str("alarm_id", event.AlarmId).Msg("Failed to load Turkey timezone, using UTC")
	} else {
		timestamp = timestamp.In(turkeyLoc)
		logger.Debug().
			Time("timestamp", timestamp).
			Str("alarm_id", event.AlarmId).
			Msg("Converted timestamp to Turkey time")
	}

	// VeritabanÄ±na kaydet
	if event.MetricName == "postgresql_slow_queries" {
		logger.Debug().
			Str("agent_id", event.AgentId).
			Str("metric_value", event.MetricValue).
			Str("message", event.Message).
			Str("alarm_id", event.AlarmId).
			Msg("Slow query alarm received")

		// postgresql_slow_queries iÃ§in database alanÄ± boÅŸsa varsayÄ±lan olarak "postgres" ata
		if event.Database == "" {
			logger.Debug().Str("alarm_id", event.AlarmId).Msg("Setting default database 'postgres' for postgresql_slow_queries alarm")
			event.Database = "postgres"
		}
	}

	logger.Debug().
		Str("alarm_id", event.AlarmId).
		Str("event_id", event.Id).
		Msg("Executing database insert")
	_, err = s.db.ExecContext(
		ctx,
		query,
		event.AlarmId,
		event.Id,
		event.AgentId,
		event.Status,
		event.MetricName,
		event.MetricValue,
		event.Message,
		event.Severity,
		timestamp,
		event.Database,
	)

	if err != nil {
		logger.Error().
			Err(err).
			Str("alarm_id", event.AlarmId).
			Str("event_id", event.Id).
			Msg("Failed to save alarm to database")
		return fmt.Errorf("alarm veritabanÄ±na kaydedilemedi: %v", err)
	}

	logger.Info().
		Str("event_id", event.Id).
		Str("metric_name", event.MetricName).
		Str("agent_id", event.AgentId).
		Str("alarm_id", event.AlarmId).
		Msg("Successfully saved alarm to database")
	return nil
}

// sendAlarmNotification, alarm olayÄ±nÄ± ilgili kanallara bildirir
func (s *Server) sendAlarmNotification(ctx context.Context, event *pb.AlarmEvent) error {
	// Notification ayarlarÄ±nÄ± veritabanÄ±ndan al
	var slackWebhookURL string
	var slackEnabled bool
	var emailEnabled bool
	var emailServer, emailPort, emailUser, emailPassword, emailFrom string
	var emailRecipientsStr string

	query := `
		SELECT 
			slack_webhook_url,
			slack_enabled,
			email_enabled,
			email_server,
			email_port,
			email_user,
			email_password,
			email_from,
			email_recipients
		FROM notification_settings
		ORDER BY id DESC
		LIMIT 1
	`

	err := s.db.QueryRowContext(ctx, query).Scan(
		&slackWebhookURL,
		&slackEnabled,
		&emailEnabled,
		&emailServer,
		&emailPort,
		&emailUser,
		&emailPassword,
		&emailFrom,
		&emailRecipientsStr,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			logger.Debug().Str("alarm_id", event.AlarmId).Msg("Notification ayarlarÄ± bulunamadÄ±, bildirim gÃ¶nderilemiyor")
			return nil // Ayar yok, hata kabul etmiyoruz
		}
		return fmt.Errorf("notification ayarlarÄ± alÄ±namadÄ±: %v", err)
	}

	// Slack bildirimi gÃ¶nder
	if slackEnabled && slackWebhookURL != "" {
		err = s.sendSlackNotification(event, slackWebhookURL)
		if err != nil {
			logger.Error().Err(err).Str("alarm_id", event.AlarmId).Msg("Slack bildirimi gÃ¶nderilemedi")
		}
	}

	// Email bildirimi gÃ¶nder
	if emailEnabled && emailServer != "" && emailFrom != "" && emailRecipientsStr != "" {
		// PostgreSQL array formatÄ±nÄ± parse et
		var emailRecipients []string
		if len(emailRecipientsStr) > 2 { // En az {} olmalÄ±
			// PostgreSQL array formatÄ±: {email1,email2,...}
			trimmedStr := emailRecipientsStr[1 : len(emailRecipientsStr)-1] // BaÅŸÄ±ndaki { ve sonundaki } karakterlerini kaldÄ±r
			if trimmedStr != "" {
				emailRecipients = strings.Split(trimmedStr, ",")
			}
		}

		if len(emailRecipients) > 0 {
			err = s.sendEmailNotification(event, emailServer, emailPort, emailUser, emailPassword, emailFrom, emailRecipients)
			if err != nil {
				logger.Error().Err(err).Str("alarm_id", event.AlarmId).Msg("Email bildirimi gÃ¶nderilemedi")
			}
		}
	}

	return nil
}

// sendSlackNotification, Slack webhook'u aracÄ±lÄ±ÄŸÄ±yla bildirim gÃ¶nderir
func (s *Server) sendSlackNotification(event *pb.AlarmEvent, webhookURL string) error {
	// Alarm durumuna gÃ¶re emoji ve renk belirle
	var emoji, color string
	if event.Status == "triggered" {
		if event.Severity == "critical" {
			emoji = ":red_circle:"
			color = "#FF0000" // KÄ±rmÄ±zÄ±
		} else if event.Severity == "warning" {
			emoji = ":warning:"
			color = "#FFA500" // Turuncu
		} else {
			emoji = ":information_source:"
			color = "#0000FF" // Mavi
		}
	} else if event.Status == "resolved" {
		emoji = ":white_check_mark:"
		color = "#00FF00" // YeÅŸil
	} else {
		emoji = ":grey_question:"
		color = "#808080" // Gri
	}

	// Mesaj iÃ§eriÄŸi
	title := fmt.Sprintf("%s Alarm: %s", strings.ToUpper(event.Status), event.MetricName)

	// JSON mesajÄ± oluÅŸtur
	message := map[string]interface{}{
		"attachments": []map[string]interface{}{
			{
				"fallback":    fmt.Sprintf("%s %s: %s - %s", emoji, title, event.MetricValue, event.Message),
				"color":       color,
				"title":       title,
				"title_link":  "http://clustereye.io/alarms", // Alarmlar sayfasÄ±na yÃ¶nlendir
				"text":        event.Message,
				"footer":      fmt.Sprintf("Agent: %s | Alarm ID: %s", event.AgentId, event.AlarmId),
				"footer_icon": "https://clustereye.io/favicon.ico", // Varsa logo URL'si
				"ts":          time.Now().Unix(),
				"fields": []map[string]interface{}{
					{
						"title": "Metric",
						"value": event.MetricName,
						"short": true,
					},
					{
						"title": "Value",
						"value": event.MetricValue,
						"short": true,
					},
					{
						"title": "Severity",
						"value": event.Severity,
						"short": true,
					},
					{
						"title": "Status",
						"value": event.Status,
						"short": true,
					},
					{
						"title": "Database",
						"value": func() string {
							if event.Database != "" {
								return event.Database
							}
							return "N/A"
						}(),
						"short": true,
					},
				},
			},
		},
	}

	// JSON'a dÃ¶nÃ¼ÅŸtÃ¼r
	jsonMessage, err := json.Marshal(message)
	if err != nil {
		return fmt.Errorf("JSON dÃ¶nÃ¼ÅŸÃ¼m hatasÄ±: %v", err)
	}

	// Slack'e gÃ¶nder
	resp, err := http.Post(webhookURL, "application/json", bytes.NewBuffer(jsonMessage))
	if err != nil {
		return fmt.Errorf("HTTP POST hatasÄ±: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("slack yanÄ±t kodu baÅŸarÄ±sÄ±z: %d", resp.StatusCode)
	}

	logger.Info().Str("alarm_id", event.Id).Msg("Slack bildirimi baÅŸarÄ±yla gÃ¶nderildi")
	return nil
}

// sendEmailNotification, email aracÄ±lÄ±ÄŸÄ±yla bildirim gÃ¶nderir
func (s *Server) sendEmailNotification(event *pb.AlarmEvent, server, port, user, password, from string, recipients []string) error {
	// Alarm durumuna gÃ¶re konu oluÅŸtur
	var subject string
	if event.Status == "triggered" {
		if event.Severity == "critical" {
			subject = fmt.Sprintf("[KRITIK ALARM] %s: %s", event.MetricName, event.MetricValue)
		} else if event.Severity == "warning" {
			subject = fmt.Sprintf("[UYARI] %s: %s", event.MetricName, event.MetricValue)
		} else {
			subject = fmt.Sprintf("[BILGI] %s: %s", event.MetricName, event.MetricValue)
		}
	} else if event.Status == "resolved" {
		subject = fmt.Sprintf("[COZULDU] %s: %s", event.MetricName, event.MetricValue)
	} else {
		subject = fmt.Sprintf("[DURUM: %s] %s: %s", event.Status, event.MetricName, event.MetricValue)
	}

	// Mesaj iÃ§eriÄŸini oluÅŸtur
	htmlBody := fmt.Sprintf(`
	<!DOCTYPE html>
	<html>
	<head>
		<style>
			body { font-family: Arial, sans-serif; line-height: 1.6; }
			.header { background-color: #f5f5f5; padding: 20px; border-bottom: 1px solid #ddd; }
			.content { padding: 20px; }
			.footer { background-color: #f5f5f5; padding: 20px; border-top: 1px solid #ddd; font-size: 12px; }
			.alarm-critical { color: #cc0000; }
			.alarm-warning { color: #ff9900; }
			.alarm-info { color: #0066cc; }
			.alarm-resolved { color: #009900; }
			.details { margin-top: 20px; border-top: 1px solid #ddd; padding-top: 20px; }
			.detail-row { display: flex; margin-bottom: 10px; }
			.detail-label { width: 150px; font-weight: bold; }
		</style>
	</head>
	<body>
		<div class="header">
			<h2>ClusterEye Alarm Bildirimi</h2>
		</div>
		<div class="content">
			<h3 class="alarm-%s">%s</h3>
			<p>%s</p>
			
			<div class="details">
				<div class="detail-row">
					<div class="detail-label">Agent ID:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">Metrik:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">DeÄŸer:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">Ã–nem Derecesi:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">Durum:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">VeritabanÄ±:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">Alarm ID:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">Olay ID:</div>
					<div>%s</div>
				</div>
				<div class="detail-row">
					<div class="detail-label">Zaman:</div>
					<div>%s</div>
				</div>
			</div>
		</div>
		<div class="footer">
			<p>Bu bildirim <a href="https://clustereye.io">ClusterEye</a> tarafÄ±ndan otomatik olarak gÃ¶nderilmiÅŸtir.</p>
		</div>
	</body>
	</html>
	`,
		event.Severity, // CSS sÄ±nÄ±fÄ± iÃ§in
		subject,        // BaÅŸlÄ±k
		event.Message,  // Mesaj
		event.AgentId,
		event.MetricName,
		event.MetricValue,
		event.Severity,
		event.Status,
		func() string {
			if event.Database != "" {
				return event.Database
			}
			return "N/A"
		}(),
		event.AlarmId,
		event.Id,
		event.Timestamp,
	)

	// Basit metin iÃ§eriÄŸi
	textBody := fmt.Sprintf("ClusterEye Alarm Bildirimi\n\n%s\n\n%s\n\nAgent: %s\nMetrik: %s\nDeÄŸer: %s\nÃ–nem: %s\nDurum: %s\nVeritabanÄ±: %s\nAlarm ID: %s\nOlay ID: %s\nZaman: %s",
		subject,
		event.Message,
		event.AgentId,
		event.MetricName,
		event.MetricValue,
		event.Severity,
		event.Status,
		func() string {
			if event.Database != "" {
				return event.Database
			}
			return "N/A"
		}(),
		event.AlarmId,
		event.Id,
		event.Timestamp,
	)

	logger.Info().
		Str("subject", subject).
		Str("alarm_id", event.Id).
		Msg("Email bildirimi gÃ¶nderiliyor")
	logger.Debug().
		Strs("recipients", recipients).
		Str("alarm_id", event.Id).
		Msg("Email alÄ±cÄ±larÄ±")

	// Email gÃ¶nderme iÅŸlemi burada gerÃ§ekleÅŸtirilecek
	// Bu kÄ±sÄ±m ÅŸimdilik log kaydÄ± yapmaktadÄ±r
	// GerÃ§ek SMTP entegrasyonu iÃ§in aÅŸaÄŸÄ±daki kodu aÃ§abilirsiniz:

	/*
		// SMTP sunucusuna baÄŸlan
		smtpAddr := fmt.Sprintf("%s:%s", server, port)

		// SMTP kimlik doÄŸrulama
		auth := smtp.PlainAuth("", user, password, server)

		// Email iÃ§eriÄŸi
		mime := "MIME-version: 1.0;\nContent-Type: text/html; charset=\"UTF-8\";\n\n"
		msg := []byte("To: " + strings.Join(recipients, ",") + "\r\n" +
			"From: " + from + "\r\n" +
			"Subject: " + subject + "\r\n" +
			mime + "\r\n" +
			htmlBody + "\r\n")

		// Email gÃ¶nder
		err := smtp.SendMail(smtpAddr, auth, from, recipients, msg)
		if err != nil {
			return fmt.Errorf("email gÃ¶nderme hatasÄ±: %v", err)
		}
	*/

	// Temizlik iÃ§in deÄŸiÅŸkenleri kullanÄ±ldÄ± olarak iÅŸaretle
	_ = htmlBody
	_ = textBody

	// BaÅŸarÄ±lÄ± bir ÅŸekilde gÃ¶nderildi
	logger.Info().Str("alarm_id", event.Id).Msg("Email bildirimi baÅŸarÄ±yla gÃ¶nderildi (simÃ¼le edildi)")
	return nil
}

// SendMongoInfo, agent'dan gelen MongoDB bilgilerini iÅŸler
func (s *Server) SendMongoInfo(ctx context.Context, req *pb.MongoInfoRequest) (*pb.MongoInfoResponse, error) {
	logger.Info().Msg("SendMongoInfo metodu Ã§aÄŸrÄ±ldÄ±")

	// Gelen MongoDB bilgilerini logla
	mongoInfo := req.MongoInfo
	
	// Agent ID'sini metrik verilerinden Ã§Ä±kar ve agent durumunu active yap
	agentID := fmt.Sprintf("agent_%s", mongoInfo.Hostname)
	s.UpdateAgentConnectedStatus(agentID)
	logger.Debug().
		Str("cluster", mongoInfo.ClusterName).
		Str("hostname", mongoInfo.Hostname).
		Str("ip", mongoInfo.Ip).
		Msg("MongoDB bilgileri alÄ±ndÄ±")

	// Daha detaylÄ± loglama
	logger.Debug().
		Str("cluster", mongoInfo.ClusterName).
		Str("ip", mongoInfo.Ip).
		Str("hostname", mongoInfo.Hostname).
		Str("node_status", mongoInfo.NodeStatus).
		Str("mongo_version", mongoInfo.MongoVersion).
		Str("location", mongoInfo.Location).
		Str("mongo_status", mongoInfo.MongoStatus).
		Str("replica_set_name", mongoInfo.ReplicaSetName).
		Int64("replication_lag_sec", mongoInfo.ReplicationLagSec).
		Str("free_disk", mongoInfo.FreeDisk).
		Int64("fd_percent", int64(mongoInfo.FdPercent)).
		Msg("MongoDB detay bilgileri")

	// VeritabanÄ±na kaydetme iÅŸlemi
	err := s.saveMongoInfoToDatabase(ctx, mongoInfo)
	if err != nil {
		logger.Error().
			Err(err).
			Str("cluster", mongoInfo.ClusterName).
			Str("hostname", mongoInfo.Hostname).
			Msg("MongoDB bilgileri veritabanÄ±na kaydedilemedi")
		return &pb.MongoInfoResponse{
			Status: "error",
		}, nil
	}

	logger.Info().
		Str("cluster", mongoInfo.ClusterName).
		Str("hostname", mongoInfo.Hostname).
		Msg("MongoDB bilgileri baÅŸarÄ±yla iÅŸlendi ve kaydedildi")

	return &pb.MongoInfoResponse{
		Status: "success",
	}, nil
}

// MongoDB bilgilerini veritabanÄ±na kaydetmek iÃ§in yardÄ±mcÄ± fonksiyon
func (s *Server) saveMongoInfoToDatabase(ctx context.Context, mongoInfo *pb.MongoInfo) error {
	// Ã–nce mevcut kaydÄ± kontrol et
	var existingData []byte
	var id int

	checkQuery := `
		SELECT id, jsondata FROM public.mongo_data 
		WHERE clustername = $1 
		ORDER BY id DESC LIMIT 1
	`

	err := s.db.QueryRowContext(ctx, checkQuery, mongoInfo.ClusterName).Scan(&id, &existingData)

	// Yeni node verisi
	mongoData := map[string]interface{}{
		"ClusterName":       mongoInfo.ClusterName,
		"Location":          mongoInfo.Location,
		"FDPercent":         mongoInfo.FdPercent,
		"FreeDisk":          mongoInfo.FreeDisk,
		"Hostname":          mongoInfo.Hostname,
		"IP":                mongoInfo.Ip,
		"NodeStatus":        mongoInfo.NodeStatus,
		"MongoStatus":       mongoInfo.MongoStatus,
		"MongoVersion":      mongoInfo.MongoVersion,
		"ReplicaSetName":    mongoInfo.ReplicaSetName,
		"ReplicationLagSec": mongoInfo.ReplicationLagSec,
		"Port":              mongoInfo.Port,
		"TotalVCPU":         mongoInfo.TotalVcpu,
		"TotalMemory":       mongoInfo.TotalMemory,
		"ConfigPath":        mongoInfo.ConfigPath,
	}

	var jsonData []byte

	// Hata kontrolÃ¼nÃ¼ dÃ¼zgÃ¼n yap
	if err == nil {
		// Mevcut kayÄ±t var, gÃ¼ncelle
		logger.Debug().
			Str("cluster", mongoInfo.ClusterName).
			Int("id", id).
			Msg("MongoDB cluster iÃ§in mevcut kayÄ±t bulundu, gÃ¼ncelleniyor")

		var existingJSON map[string][]interface{}
		if err := json.Unmarshal(existingData, &existingJSON); err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mongoInfo.ClusterName).
				Msg("Mevcut MongoDB JSON ayrÄ±ÅŸtÄ±rma hatasÄ±")
			return err
		}

		// Cluster array'ini al
		clusterData, ok := existingJSON[mongoInfo.ClusterName]
		if !ok {
			// EÄŸer cluster verisi yoksa yeni oluÅŸtur
			clusterData = []interface{}{}
		}

		// Node'u bul ve gÃ¼ncelle
		nodeFound := false
		nodeChanged := false
		for i, node := range clusterData {
			nodeMap, ok := node.(map[string]interface{})
			if !ok {
				continue
			}

			// Hostname ve IP ile node eÅŸleÅŸmesi kontrol et
			if nodeMap["Hostname"] == mongoInfo.Hostname && nodeMap["IP"] == mongoInfo.Ip {
				// Sadece deÄŸiÅŸen alanlarÄ± gÃ¼ncelle
				nodeFound = true

				// DeÄŸiÅŸiklikleri takip et
				for key, newValue := range mongoData {
					currentValue, exists := nodeMap[key]
					var hasChanged bool

					if !exists {
						// DeÄŸer mevcut deÄŸil, yeni alan ekleniyor
						hasChanged = true
						logger.Debug().
							Str("hostname", mongoInfo.Hostname).
							Str("field", key).
							Msg("MongoDB node'da yeni alan eklendi")
					} else {
						// Mevcut deÄŸer ile yeni deÄŸeri karÅŸÄ±laÅŸtÄ±r
						// Numeric deÄŸerler iÃ§in Ã¶zel karÅŸÄ±laÅŸtÄ±rma yap
						hasChanged = !compareValues(currentValue, newValue)
						if hasChanged {
							logger.Debug().
								Str("hostname", mongoInfo.Hostname).
								Str("field", key).
								Interface("old_value", currentValue).
								Interface("new_value", newValue).
								Msg("MongoDB node'da deÄŸiÅŸiklik tespit edildi")
						}
					}

					if hasChanged {
						nodeMap[key] = newValue
						// NodeStatus, MongoStatus, FreeDisk gibi Ã¶nemli alanlar deÄŸiÅŸtiyse iÅŸaretle
						if key == "NodeStatus" || key == "MongoStatus" || key == "FreeDisk" || key == "ReplicationLagSec" {
							nodeChanged = true
						}
					}
				}

				clusterData[i] = nodeMap
				break
			}
		}

		// EÄŸer node bulunamadÄ±ysa yeni ekle
		if !nodeFound {
			clusterData = append(clusterData, mongoData)
			nodeChanged = true
			logger.Info().
				Str("hostname", mongoInfo.Hostname).
				Str("cluster", mongoInfo.ClusterName).
				Msg("Yeni MongoDB node eklendi")
		}

		// EÄŸer Ã¶nemli bir deÄŸiÅŸiklik yoksa veritabanÄ±nÄ± gÃ¼ncelleme
		if !nodeChanged {
			logger.Debug().
				Str("hostname", mongoInfo.Hostname).
				Str("cluster", mongoInfo.ClusterName).
				Msg("MongoDB node'da Ã¶nemli bir deÄŸiÅŸiklik yok, gÃ¼ncelleme yapÄ±lmadÄ±")
			return nil
		}

		existingJSON[mongoInfo.ClusterName] = clusterData

		// JSON'Ä± gÃ¼ncelle
		jsonData, err = json.Marshal(existingJSON)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mongoInfo.ClusterName).
				Msg("MongoDB JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
			return err
		}

		// VeritabanÄ±nÄ± gÃ¼ncelle
		updateQuery := `
			UPDATE public.mongo_data 
			SET jsondata = $1, updated_at = CURRENT_TIMESTAMP
			WHERE id = $2
		`

		_, err = s.db.ExecContext(ctx, updateQuery, jsonData, id)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mongoInfo.ClusterName).
				Int("id", id).
				Msg("MongoDB veritabanÄ± gÃ¼ncelleme hatasÄ±")
			return err
		}

		logger.Info().
			Str("hostname", mongoInfo.Hostname).
			Str("cluster", mongoInfo.ClusterName).
			Int("record_id", id).
			Msg("MongoDB node bilgileri baÅŸarÄ±yla gÃ¼ncellendi (Ã¶nemli deÄŸiÅŸiklik nedeniyle)")
	} else if err == sql.ErrNoRows {
		// KayÄ±t bulunamadÄ±, yeni kayÄ±t oluÅŸtur
		logger.Info().
			Str("cluster", mongoInfo.ClusterName).
			Str("hostname", mongoInfo.Hostname).
			Msg("MongoDB cluster iÃ§in kayÄ±t bulunamadÄ±, yeni kayÄ±t oluÅŸturuluyor")

		outerJSON := map[string][]interface{}{
			mongoInfo.ClusterName: {mongoData},
		}

		jsonData, err = json.Marshal(outerJSON)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mongoInfo.ClusterName).
				Msg("MongoDB yeni kayÄ±t JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
			return err
		}

		insertQuery := `
			INSERT INTO public.mongo_data (
				jsondata, clustername, created_at, updated_at
			) VALUES ($1, $2, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
			ON CONFLICT (clustername) DO UPDATE SET
				jsondata = EXCLUDED.jsondata,
				updated_at = CURRENT_TIMESTAMP
		`

		_, err = s.db.ExecContext(ctx, insertQuery, jsonData, mongoInfo.ClusterName)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mongoInfo.ClusterName).
				Str("hostname", mongoInfo.Hostname).
				Msg("MongoDB veritabanÄ± ekleme hatasÄ±")
			return err
		}

		logger.Info().
			Str("cluster", mongoInfo.ClusterName).
			Str("hostname", mongoInfo.Hostname).
			Msg("MongoDB node bilgileri baÅŸarÄ±yla veritabanÄ±na kaydedildi (yeni kayÄ±t)")
	} else {
		// BaÅŸka bir veritabanÄ± hatasÄ± oluÅŸtu
		logger.Error().
			Err(err).
			Str("cluster", mongoInfo.ClusterName).
			Str("hostname", mongoInfo.Hostname).
			Msg("MongoDB cluster kayÄ±t kontrolÃ¼ sÄ±rasÄ±nda hata")
		return fmt.Errorf("veritabanÄ± kontrol hatasÄ±: %v", err)
	}

	return nil
}

// GetStatusMongo, MongoDB veritabanÄ±ndan durum bilgilerini Ã§eker
func (s *Server) GetStatusMongo(ctx context.Context, _ *structpb.Struct) (*structpb.Value, error) {
	rows, err := s.db.QueryContext(ctx, "SELECT json_agg(sub.jsondata) FROM (SELECT jsondata FROM mongo_data ORDER BY id) AS sub")
	if err != nil {
		return nil, status.Errorf(codes.Internal, "VeritabanÄ± sorgusu baÅŸarÄ±sÄ±z: %v", err)
	}
	defer rows.Close()

	var jsonData []byte
	if rows.Next() {
		err := rows.Scan(&jsonData)
		if err != nil {
			return nil, status.Errorf(codes.Internal, "Veri okuma hatasÄ±: %v", err)
		}
	}

	// JSON verisini structpb.Value'ya dÃ¶nÃ¼ÅŸtÃ¼r
	var jsonValue interface{}
	if err := json.Unmarshal(jsonData, &jsonValue); err != nil {
		return nil, status.Errorf(codes.Internal, "JSON ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
	}

	value, err := structpb.NewValue(jsonValue)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "Veri dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
	}

	return value, nil
}

// ListMongoLogs, belirtilen agent'tan MongoDB log dosyalarÄ±nÄ± listeler
func (s *Server) ListMongoLogs(ctx context.Context, req *pb.MongoLogListRequest) (*pb.MongoLogListResponse, error) {
	logger.Info().Msg("ListMongoLogs Ã§aÄŸrÄ±ldÄ±")

	// Agent ID'yi Ã¶nce metadata'dan almayÄ± dene
	md, ok := metadata.FromIncomingContext(ctx)
	if ok {
		agentIDValues := md.Get("agent-id")
		if len(agentIDValues) > 0 {
			logger.Debug().
				Str("agent_id", agentIDValues[0]).
				Msg("Metadata'dan agent ID alÄ±ndÄ±")
			// Metadata'dan gelen agent ID'yi kullan
			agentID := agentIDValues[0]

			// Agent'a istek gÃ¶nder ve sonucu al
			response, err := s.sendMongoLogListQuery(ctx, agentID)
			if err != nil {
				logger.Error().
					Err(err).
					Str("agent_id", agentID).
					Msg("MongoDB log dosyalarÄ± listelenirken hata")

				// Daha aÃ§Ä±klayÄ±cÄ± hata mesajlarÄ± iÃ§in gRPC status kodlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
				if strings.Contains(err.Error(), "agent bulunamadÄ±") {
					return nil, status.Errorf(codes.NotFound, "Agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
				} else if err == context.DeadlineExceeded {
					return nil, status.Errorf(codes.DeadlineExceeded, "Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
				}

				return nil, status.Errorf(codes.Internal, "MongoDB log dosyalarÄ± listelenirken bir hata oluÅŸtu: %v", err)
			}

			logger.Info().
				Str("agent_id", agentID).
				Int("file_count", len(response.LogFiles)).
				Msg("MongoDB log dosyalarÄ± baÅŸarÄ±yla listelendi")
			return response, nil
		}
	}

	// Metadata'dan alÄ±namadÄ±ysa, context'ten almayÄ± dene
	agentID := ""
	queryCtx, ok := ctx.Value("agent_id").(string)
	if ok && queryCtx != "" {
		agentID = queryCtx
	}

	if agentID == "" {
		return nil, status.Errorf(codes.InvalidArgument, "Agent ID belirtilmedi")
	}

	// Agent'a istek gÃ¶nder ve sonucu al
	response, err := s.sendMongoLogListQuery(ctx, agentID)
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Msg("MongoDB log dosyalarÄ± listelenirken hata")

		// Daha aÃ§Ä±klayÄ±cÄ± hata mesajlarÄ± iÃ§in gRPC status kodlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
		if strings.Contains(err.Error(), "agent bulunamadÄ±") {
			return nil, status.Errorf(codes.NotFound, "Agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
		} else if err == context.DeadlineExceeded {
			return nil, status.Errorf(codes.DeadlineExceeded, "Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
		}

		return nil, status.Errorf(codes.Internal, "MongoDB log dosyalarÄ± listelenirken bir hata oluÅŸtu: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Int("file_count", len(response.LogFiles)).
		Msg("MongoDB log dosyalarÄ± baÅŸarÄ±yla listelendi")
	return response, nil
}

// AnalyzeMongoLog, belirtilen agent'tan MongoDB log dosyasÄ±nÄ± analiz etmesini ister
func (s *Server) AnalyzeMongoLog(ctx context.Context, req *pb.MongoLogAnalyzeRequest) (*pb.MongoLogAnalyzeResponse, error) {
	logger.Info().
		Str("log_file_path", req.LogFilePath).
		Bool("log_path_empty", req.LogFilePath == "").
		Int64("threshold_ms", req.SlowQueryThresholdMs).
		Str("agent_id", req.AgentId).
		Msg("AnalyzeMongoLog Ã§aÄŸrÄ±ldÄ±")

	// Agent ID'yi Ã¶nce doÄŸrudan istekten al
	agentID := req.AgentId

	// BoÅŸsa metadata'dan almayÄ± dene
	if agentID == "" {
		md, ok := metadata.FromIncomingContext(ctx)
		if ok {
			agentIDValues := md.Get("agent-id")
			if len(agentIDValues) > 0 {
				logger.Debug().
					Str("agent_id", agentIDValues[0]).
					Msg("Metadata'dan agent ID alÄ±ndÄ±")
				agentID = agentIDValues[0]
			}
		}
	}

	// Hala boÅŸsa context'ten almayÄ± dene
	if agentID == "" {
		queryCtx, ok := ctx.Value("agent_id").(string)
		if ok && queryCtx != "" {
			agentID = queryCtx
		}
	}

	logger.Debug().Str("agent_id", agentID).Msg("KullanÄ±lan agent_id")

	if agentID == "" {
		return nil, status.Errorf(codes.InvalidArgument, "Agent ID belirtilmedi")
	}

	// Ä°stek parametrelerini kontrol et
	if req.LogFilePath == "" {
		return nil, status.Errorf(codes.InvalidArgument, "Log dosya yolu belirtilmedi")
	}

	// Threshold iÃ§in varsayÄ±lan deÄŸeri ayarla
	threshold := req.SlowQueryThresholdMs
	if threshold <= 0 {
		threshold = 100 // VarsayÄ±lan 100ms
		logger.Debug().
			Int64("default_threshold", threshold).
			Msg("Threshold deÄŸeri 0 veya negatif, varsayÄ±lan deÄŸer kullanÄ±lÄ±yor")
	}

	// Agent'a istek gÃ¶nder ve sonucu al
	response, err := s.sendMongoLogAnalyzeQuery(ctx, agentID, req.LogFilePath, threshold)
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Str("log_file_path", req.LogFilePath).
			Msg("MongoDB log analizi iÃ§in hata")

		// Daha aÃ§Ä±klayÄ±cÄ± hata mesajlarÄ± iÃ§in gRPC status kodlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
		if strings.Contains(err.Error(), "agent bulunamadÄ±") {
			return nil, status.Errorf(codes.NotFound, "Agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
		} else if err == context.DeadlineExceeded {
			return nil, status.Errorf(codes.DeadlineExceeded, "Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
		}

		return nil, status.Errorf(codes.Internal, "MongoDB log analizi iÃ§in bir hata oluÅŸtu: %v", err)
	}

	return response, nil
}

// sendMongoLogListQuery, agent'a MongoDB log dosyalarÄ±nÄ± listelemesi iÃ§in sorgu gÃ¶nderir
func (s *Server) sendMongoLogListQuery(ctx context.Context, agentID string) (*pb.MongoLogListResponse, error) {
	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// MongoDB log dosyalarÄ±nÄ± listeleyen bir komut oluÅŸtur
	command := "list_mongo_logs"
	queryID := fmt.Sprintf("mongo_log_list_%d", time.Now().UnixNano())

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("MongoDB log listesi iÃ§in komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	if err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	}); err != nil {
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("MongoDB log dosyalarÄ± iÃ§in sorgu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Msg("Agent'tan yanÄ±t alÄ±ndÄ±")

		// Ã–nce struct olarak ayrÄ±ÅŸtÄ±rmayÄ± dene (Agent'Ä±n gÃ¶nderdiÄŸi tipte)
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Debug().
				Err(err).
				Str("query_id", queryID).
				Msg("Struct ayrÄ±ÅŸtÄ±rma hatasÄ±")

			// Struct ayrÄ±ÅŸtÄ±rma baÅŸarÄ±sÄ±z olursa, MongoLogListResponse olarak dene
			var logListResponse pb.MongoLogListResponse
			if err := result.Result.UnmarshalTo(&logListResponse); err != nil {
				logger.Error().
					Err(err).
					Str("query_id", queryID).
					Msg("MongoLogListResponse ayrÄ±ÅŸtÄ±rma hatasÄ±")
				return nil, fmt.Errorf("sonuÃ§ ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
			}

			logger.Debug().
				Int("file_count", len(logListResponse.LogFiles)).
				Str("query_id", queryID).
				Msg("DoÄŸrudan MongoLogListResponse'a baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±")
			return &logListResponse, nil
		}

		// Struct'tan MongoLogListResponse oluÅŸtur
		logFiles := make([]*pb.MongoLogFile, 0)
		filesValue, ok := resultStruct.Fields["log_files"]
		if ok && filesValue != nil && filesValue.GetListValue() != nil {
			for _, fileValue := range filesValue.GetListValue().Values {
				if fileValue.GetStructValue() != nil {
					fileStruct := fileValue.GetStructValue()

					// Dosya deÄŸerlerini al
					nameValue := fileStruct.Fields["name"].GetStringValue()
					pathValue := fileStruct.Fields["path"].GetStringValue()
					sizeValue := int64(fileStruct.Fields["size"].GetNumberValue())
					lastModifiedValue := int64(fileStruct.Fields["last_modified"].GetNumberValue())

					// MongoLogFile oluÅŸtur
					logFile := &pb.MongoLogFile{
						Name:         nameValue,
						Path:         pathValue,
						Size:         sizeValue,
						LastModified: lastModifiedValue,
					}

					logFiles = append(logFiles, logFile)
				}
			}
		}

		logger.Debug().
			Int("file_count", len(logFiles)).
			Str("query_id", queryID).
			Msg("Struct'tan oluÅŸturulan log dosyalarÄ± sayÄ±sÄ±")

		return &pb.MongoLogListResponse{
			LogFiles: logFiles,
		}, nil

	case <-ctx.Done():
		// Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±
		return nil, ctx.Err()
	}
}

// SendMongoDatabaseListQuery, agent'a MongoDB veritabanlarÄ±nÄ± listelemesi iÃ§in sorgu gÃ¶nderir
func (s *Server) SendMongoDatabaseListQuery(ctx context.Context, agentID string) ([]string, error) {
	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// MongoDB database listesi iÃ§in bir komut oluÅŸtur
	command := "list_mongo_databases"
	queryID := fmt.Sprintf("mongo_db_list_%d", time.Now().UnixNano())

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("MongoDB database listesi iÃ§in komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	if err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	}); err != nil {
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("MongoDB database listesi iÃ§in sorgu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Msg("Agent'tan yanÄ±t alÄ±ndÄ±")

		// Struct olarak ayrÄ±ÅŸtÄ±r
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Error().
				Err(err).
				Str("query_id", queryID).
				Msg("Struct ayrÄ±ÅŸtÄ±rma hatasÄ±")
			return nil, fmt.Errorf("sonuÃ§ ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
		}

		// Debug: TÃ¼m field'larÄ± log'la
		logger.Debug().
			Str("query_id", queryID).
			Interface("fields", resultStruct.Fields).
			Msg("Agent'tan gelen response fields")

		// Status kontrolÃ¼
		statusValue, ok := resultStruct.Fields["status"]
		if !ok {
			logger.Error().
				Str("query_id", queryID).
				Msg("Status field bulunamadÄ±")
			return nil, fmt.Errorf("agent hatasÄ±: Status field yok")
		}
		
		statusStr := statusValue.GetStringValue()
		logger.Debug().
			Str("query_id", queryID).
			Str("status", statusStr).
			Msg("Agent status")
			
		if statusStr != "success" {
			errorMsg := "Bilinmeyen hata"
			if messageValue, exists := resultStruct.Fields["message"]; exists {
				errorMsg = messageValue.GetStringValue()
			}
			logger.Error().
				Str("query_id", queryID).
				Str("status", statusStr).
				Str("error", errorMsg).
				Msg("Agent baÅŸarÄ±sÄ±z status dÃ¶ndÃ¼rdÃ¼")
			return nil, fmt.Errorf("agent hatasÄ±: %s", errorMsg)
		}

		// Database listesini Ã§Ä±kar
		var databases []string
		databasesValue, ok := resultStruct.Fields["databases"]
		if ok && databasesValue != nil && databasesValue.GetListValue() != nil {
			for _, dbValue := range databasesValue.GetListValue().Values {
				if dbValue.GetStructValue() != nil {
					dbStruct := dbValue.GetStructValue()
					if nameField, exists := dbStruct.Fields["name"]; exists {
						databases = append(databases, nameField.GetStringValue())
					}
				}
			}
		}

		logger.Debug().
			Int("database_count", len(databases)).
			Str("query_id", queryID).
			Msg("Struct'tan oluÅŸturulan database listesi")

		return databases, nil

	case <-ctx.Done():
		// Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±
		return nil, ctx.Err()
	}
}

// sendMongoLogAnalyzeQuery, agent'a MongoDB log dosyasÄ±nÄ± analiz etmesi iÃ§in sorgu gÃ¶nderir
func (s *Server) sendMongoLogAnalyzeQuery(ctx context.Context, agentID, logFilePath string, thresholdMs int64) (*pb.MongoLogAnalyzeResponse, error) {
	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// logFilePath boÅŸ mu kontrol et
	if logFilePath == "" {
		return nil, fmt.Errorf("log dosya yolu boÅŸ olamaz")
	}

	// MongoDB log analizi iÃ§in bir komut oluÅŸtur
	command := fmt.Sprintf("analyze_mongo_log|%s|%d", logFilePath, thresholdMs)
	queryID := fmt.Sprintf("mongo_log_analyze_%d", time.Now().UnixNano())

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Str("log_file_path", logFilePath).
		Int64("threshold_ms", thresholdMs).
		Msg("MongoDB log analizi iÃ§in komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	})
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Str("query_id", queryID).
			Str("log_file_path", logFilePath).
			Msg("MongoDB log analizi sorgusu gÃ¶nderilemedi")
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("log_file_path", logFilePath).
		Str("query_id", queryID).
		Msg("MongoDB log analizi iÃ§in sorgu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			logger.Error().
				Str("query_id", queryID).
				Msg("Null sorgu sonucu alÄ±ndÄ±")
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Msg("Agent'tan yanÄ±t alÄ±ndÄ±")

		// Ã–nce struct olarak ayrÄ±ÅŸtÄ±rmayÄ± dene (Agent'Ä±n gÃ¶nderdiÄŸi tipte)
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Debug().
				Err(err).
				Str("query_id", queryID).
				Msg("Struct ayrÄ±ÅŸtÄ±rma hatasÄ±")

			// Struct ayrÄ±ÅŸtÄ±rma baÅŸarÄ±sÄ±z olursa, MongoLogAnalyzeResponse olarak dene
			var analyzeResponse pb.MongoLogAnalyzeResponse
			if err := result.Result.UnmarshalTo(&analyzeResponse); err != nil {
				logger.Error().
					Err(err).
					Str("query_id", queryID).
					Msg("MongoLogAnalyzeResponse ayrÄ±ÅŸtÄ±rma hatasÄ±")
				return nil, fmt.Errorf("sonuÃ§ ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
			}

			logger.Debug().
				Int("log_entries_count", len(analyzeResponse.LogEntries)).
				Str("query_id", queryID).
				Msg("DoÄŸrudan MongoLogAnalyzeResponse'a baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±")
			return &analyzeResponse, nil
		}

		// Sonucun iÃ§eriÄŸini logla		// Struct'tan MongoLogAnalyzeResponse oluÅŸtur
		logEntries := make([]*pb.MongoLogEntry, 0)
		entriesValue, ok := resultStruct.Fields["log_entries"]
		if ok && entriesValue != nil && entriesValue.GetListValue() != nil {
			for _, entryValue := range entriesValue.GetListValue().Values {
				if entryValue.GetStructValue() != nil {
					entryStruct := entryValue.GetStructValue()

					// Log giriÅŸ deÄŸerlerini al
					timestamp := int64(entryStruct.Fields["timestamp"].GetNumberValue())
					severity := entryStruct.Fields["severity"].GetStringValue() // String olarak al
					component := entryStruct.Fields["component"].GetStringValue()
					context := entryStruct.Fields["context"].GetStringValue()
					message := entryStruct.Fields["message"].GetStringValue()
					dbName := entryStruct.Fields["db_name"].GetStringValue()
					durationMillis := int64(entryStruct.Fields["duration_millis"].GetNumberValue())
					command := entryStruct.Fields["command"].GetStringValue()
					planSummary := entryStruct.Fields["plan_summary"].GetStringValue()
					namespace := entryStruct.Fields["namespace"].GetStringValue()

					// MongoLogEntry oluÅŸtur
					logEntry := &pb.MongoLogEntry{
						Timestamp:      timestamp,
						Severity:       severity, // String olarak kullan
						Component:      component,
						Context:        context,
						Message:        message,
						DbName:         dbName,
						DurationMillis: durationMillis,
						Command:        command,
						PlanSummary:    planSummary,
						Namespace:      namespace,
					}

					logEntries = append(logEntries, logEntry)
				}
			}
		}

		logger.Debug().
			Int("log_entries_count", len(logEntries)).
			Str("query_id", queryID).
			Msg("Struct'tan oluÅŸturulan log giriÅŸleri sayÄ±sÄ±")

		return &pb.MongoLogAnalyzeResponse{
			LogEntries: logEntries,
		}, nil

	case <-ctx.Done():
		// Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±
		logger.Warn().
			Str("query_id", queryID).
			Msg("Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
		return nil, ctx.Err()
	}
}

// sendPostgresLogListQuery, agent'a PostgreSQL log dosyalarÄ±nÄ± listelemesi iÃ§in sorgu gÃ¶nderir
func (s *Server) sendPostgresLogListQuery(ctx context.Context, agentID string) (*pb.PostgresLogListResponse, error) {
	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// PostgreSQL log dosyalarÄ±nÄ± listeleyen bir komut oluÅŸtur
	command := "list_postgres_logs"
	queryID := fmt.Sprintf("postgres_log_list_%d", time.Now().UnixNano())

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("PostgreSQL log listesi iÃ§in komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	if err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	}); err != nil {
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("PostgreSQL log dosyalarÄ± iÃ§in sorgu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Str("agent_id", agentID).
			Msg("PostgreSQL log list query yanÄ±t alÄ±ndÄ±")

		// Ã–nce struct olarak ayrÄ±ÅŸtÄ±rmayÄ± dene (Agent'Ä±n gÃ¶nderdiÄŸi tipte)
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Debug().
				Err(err).
				Str("query_id", queryID).
				Str("agent_id", agentID).
				Msg("PostgreSQL log list struct ayrÄ±ÅŸtÄ±rma hatasÄ±")

			// Struct ayrÄ±ÅŸtÄ±rma baÅŸarÄ±sÄ±z olursa, PostgresLogListResponse olarak dene
			var logListResponse pb.PostgresLogListResponse
			if err := result.Result.UnmarshalTo(&logListResponse); err != nil {
				logger.Error().
					Err(err).
					Str("query_id", queryID).
					Msg("PostgresLogListResponse ayrÄ±ÅŸtÄ±rma hatasÄ±")
				return nil, fmt.Errorf("sonuÃ§ ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
			}

			logger.Debug().
				Int("file_count", len(logListResponse.LogFiles)).
				Str("query_id", queryID).
				Msg("DoÄŸrudan PostgresLogListResponse'a baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±")
			return &logListResponse, nil
		}

		// Sonucun iÃ§eriÄŸini logla
		structBytes, _ := json.Marshal(resultStruct.AsMap())
		logger.Debug().
			Str("query_id", queryID).
			RawJSON("struct_content", structBytes).
			Msg("PostgreSQL log struct iÃ§eriÄŸi")

		// Struct'tan PostgresLogListResponse oluÅŸtur
		logFiles := make([]*pb.PostgresLogFile, 0)
		filesValue, ok := resultStruct.Fields["log_files"]
		if ok && filesValue != nil && filesValue.GetListValue() != nil {
			for _, fileValue := range filesValue.GetListValue().Values {
				if fileValue.GetStructValue() != nil {
					fileStruct := fileValue.GetStructValue()

					// Dosya deÄŸerlerini al
					nameValue := fileStruct.Fields["name"].GetStringValue()
					pathValue := fileStruct.Fields["path"].GetStringValue()
					sizeValue := int64(fileStruct.Fields["size"].GetNumberValue())
					lastModifiedValue := int64(fileStruct.Fields["last_modified"].GetNumberValue())

					// PostgresLogFile oluÅŸtur
					logFile := &pb.PostgresLogFile{
						Name:         nameValue,
						Path:         pathValue,
						Size:         sizeValue,
						LastModified: lastModifiedValue,
					}

					logFiles = append(logFiles, logFile)
				}
			}
		}

		logger.Debug().
			Int("file_count", len(logFiles)).
			Str("query_id", queryID).
			Msg("Struct'tan oluÅŸturulan log dosyalarÄ± sayÄ±sÄ±")

		return &pb.PostgresLogListResponse{
			LogFiles: logFiles,
		}, nil

	case <-ctx.Done():
		// Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±
		return nil, ctx.Err()
	}
}

// ListPostgresLogs, belirtilen agent'tan PostgreSQL log dosyalarÄ±nÄ± listeler
func (s *Server) ListPostgresLogs(ctx context.Context, req *pb.PostgresLogListRequest) (*pb.PostgresLogListResponse, error) {
	logger.Info().Msg("ListPostgresLogs Ã§aÄŸrÄ±ldÄ±")

	// Agent ID'yi Ã¶nce metadata'dan almayÄ± dene
	md, ok := metadata.FromIncomingContext(ctx)
	if ok {
		agentIDValues := md.Get("agent-id")
		if len(agentIDValues) > 0 {
			logger.Debug().
				Str("agent_id", agentIDValues[0]).
				Msg("Metadata'dan agent ID alÄ±ndÄ±")
			// Metadata'dan gelen agent ID'yi kullan
			agentID := agentIDValues[0]

			// Agent'a istek gÃ¶nder ve sonucu al
			response, err := s.sendPostgresLogListQuery(ctx, agentID)
			if err != nil {
				logger.Error().
					Err(err).
					Str("agent_id", agentID).
					Msg("PostgreSQL log dosyalarÄ± listelenirken hata")

				// Daha aÃ§Ä±klayÄ±cÄ± hata mesajlarÄ± iÃ§in gRPC status kodlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
				if strings.Contains(err.Error(), "agent bulunamadÄ±") {
					return nil, status.Errorf(codes.NotFound, "Agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
				} else if err == context.DeadlineExceeded {
					return nil, status.Errorf(codes.DeadlineExceeded, "Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
				}

				return nil, status.Errorf(codes.Internal, "PostgreSQL log dosyalarÄ± listelenirken bir hata oluÅŸtu: %v", err)
			}

			logger.Info().
				Str("agent_id", agentID).
				Int("file_count", len(response.LogFiles)).
				Msg("PostgreSQL log dosyalarÄ± baÅŸarÄ±yla listelendi")
			return response, nil
		}
	}

	// Metadata'dan alÄ±namadÄ±ysa, context'ten almayÄ± dene
	agentID := ""
	queryCtx, ok := ctx.Value("agent_id").(string)
	if ok && queryCtx != "" {
		agentID = queryCtx
	}

	if agentID == "" {
		return nil, status.Errorf(codes.InvalidArgument, "Agent ID belirtilmedi")
	}

	// Agent'a istek gÃ¶nder ve sonucu al
	response, err := s.sendPostgresLogListQuery(ctx, agentID)
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Msg("PostgreSQL log dosyalarÄ± listelenirken hata")

		// Daha aÃ§Ä±klayÄ±cÄ± hata mesajlarÄ± iÃ§in gRPC status kodlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
		if strings.Contains(err.Error(), "agent bulunamadÄ±") {
			return nil, status.Errorf(codes.NotFound, "Agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
		} else if err == context.DeadlineExceeded {
			return nil, status.Errorf(codes.DeadlineExceeded, "Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
		}

		return nil, status.Errorf(codes.Internal, "PostgreSQL log dosyalarÄ± listelenirken bir hata oluÅŸtu: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Int("file_count", len(response.LogFiles)).
		Msg("PostgreSQL log dosyalarÄ± baÅŸarÄ±yla listelendi")
	return response, nil
}

// AnalyzePostgresLog, belirtilen PostgreSQL log dosyasÄ±nÄ± analiz eder
func (s *Server) AnalyzePostgresLog(ctx context.Context, req *pb.PostgresLogAnalyzeRequest) (*pb.PostgresLogAnalyzeResponse, error) {
	// Agent ID'yi context'ten al
	agentID := req.AgentId
	if agentID == "" {
		return nil, fmt.Errorf("agent_id gerekli")
	}

	// Log dosya yolunu kontrol et
	if req.LogFilePath == "" {
		return nil, fmt.Errorf("log_file_path gerekli")
	}

	// VarsayÄ±lan threshold deÄŸerini ayarla
	thresholdMs := req.SlowQueryThresholdMs
	if thresholdMs <= 0 {
		thresholdMs = 1000 // VarsayÄ±lan 1 saniye
	}

	// PostgreSQL log analizi isteÄŸini gÃ¶nder
	return s.sendPostgresLogAnalyzeQuery(ctx, agentID, req.LogFilePath, thresholdMs)
}

// sendPostgresLogAnalyzeQuery, agent'a PostgreSQL log dosyasÄ±nÄ± analiz etmesi iÃ§in sorgu gÃ¶nderir
func (s *Server) sendPostgresLogAnalyzeQuery(ctx context.Context, agentID, logFilePath string, thresholdMs int64) (*pb.PostgresLogAnalyzeResponse, error) {
	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// logFilePath boÅŸ mu kontrol et
	if logFilePath == "" {
		return nil, fmt.Errorf("log dosya yolu boÅŸ olamaz")
	}

	// PostgreSQL log analizi iÃ§in bir komut oluÅŸtur
	command := fmt.Sprintf("analyze_postgres_log|%s|%d", logFilePath, thresholdMs)
	queryID := fmt.Sprintf("postgres_log_analyze_%d", time.Now().UnixNano())

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Str("log_file_path", logFilePath).
		Int64("threshold_ms", thresholdMs).
		Msg("PostgreSQL log analizi iÃ§in komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	})
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Str("query_id", queryID).
			Str("log_file_path", logFilePath).
			Msg("PostgreSQL log analizi sorgusu gÃ¶nderilemedi")
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("log_file_path", logFilePath).
		Str("query_id", queryID).
		Msg("PostgreSQL log analizi iÃ§in sorgu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			logger.Error().
				Str("query_id", queryID).
				Msg("Null sorgu sonucu alÄ±ndÄ±")
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Msg("Agent'tan yanÄ±t alÄ±ndÄ±")

		// Ã–nce struct olarak ayrÄ±ÅŸtÄ±rmayÄ± dene (Agent'Ä±n gÃ¶nderdiÄŸi tipte)
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Debug().
				Err(err).
				Str("query_id", queryID).
				Msg("Struct ayrÄ±ÅŸtÄ±rma hatasÄ±")

			// Struct ayrÄ±ÅŸtÄ±rma baÅŸarÄ±sÄ±z olursa, PostgresLogAnalyzeResponse olarak dene
			var analyzeResponse pb.PostgresLogAnalyzeResponse
			if err := result.Result.UnmarshalTo(&analyzeResponse); err != nil {
				logger.Error().
					Err(err).
					Str("query_id", queryID).
					Msg("PostgresLogAnalyzeResponse ayrÄ±ÅŸtÄ±rma hatasÄ±")
				return nil, fmt.Errorf("sonuÃ§ ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
			}

			logger.Debug().
				Int("log_entries_count", len(analyzeResponse.LogEntries)).
				Str("query_id", queryID).
				Msg("DoÄŸrudan PostgresLogAnalyzeResponse'a baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±")
			return &analyzeResponse, nil
		}

		// Sonucun iÃ§eriÄŸini logla
		json.Marshal(resultStruct.AsMap())

		// Struct'tan PostgresLogAnalyzeResponse oluÅŸtur
		logEntries := make([]*pb.PostgresLogEntry, 0)
		entriesValue, ok := resultStruct.Fields["log_entries"]
		if ok && entriesValue != nil && entriesValue.GetListValue() != nil {
			for _, entryValue := range entriesValue.GetListValue().Values {
				if entryValue.GetStructValue() != nil {
					entryStruct := entryValue.GetStructValue()

					// Log giriÅŸ deÄŸerlerini al
					timestamp := int64(entryStruct.Fields["timestamp"].GetNumberValue())
					logLevel := entryStruct.Fields["log_level"].GetStringValue()
					userName := entryStruct.Fields["user_name"].GetStringValue()
					database := entryStruct.Fields["database"].GetStringValue()
					processId := entryStruct.Fields["process_id"].GetStringValue()
					connectionFrom := entryStruct.Fields["connection_from"].GetStringValue()
					sessionId := entryStruct.Fields["session_id"].GetStringValue()
					sessionLineNum := entryStruct.Fields["session_line_num"].GetStringValue()
					commandTag := entryStruct.Fields["command_tag"].GetStringValue()
					sessionStartTime := entryStruct.Fields["session_start_time"].GetStringValue()
					virtualTransactionId := entryStruct.Fields["virtual_transaction_id"].GetStringValue()
					transactionId := entryStruct.Fields["transaction_id"].GetStringValue()
					errorSeverity := entryStruct.Fields["error_severity"].GetStringValue()
					sqlStateCode := entryStruct.Fields["sql_state_code"].GetStringValue()
					message := entryStruct.Fields["message"].GetStringValue()
					detail := entryStruct.Fields["detail"].GetStringValue()
					hint := entryStruct.Fields["hint"].GetStringValue()
					internalQuery := entryStruct.Fields["internal_query"].GetStringValue()
					durationMs := int64(entryStruct.Fields["duration_ms"].GetNumberValue())

					// PostgresLogEntry oluÅŸtur
					logEntry := &pb.PostgresLogEntry{
						Timestamp:            timestamp,
						LogLevel:             logLevel,
						UserName:             userName,
						Database:             database,
						ProcessId:            processId,
						ConnectionFrom:       connectionFrom,
						SessionId:            sessionId,
						SessionLineNum:       sessionLineNum,
						CommandTag:           commandTag,
						SessionStartTime:     sessionStartTime,
						VirtualTransactionId: virtualTransactionId,
						TransactionId:        transactionId,
						ErrorSeverity:        errorSeverity,
						SqlStateCode:         sqlStateCode,
						Message:              message,
						Detail:               detail,
						Hint:                 hint,
						InternalQuery:        internalQuery,
						DurationMs:           durationMs,
					}

					logEntries = append(logEntries, logEntry)
				}
			}
		}

		logger.Debug().
			Int("log_entries_count", len(logEntries)).
			Str("query_id", queryID).
			Str("agent_id", agentID).
			Msg("PostgreSQL log analizi struct'tan oluÅŸturuldu")

		return &pb.PostgresLogAnalyzeResponse{
			LogEntries: logEntries,
		}, nil

	case <-ctx.Done():
		// Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±
		logger.Error().
			Err(ctx.Err()).
			Str("query_id", queryID).
			Str("agent_id", agentID).
			Msg("PostgreSQL log analizi context timeout")
		return nil, ctx.Err()
	}
}

// GetAlarms, veritabanÄ±ndan alarm kayÄ±tlarÄ±nÄ± Ã§eker
func (s *Server) GetAlarms(ctx context.Context, onlyUnacknowledged bool, limit, offset int, severityFilter, metricFilter string, dateFrom, dateTo *time.Time) ([]map[string]interface{}, int64, error) {
	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return nil, 0, fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// VarsayÄ±lan limit kontrolÃ¼
	if limit <= 0 || limit > 1000 {
		limit = 250 // VarsayÄ±lan 250 kayÄ±t
	}
	if offset < 0 {
		offset = 0
	}

	// WHERE koÅŸullarÄ±nÄ± ve parametreleri hazÄ±rla
	var whereConditions []string
	var queryParams []interface{}
	paramIndex := 1

	// Sadece acknowledge edilmemiÅŸ alarmlarÄ± getir
	if onlyUnacknowledged {
		whereConditions = append(whereConditions, fmt.Sprintf("acknowledged = $%d", paramIndex))
		queryParams = append(queryParams, false)
		paramIndex++
	}

	// Severity filtresi
	if severityFilter != "" {
		whereConditions = append(whereConditions, fmt.Sprintf("severity = $%d", paramIndex))
		queryParams = append(queryParams, severityFilter)
		paramIndex++
	}

	// Metric name filtresi
	if metricFilter != "" {
		whereConditions = append(whereConditions, fmt.Sprintf("metric_name = $%d", paramIndex))
		queryParams = append(queryParams, metricFilter)
		paramIndex++
	}

	// Tarih aralÄ±ÄŸÄ± filtresi
	if dateFrom != nil {
		whereConditions = append(whereConditions, fmt.Sprintf("created_at >= $%d", paramIndex))
		queryParams = append(queryParams, *dateFrom)
		paramIndex++
	}
	if dateTo != nil {
		whereConditions = append(whereConditions, fmt.Sprintf("created_at <= $%d", paramIndex))
		queryParams = append(queryParams, *dateTo)
		paramIndex++
	}

	// WHERE clause oluÅŸtur
	whereClause := ""
	if len(whereConditions) > 0 {
		whereClause = " WHERE " + strings.Join(whereConditions, " AND ")
	}

	// Ã–nce toplam kayÄ±t sayÄ±sÄ±nÄ± al (count query)
	countQuery := "SELECT COUNT(*) FROM alarms" + whereClause
	var totalCount int64
	err := s.db.QueryRowContext(ctx, countQuery, queryParams...).Scan(&totalCount)
	if err != nil {
		return nil, 0, fmt.Errorf("toplam alarm sayÄ±sÄ± alÄ±namadÄ±: %v", err)
	}

	// Ana SQL sorgusu
	query := `
		SELECT 
			alarm_id,
			event_id,
			agent_id,
			status,
			metric_name,
			metric_value,
			message,
			severity,
			created_at,
			acknowledged,
			database
		FROM alarms` + whereClause + `
		ORDER BY created_at DESC
		LIMIT $` + fmt.Sprintf("%d", paramIndex) + ` OFFSET $` + fmt.Sprintf("%d", paramIndex+1)

	// LIMIT ve OFFSET parametrelerini ekle
	queryParams = append(queryParams, limit, offset)

	logger.Debug().
		Int("limit", limit).
		Int("offset", offset).
		Int64("total", totalCount).
		Str("severity_filter", severityFilter).
		Str("metric_filter", metricFilter).
		Msg("Alarm sorgusu")

	// Sorguyu Ã§alÄ±ÅŸtÄ±r
	rows, err := s.db.QueryContext(ctx, query, queryParams...)
	if err != nil {
		return nil, 0, fmt.Errorf("alarm verileri Ã§ekilemedi: %v", err)
	}
	defer rows.Close()

	// SonuÃ§larÄ± topla
	alarms := make([]map[string]interface{}, 0)
	for rows.Next() {
		var (
			alarmID      string
			eventID      string
			agentID      string
			status       string
			metricName   string
			metricValue  string
			message      string
			severity     string
			createdAt    time.Time
			acknowledged bool
			database     sql.NullString // NULL deÄŸerleri kabul edebilmesi iÃ§in NullString kullanÄ±yoruz
		)

		// SatÄ±rÄ± oku
		err := rows.Scan(
			&alarmID,
			&eventID,
			&agentID,
			&status,
			&metricName,
			&metricValue,
			&message,
			&severity,
			&createdAt,
			&acknowledged,
			&database,
		)
		if err != nil {
			return nil, 0, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
		}

		// Database deÄŸerini kontrolle ekle
		var databaseValue string
		if database.Valid {
			databaseValue = database.String
		} else {
			databaseValue = "" // EÄŸer NULL ise boÅŸ string atanÄ±r
		}

		// Her alarmÄ± map olarak oluÅŸtur
		alarm := map[string]interface{}{
			"alarm_id":     alarmID,
			"event_id":     eventID,
			"agent_id":     agentID,
			"status":       status,
			"metric_name":  metricName,
			"metric_value": metricValue,
			"message":      message,
			"severity":     severity,
			"created_at":   createdAt.Format(time.RFC3339),
			"acknowledged": acknowledged,
			"database":     databaseValue,
		}

		alarms = append(alarms, alarm)
	}

	// SatÄ±r okuma hatasÄ± kontrolÃ¼
	if err = rows.Err(); err != nil {
		return nil, 0, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
	}

	return alarms, totalCount, nil
}

// GetAlarmsStatus, alarm verilerini dÃ¶ndÃ¼rÃ¼r
func (s *Server) GetAlarmsStatus(ctx context.Context, _ *structpb.Struct) (*structpb.Value, error) {
	// AlarmlarÄ± getir - Son 100 alarmÄ± al
	alarms, _, err := s.GetAlarms(ctx, false, 100, 0, "", "", nil, nil)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "Alarm verileri alÄ±namadÄ±: %v", err)
	}

	// JSON verisini structpb.Value'ya dÃ¶nÃ¼ÅŸtÃ¼r
	value, err := structpb.NewValue(alarms)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "Veri dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
	}

	return value, nil
}

// AcknowledgeAlarm, belirtilen event_id'ye sahip alarmÄ± acknowledge eder
func (s *Server) AcknowledgeAlarm(ctx context.Context, eventID string) error {
	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// SQL sorgusu
	query := `
		UPDATE alarms 
		SET acknowledged = true 
		WHERE event_id = $1
	`

	result, err := s.db.ExecContext(ctx, query, eventID)
	if err != nil {
		return fmt.Errorf("alarm gÃ¼ncellenemedi: %v", err)
	}

	// Etkilenen satÄ±r sayÄ±sÄ±nÄ± kontrol et
	rowsAffected, err := result.RowsAffected()
	if err != nil {
		return fmt.Errorf("etkilenen satÄ±r sayÄ±sÄ± alÄ±namadÄ±: %v", err)
	}

	if rowsAffected == 0 {
		return fmt.Errorf("belirtilen event_id ile alarm bulunamadÄ±: %s", eventID)
	}

	return nil
}

// ReadPostgresConfig, belirtilen PostgreSQL config dosyasÄ±nÄ± okur
func (s *Server) ReadPostgresConfig(ctx context.Context, req *pb.PostgresConfigRequest) (*pb.PostgresConfigResponse, error) {
	// Agent ID'yi kontrol et
	agentID := req.AgentId
	if agentID == "" {
		return nil, fmt.Errorf("agent_id gerekli")
	}

	// Config dosya yolunu kontrol et
	configPath := req.ConfigPath
	if configPath == "" {
		return nil, fmt.Errorf("config_path gerekli")
	}

	// Config dosyasÄ±nÄ± okuma isteÄŸini gÃ¶nder
	return s.sendPostgresConfigQuery(ctx, agentID, configPath)
}

// sendPostgresConfigQuery, agent'a PostgreSQL config dosyasÄ±nÄ± okumasÄ± iÃ§in sorgu gÃ¶nderir
func (s *Server) sendPostgresConfigQuery(ctx context.Context, agentID, configPath string) (*pb.PostgresConfigResponse, error) {
	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// configPath boÅŸ mu kontrol et
	if configPath == "" {
		return nil, fmt.Errorf("config dosya yolu boÅŸ olamaz")
	}

	// PostgreSQL config okumasÄ± iÃ§in bir komut oluÅŸtur
	command := fmt.Sprintf("read_postgres_config|%s", configPath)
	queryID := fmt.Sprintf("postgres_config_%d", time.Now().UnixNano())

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Str("config_path", configPath).
		Msg("PostgreSQL config okumasÄ± iÃ§in komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	})
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Str("query_id", queryID).
			Str("config_path", configPath).
			Msg("PostgreSQL config sorgusu gÃ¶nderilemedi")
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("config_path", configPath).
		Str("query_id", queryID).
		Msg("PostgreSQL config iÃ§in sorgu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			logger.Error().
				Str("query_id", queryID).
				Str("agent_id", agentID).
				Str("config_path", configPath).
				Msg("PostgreSQL config query null sonuÃ§")
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Str("agent_id", agentID).
			Msg("PostgreSQL config query yanÄ±t alÄ±ndÄ±")

		// Ã–nce struct olarak ayrÄ±ÅŸtÄ±rmayÄ± dene (Agent'Ä±n gÃ¶nderdiÄŸi tipte)
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Debug().
				Err(err).
				Str("query_id", queryID).
				Str("agent_id", agentID).
				Msg("PostgreSQL config struct ayrÄ±ÅŸtÄ±rma hatasÄ±")

			// Struct ayrÄ±ÅŸtÄ±rma baÅŸarÄ±sÄ±z olursa, PostgresConfigResponse olarak dene
			var configResponse pb.PostgresConfigResponse
			if err := result.Result.UnmarshalTo(&configResponse); err != nil {
				logger.Error().
					Err(err).
					Str("query_id", queryID).
					Msg("PostgresConfigResponse ayrÄ±ÅŸtÄ±rma hatasÄ±")
				return nil, fmt.Errorf("sonuÃ§ ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
			}

			logger.Debug().
				Int("config_entries_count", len(configResponse.Configurations)).
				Str("query_id", queryID).
				Msg("DoÄŸrudan PostgresConfigResponse'a baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±")
			return &configResponse, nil
		}

		// Struct'tan PostgresConfigResponse oluÅŸtur
		configEntries := make([]*pb.PostgresConfigEntry, 0)
		entriesValue, ok := resultStruct.Fields["configurations"]
		if ok && entriesValue != nil && entriesValue.GetListValue() != nil {
			for _, entryValue := range entriesValue.GetListValue().Values {
				if entryValue.GetStructValue() != nil {
					entryStruct := entryValue.GetStructValue()

					// Config giriÅŸ deÄŸerlerini al
					parameter := entryStruct.Fields["parameter"].GetStringValue()
					value := entryStruct.Fields["value"].GetStringValue()
					description := entryStruct.Fields["description"].GetStringValue()
					isDefault := entryStruct.Fields["is_default"].GetBoolValue()
					category := entryStruct.Fields["category"].GetStringValue()

					// PostgresConfigEntry oluÅŸtur
					configEntry := &pb.PostgresConfigEntry{
						Parameter:   parameter,
						Value:       value,
						Description: description,
						IsDefault:   isDefault,
						Category:    category,
					}

					configEntries = append(configEntries, configEntry)
				}
			}
		}

		logger.Debug().
			Int("config_entries_count", len(configEntries)).
			Str("query_id", queryID).
			Str("agent_id", agentID).
			Msg("PostgreSQL config struct'tan oluÅŸturuldu")

		// Config dosya yolunu al
		configPathValue := ""
		if pathValue, exists := resultStruct.Fields["config_path"]; exists {
			configPathValue = pathValue.GetStringValue()
		}

		return &pb.PostgresConfigResponse{
			Status:         "success",
			ConfigPath:     configPathValue,
			Configurations: configEntries,
		}, nil

	case <-ctx.Done():
		// Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±
		logger.Error().
			Err(ctx.Err()).
			Str("query_id", queryID).
			Str("agent_id", agentID).
			Str("config_path", configPath).
			Msg("PostgreSQL config query context timeout")
		return nil, ctx.Err()
	}
}

// ReportVersion, agent'Ä±n versiyon bilgilerini iÅŸler
func (s *Server) ReportVersion(ctx context.Context, req *pb.ReportVersionRequest) (*pb.ReportVersionResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Msg("ReportVersion metodu Ã§aÄŸrÄ±ldÄ±")

	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Msg("VeritabanÄ± baÄŸlantÄ± hatasÄ±")
		return &pb.ReportVersionResponse{
			Status: "error",
		}, fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// Versiyon bilgilerini logla
	versionInfo := req.VersionInfo
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("version", versionInfo.Version).
		Str("platform", versionInfo.Platform).
		Str("architecture", versionInfo.Architecture).
		Str("hostname", versionInfo.Hostname).
		Str("os_version", versionInfo.OsVersion).
		Str("go_version", versionInfo.GoVersion).
		Msg("Agent versiyon bilgileri alÄ±ndÄ±")

	// Versiyon bilgilerini veritabanÄ±na kaydet
	query := `
		INSERT INTO agent_versions (
			agent_id,
			version,
			platform,
			architecture,
			hostname,
			os_version,
			go_version,
			reported_at
		) VALUES ($1, $2, $3, $4, $5, $6, $7, CURRENT_TIMESTAMP)
		ON CONFLICT (agent_id) DO UPDATE SET
			version = EXCLUDED.version,
			platform = EXCLUDED.platform,
			architecture = EXCLUDED.architecture,
			hostname = EXCLUDED.hostname,
			os_version = EXCLUDED.os_version,
			go_version = EXCLUDED.go_version,
			reported_at = CURRENT_TIMESTAMP
	`

	_, err := s.db.ExecContext(ctx, query,
		req.AgentId,
		versionInfo.Version,
		versionInfo.Platform,
		versionInfo.Architecture,
		versionInfo.Hostname,
		versionInfo.OsVersion,
		versionInfo.GoVersion,
	)

	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Msg("Versiyon bilgileri kaydedilemedi")
		return &pb.ReportVersionResponse{
			Status: "error",
		}, fmt.Errorf("versiyon bilgileri kaydedilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", req.AgentId).
		Str("version", versionInfo.Version).
		Msg("Agent versiyon bilgileri baÅŸarÄ±yla kaydedildi")
	return &pb.ReportVersionResponse{
		Status: "success",
	}, nil
}

// GetAgentVersions, veritabanÄ±ndan agent versiyon bilgilerini Ã§eker
func (s *Server) GetAgentVersions(ctx context.Context) ([]map[string]interface{}, error) {
	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return nil, fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// SQL sorgusu
	query := `
		SELECT 
			agent_id,
			version,
			platform,
			architecture,
			hostname,
			os_version,
			go_version,
			reported_at,
			created_at,
			updated_at
		FROM agent_versions
		ORDER BY reported_at DESC
	`

	// Sorguyu Ã§alÄ±ÅŸtÄ±r
	rows, err := s.db.QueryContext(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("versiyon bilgileri Ã§ekilemedi: %v", err)
	}
	defer rows.Close()

	// SonuÃ§larÄ± topla
	versions := make([]map[string]interface{}, 0)
	for rows.Next() {
		var (
			agentID      string
			version      string
			platform     string
			architecture string
			hostname     string
			osVersion    string
			goVersion    string
			reportedAt   time.Time
			createdAt    time.Time
			updatedAt    time.Time
		)

		// SatÄ±rÄ± oku
		err := rows.Scan(
			&agentID,
			&version,
			&platform,
			&architecture,
			&hostname,
			&osVersion,
			&goVersion,
			&reportedAt,
			&createdAt,
			&updatedAt,
		)
		if err != nil {
			return nil, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
		}

		// Her versiyonu map olarak oluÅŸtur
		versionInfo := map[string]interface{}{
			"agent_id":     agentID,
			"version":      version,
			"platform":     platform,
			"architecture": architecture,
			"hostname":     hostname,
			"os_version":   osVersion,
			"go_version":   goVersion,
			"reported_at":  reportedAt.Format(time.RFC3339),
			"created_at":   createdAt.Format(time.RFC3339),
			"updated_at":   updatedAt.Format(time.RFC3339),
		}

		versions = append(versions, versionInfo)
	}

	// SatÄ±r okuma hatasÄ± kontrolÃ¼
	if err = rows.Err(); err != nil {
		return nil, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
	}

	return versions, nil
}

// PromoteMongoToPrimary, MongoDB node'unu primary'ye yÃ¼kseltir
func (s *Server) PromoteMongoToPrimary(ctx context.Context, req *pb.MongoPromotePrimaryRequest) (*pb.MongoPromotePrimaryResponse, error) {
	// Job oluÅŸtur
	job := &pb.Job{
		JobId:     req.JobId,
		Type:      pb.JobType_JOB_TYPE_MONGO_PROMOTE_PRIMARY,
		Status:    pb.JobStatus_JOB_STATUS_PENDING,
		AgentId:   req.AgentId,
		CreatedAt: timestamppb.Now(),
		UpdatedAt: timestamppb.Now(),
		Parameters: map[string]string{
			"node_hostname": req.NodeHostname,
			"port":          fmt.Sprintf("%d", req.Port),
			"replica_set":   req.ReplicaSet,
			"node_status":   req.NodeStatus, // Node status'Ä±nÄ± ekle
		},
	}

	// Job'Ä± kaydet
	s.jobMu.Lock()
	s.jobs[job.JobId] = job
	s.jobMu.Unlock()

	// Ä°lgili agent'Ä± bul
	s.mu.RLock()
	agent, exists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !exists {
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = "Agent bulunamadÄ±"
		job.UpdatedAt = timestamppb.Now()
		return &pb.MongoPromotePrimaryResponse{
			JobId:        job.JobId,
			Status:       job.Status,
			ErrorMessage: job.ErrorMessage,
		}, fmt.Errorf("agent bulunamadÄ±: %s", req.AgentId)
	}

	// Job'Ä± veritabanÄ±na kaydet
	if err := s.saveJobToDatabase(ctx, job); err != nil {
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = fmt.Sprintf("Job veritabanÄ±na kaydedilemedi: %v", err)
		job.UpdatedAt = timestamppb.Now()
		return &pb.MongoPromotePrimaryResponse{
			JobId:        job.JobId,
			Status:       job.Status,
			ErrorMessage: job.ErrorMessage,
		}, err
	}

	// Job'Ä± Ã§alÄ±ÅŸtÄ±r
	go func() {
		job.Status = pb.JobStatus_JOB_STATUS_RUNNING

		// MongoDB komutu oluÅŸtur - node_status'a gÃ¶re farklÄ± komut gÃ¶nder
		var command string
		if req.NodeStatus == "PRIMARY" {
			// EÄŸer node primary ise, primary'i step down yapalÄ±m
			command = "rs.stepDown()"
		} else {
			// Secondary node durumunda bir ÅŸey yapmaya gerek yok (freeze endpoint'i yoluyla hallolacak)
			command = fmt.Sprintf("db.hello() // Node %s secondary durumunda", req.NodeHostname)
		}

		// Agent'a promote isteÄŸi gÃ¶nder
		err := agent.Stream.Send(&pb.ServerMessage{
			Payload: &pb.ServerMessage_Query{
				Query: &pb.Query{
					QueryId: job.JobId,
					Command: command,
				},
			},
		})

		if err != nil {
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = fmt.Sprintf("Agent'a istek gÃ¶nderilemedi: %v", err)
		} else {
			job.Status = pb.JobStatus_JOB_STATUS_COMPLETED
			job.Result = "Primary promotion request sent successfully"
		}

		job.UpdatedAt = timestamppb.Now()
		s.updateJobInDatabase(context.Background(), job)
	}()

	return &pb.MongoPromotePrimaryResponse{
		JobId:  job.JobId,
		Status: pb.JobStatus_JOB_STATUS_PENDING,
	}, nil
}

// PromotePostgresToMaster, PostgreSQL node'unu master'a yÃ¼kseltir
//
// Yeni JSON formatÄ±:
//
//	{
//	  "agent_id": "agent_slave_hostname",
//	  "node_hostname": "slave_hostname",
//	  "data_directory": "/var/lib/postgresql/data",
//	  "current_master_host": "master_hostname",
//	  "current_master_ip": "master_ip_address",
//	  "slaves": [
//	    {"hostname": "other_slave1_hostname", "ip": "other_slave1_ip"},
//	    {"hostname": "other_slave2_hostname", "ip": "other_slave2_ip"}
//	  ]
//	}
//
// âœ… Protobuf gÃ¼ncellendi ve yeni alanlar eklendi:
// - CurrentMasterIp string
// - Slaves []*SlaveNode (SlaveNode: Hostname, Ip alanlarÄ± olan struct)
//
// NOT: Replication user/password bilgileri artÄ±k gÃ¼venlik nedeniyle agent config'inden alÄ±nacak
func (s *Server) PromotePostgresToMaster(ctx context.Context, req *pb.PostgresPromoteMasterRequest) (*pb.PostgresPromoteMasterResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("node_hostname", req.NodeHostname).
		Str("data_directory", req.DataDirectory).
		Str("current_master_host", req.CurrentMasterHost).
		Str("current_master_ip", req.CurrentMasterIp).
		Int("slave_count", len(req.Slaves)).
		Str("job_id", req.JobId).
		Msg("PromotePostgresToMaster Ã§aÄŸrÄ±ldÄ±")

	// DEBUG: Request iÃ§eriÄŸini detaylÄ± logla
	logger.Debug().
		Str("agent_id", req.AgentId).
		Str("node_hostname", req.NodeHostname).
		Str("data_directory", req.DataDirectory).
		Str("current_master_host", req.CurrentMasterHost).
		Str("current_master_ip", req.CurrentMasterIp).
		Int("slaves_count", len(req.Slaves)).
		Interface("slaves_raw", req.Slaves).
		Str("job_id", req.JobId).
		Msg("DEBUG: PromotePostgresToMaster request detaylarÄ±")

	// EXTRA DEBUG: Her slave'i ayrÄ± ayrÄ± logla
	for i, slave := range req.Slaves {
		logger.Debug().
			Int("slave_index", i).
			Str("slave_hostname", slave.Hostname).
			Str("slave_ip", slave.Ip).
			Msg("DEBUG: Individual slave node")
	}

	// EXTRA DEBUG: Request'in nil olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	if req.CurrentMasterIp == "" {
		logger.Warn().Msg("DEBUG: current_master_ip is EMPTY!")
	}
	if len(req.Slaves) == 0 {
		logger.Warn().Msg("DEBUG: slaves array is EMPTY!")
	}

	// Slave bilgilerini logla
	if len(req.Slaves) > 0 {
		for i, slave := range req.Slaves {
			logger.Debug().
				Int("slave_index", i).
				Str("slave_hostname", slave.Hostname).
				Str("slave_ip", slave.Ip).
				Msg("Other slave node")
		}
	}

	// Process log takibi iÃ§in metadata oluÅŸtur
	metadata := map[string]string{
		"node_hostname":       req.NodeHostname,
		"data_directory":      req.DataDirectory,
		"job_id":              req.JobId,
		"current_master_host": req.CurrentMasterHost,
		"current_master_ip":   req.CurrentMasterIp,
		"slave_count":         fmt.Sprintf("%d", len(req.Slaves)),
	}

	// Slave bilgilerini metadata'ya ekle
	for i, slave := range req.Slaves {
		metadata[fmt.Sprintf("slave_%d_hostname", i)] = slave.Hostname
		metadata[fmt.Sprintf("slave_%d_ip", i)] = slave.Ip
	}

	// Job oluÅŸtur
	job := &pb.Job{
		JobId:     req.JobId,
		Type:      pb.JobType_JOB_TYPE_POSTGRES_PROMOTE_MASTER,
		Status:    pb.JobStatus_JOB_STATUS_PENDING,
		AgentId:   req.AgentId,
		CreatedAt: timestamppb.Now(),
		UpdatedAt: timestamppb.Now(),
		Parameters: map[string]string{
			"node_hostname":       req.NodeHostname,
			"data_directory":      req.DataDirectory,
			"process_id":          req.JobId,                          // Process ID olarak job ID'yi kullan
			"current_master_host": req.CurrentMasterHost,              // Eski master bilgisi
			"current_master_ip":   req.CurrentMasterIp,                // Eski master IP bilgisi
			"slave_count":         fmt.Sprintf("%d", len(req.Slaves)), // DiÄŸer slave sayÄ±sÄ±
			// Replication user/password artÄ±k agent config'inden alÄ±nacak (gÃ¼venlik)
		},
	}

	// Slave bilgilerini job parameters'a ekle
	for i, slave := range req.Slaves {
		job.Parameters[fmt.Sprintf("slave_%d_hostname", i)] = slave.Hostname
		job.Parameters[fmt.Sprintf("slave_%d_ip", i)] = slave.Ip
	}

	// Job'Ä± kaydet
	s.jobMu.Lock()
	s.jobs[job.JobId] = job
	s.jobMu.Unlock()

	// Ä°lgili agent'Ä± bul
	s.mu.RLock()
	agent, exists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !exists {
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = "Agent bulunamadÄ±"
		job.UpdatedAt = timestamppb.Now()

		// Process log oluÅŸtur - hata durumu
		processLog := &pb.ProcessLogUpdate{
			AgentId:      req.AgentId,
			ProcessId:    req.JobId,
			ProcessType:  "postgresql_promotion",
			Status:       "failed",
			LogMessages:  []string{"Agent bulunamadÄ±, iÅŸlem baÅŸlatÄ±lamadÄ±"},
			ElapsedTimeS: 0,
			UpdatedAt:    time.Now().Format(time.RFC3339),
			Metadata:     metadata,
		}
		s.saveProcessLogs(ctx, processLog)

		return &pb.PostgresPromoteMasterResponse{
			JobId:        job.JobId,
			Status:       job.Status,
			ErrorMessage: job.ErrorMessage,
		}, fmt.Errorf("agent bulunamadÄ±: %s", req.AgentId)
	}

	// Job'Ä± veritabanÄ±na kaydet
	if err := s.saveJobToDatabase(ctx, job); err != nil {
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = fmt.Sprintf("Job veritabanÄ±na kaydedilemedi: %v", err)
		job.UpdatedAt = timestamppb.Now()

		// Process log oluÅŸtur - veritabanÄ± hatasÄ±
		processLog := &pb.ProcessLogUpdate{
			AgentId:      req.AgentId,
			ProcessId:    req.JobId,
			ProcessType:  "postgresql_promotion",
			Status:       "failed",
			LogMessages:  []string{fmt.Sprintf("Job veritabanÄ±na kaydedilemedi: %v", err)},
			ElapsedTimeS: 0,
			UpdatedAt:    time.Now().Format(time.RFC3339),
			Metadata:     metadata,
		}
		s.saveProcessLogs(ctx, processLog)

		return &pb.PostgresPromoteMasterResponse{
			JobId:        job.JobId,
			Status:       job.Status,
			ErrorMessage: job.ErrorMessage,
		}, err
	}

	// Process log baÅŸlangÄ±Ã§ kaydÄ± oluÅŸtur
	startLog := &pb.ProcessLogUpdate{
		AgentId:      req.AgentId,
		ProcessId:    req.JobId,
		ProcessType:  "postgresql_promotion",
		Status:       "running",
		LogMessages:  []string{fmt.Sprintf("PostgreSQL promotion iÅŸlemi baÅŸlatÄ±lÄ±yor - Node: %s", req.NodeHostname)},
		ElapsedTimeS: 0,
		UpdatedAt:    time.Now().Format(time.RFC3339),
		Metadata:     metadata,
	}
	s.saveProcessLogs(ctx, startLog)

	// Job'Ä± Ã§alÄ±ÅŸtÄ±r
	go func() {
		job.Status = pb.JobStatus_JOB_STATUS_RUNNING
		job.UpdatedAt = timestamppb.Now()

		// ðŸ”§ FIX: Database update iÃ§in timeout context kullan
		dbCtx, dbCancel := context.WithTimeout(context.Background(), 5*time.Second)
		s.updateJobInDatabase(dbCtx, job)
		dbCancel()

		// Yeni komut formatÄ±: "postgres_promote|data_dir|process_id|old_master_host|old_master_ip|slave_count|slaves_info"
		// Agent tarafÄ±ndaki ProcessLogger'Ä± etkinleÅŸtirmek iÃ§in process_id gÃ¶nderiyoruz
		// Eski master bilgisini ve diÄŸer slave bilgilerini de gÃ¶nderiyoruz (koordinasyon iÃ§in)
		// Agent kendi config'inden replication bilgilerini alacak

		// Slave bilgilerini string formatÄ±na Ã§evir
		slave_info := ""
		if len(req.Slaves) > 0 {
			var slaveInfos []string
			for _, slave := range req.Slaves {
				slaveInfos = append(slaveInfos, fmt.Sprintf("%s:%s", slave.Hostname, slave.Ip))
			}
			slave_info = strings.Join(slaveInfos, ",")
		}

		// Tam komut formatÄ±: postgres_promote|data_dir|process_id|new_master_host|old_master_host|old_master_ip|slave_count|slaves_info
		command := fmt.Sprintf("postgres_promote|%s|%s|%s|%s|%s|%d|%s",
			req.DataDirectory, req.JobId, req.NodeHostname, req.CurrentMasterHost, req.CurrentMasterIp, len(req.Slaves), slave_info)

		// Slave bilgilerini ve komut detaylarÄ±nÄ± logla
		logger.Info().
			Str("job_id", req.JobId).
			Str("agent_id", req.AgentId).
			Str("new_master_host", req.NodeHostname).
			Str("old_master_host", req.CurrentMasterHost).
			Str("old_master_ip", req.CurrentMasterIp).
			Str("slave_info", slave_info).
			Int("slave_count", len(req.Slaves)).
			Str("command", command).
			Msg("PostgreSQL promotion komutu agent'a gÃ¶nderiliyor")

		// ðŸ”§ FIX: Timeout ile Stream.Send() Ã§aÄŸrÄ±sÄ±
		sendCtx, sendCancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer sendCancel()

		// Channel kullanarak timeout kontrollÃ¼ gÃ¶nderme
		sendDone := make(chan error, 1)
		go func() {
			err := agent.Stream.Send(&pb.ServerMessage{
				Payload: &pb.ServerMessage_Query{
					Query: &pb.Query{
						QueryId: job.JobId,
						Command: command,
					},
				},
			})
			sendDone <- err
		}()

		// Timeout veya baÅŸarÄ± durumunu bekle
		select {
		case err := <-sendDone:
			if err != nil {
				job.Status = pb.JobStatus_JOB_STATUS_FAILED
				job.ErrorMessage = fmt.Sprintf("Agent'a istek gÃ¶nderilemedi: %v", err)
				job.UpdatedAt = timestamppb.Now()

				// Database update iÃ§in timeout context
				dbCtx2, dbCancel2 := context.WithTimeout(context.Background(), 5*time.Second)
				s.updateJobInDatabase(dbCtx2, job)
				dbCancel2()

				// Hata durumu process log'u
				errorLog := &pb.ProcessLogUpdate{
					AgentId:      req.AgentId,
					ProcessId:    req.JobId,
					ProcessType:  "postgresql_promotion",
					Status:       "failed",
					LogMessages:  []string{fmt.Sprintf("Agent'a istek gÃ¶nderilemedi: %v", err)},
					ElapsedTimeS: 0,
					UpdatedAt:    time.Now().Format(time.RFC3339),
					Metadata:     metadata,
				}
				// Process logs iÃ§in de timeout context
				logCtx, logCancel := context.WithTimeout(context.Background(), 5*time.Second)
				s.saveProcessLogs(logCtx, errorLog)
				logCancel()
			} else {
				// BaÅŸarÄ±lÄ± baÅŸlatma durumu
				logger.Info().
					Str("job_id", job.JobId).
					Str("agent_id", req.AgentId).
					Str("node_hostname", req.NodeHostname).
					Msg("PostgreSQL promotion iÅŸlemi agent'a iletildi")

				// Job'Ä± IN_PROGRESS olarak iÅŸaretle
				// Agent ProcessLogger ile ilerleyiÅŸi bildirecek, burada bir ÅŸey yapmamÄ±za gerek yok

				// TamamlandÄ± olarak iÅŸaretleme iÅŸlemini artÄ±k agent tarafÄ±ndan gelen
				// son log mesajÄ± (completed statÃ¼sÃ¼nde) ile yapacaÄŸÄ±z.
			}
		case <-sendCtx.Done():
			logger.Error().
				Str("job_id", job.JobId).
				Str("agent_id", req.AgentId).
				Dur("timeout", 10*time.Second).
				Msg("Timeout: PostgreSQL promotion agent'a gÃ¶nderilemedi")
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = "Timeout: Agent'a istek gÃ¶nderilemedi (10s timeout)"
			job.UpdatedAt = timestamppb.Now()

			// Database update iÃ§in timeout context
			dbCtx3, dbCancel3 := context.WithTimeout(context.Background(), 5*time.Second)
			s.updateJobInDatabase(dbCtx3, job)
			dbCancel3()

			// Timeout durumu process log'u
			timeoutLog := &pb.ProcessLogUpdate{
				AgentId:      req.AgentId,
				ProcessId:    req.JobId,
				ProcessType:  "postgresql_promotion",
				Status:       "failed",
				LogMessages:  []string{"Timeout: Agent'a istek gÃ¶nderilemedi (10s timeout)"},
				ElapsedTimeS: 0,
				UpdatedAt:    time.Now().Format(time.RFC3339),
				Metadata:     metadata,
			}
			// Process logs iÃ§in de timeout context
			logCtx2, logCancel2 := context.WithTimeout(context.Background(), 5*time.Second)
			s.saveProcessLogs(logCtx2, timeoutLog)
			logCancel2()
		}
	}()

	return &pb.PostgresPromoteMasterResponse{
		JobId:  job.JobId,
		Status: pb.JobStatus_JOB_STATUS_PENDING,
	}, nil
}

// ConvertPostgresToSlave PostgreSQL master'Ä± slave'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
func (s *Server) ConvertPostgresToSlave(ctx context.Context, req *pb.ConvertPostgresToSlaveRequest) (*pb.ConvertPostgresToSlaveResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("node_hostname", req.NodeHostname).
		Str("new_master_host", req.NewMasterHost).
		Str("new_master_ip", req.NewMasterIp).
		Int32("new_master_port", req.NewMasterPort).
		Str("coordination_job_id", req.CoordinationJobId).
		Str("old_master_host", req.OldMasterHost).
		Str("job_id", req.JobId).
		Msg("ConvertPostgresToSlave Ã§aÄŸrÄ±ldÄ±")

	// Job oluÅŸtur
	job := &pb.Job{
		JobId:     req.JobId,
		Type:      pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE,
		Status:    pb.JobStatus_JOB_STATUS_PENDING,
		AgentId:   req.AgentId,
		CreatedAt: timestamppb.Now(),
		UpdatedAt: timestamppb.Now(),
		Parameters: map[string]string{
			"node_hostname":       req.NodeHostname,
			"new_master_host":     req.NewMasterHost,
			"new_master_ip":       req.NewMasterIp,
			"new_master_port":     fmt.Sprintf("%d", req.NewMasterPort),
			"data_directory":      req.DataDirectory,
			"coordination_job_id": req.CoordinationJobId,
			"old_master_host":     req.OldMasterHost,
			// replication_user KALDIRILDI - artÄ±k agent config'den okunacak
		},
	}

	// Agent'Ä± bul ve komut gÃ¶nder
	s.mu.RLock()
	agent, exists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !exists {
		return &pb.ConvertPostgresToSlaveResponse{
			JobId:        req.JobId,
			Status:       pb.JobStatus_JOB_STATUS_FAILED,
			ErrorMessage: "Agent bulunamadÄ±",
		}, fmt.Errorf("agent bulunamadÄ±: %s", req.AgentId)
	}

	// Komutu oluÅŸtur ve gÃ¶nder (yeni format)
	// NOT: Replication bilgileri artÄ±k agent config'inden alÄ±nacak (gÃ¼venlik)
	command := fmt.Sprintf("convert_postgres_to_slave|%s|%s|%d|%s|%s|%s",
		req.NewMasterHost,     // new_master_host
		req.NewMasterIp,       // new_master_ip (YENÄ°)
		req.NewMasterPort,     // new_master_port (INT)
		req.DataDirectory,     // data_dir
		req.CoordinationJobId, // coordination_job_id (opsiyonel)
		req.OldMasterHost)     // old_master_host (opsiyonel)

	// ðŸ”§ FIX: Timeout ile Stream.Send() Ã§aÄŸrÄ±sÄ±
	sendCtx, sendCancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer sendCancel()

	// Channel kullanarak timeout kontrollÃ¼ gÃ¶nderme
	sendDone := make(chan error, 1)
	go func() {
		err := agent.Stream.Send(&pb.ServerMessage{
			Payload: &pb.ServerMessage_Query{
				Query: &pb.Query{
					QueryId: req.JobId,
					Command: command,
				},
			},
		})
		sendDone <- err
	}()

	// Timeout veya baÅŸarÄ± durumunu bekle
	select {
	case err := <-sendDone:
		if err != nil {
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = fmt.Sprintf("Agent'a komut gÃ¶nderilemedi: %v", err)
		} else {
			job.Status = pb.JobStatus_JOB_STATUS_RUNNING
		}
	case <-sendCtx.Done():
		logger.Error().
			Str("job_id", req.JobId).
			Str("agent_id", req.AgentId).
			Dur("timeout", 10*time.Second).
			Msg("Timeout: ConvertPostgresToSlave agent'a gÃ¶nderilemedi")
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = "Timeout: Agent'a komut gÃ¶nderilemedi (10s timeout)"
	}

	job.UpdatedAt = timestamppb.Now()
	s.jobMu.Lock()
	s.jobs[job.JobId] = job
	s.jobMu.Unlock()

	return &pb.ConvertPostgresToSlaveResponse{
		JobId:        job.JobId,
		Status:       job.Status,
		ErrorMessage: job.ErrorMessage,
	}, nil
}

// GetJob, belirli bir job'Ä±n detaylarÄ±nÄ± getirir
func (s *Server) GetJob(ctx context.Context, req *pb.GetJobRequest) (*pb.GetJobResponse, error) {
	s.jobMu.RLock()
	job, exists := s.jobs[req.JobId]
	s.jobMu.RUnlock()

	if !exists {
		return nil, fmt.Errorf("job bulunamadÄ±: %s", req.JobId)
	}

	return &pb.GetJobResponse{
		Job: job,
	}, nil
}

// ListJobs, job listesini veritabanÄ±ndan doÄŸrudan sorgular ve getirir
func (s *Server) ListJobs(ctx context.Context, req *pb.ListJobsRequest) (*pb.ListJobsResponse, error) {
	logger.Debug().
		Str("agent_id", req.AgentId).
		Interface("status", req.Status).
		Interface("type", req.Type).
		Int32("limit", req.Limit).
		Int32("offset", req.Offset).
		Msg("ListJobs Ã§aÄŸrÄ±ldÄ±")

	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return nil, fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// Sorgu iÃ§in parametreleri hazÄ±rla
	var queryParams []interface{}
	var conditions []string

	// Temel sorgu oluÅŸtur
	baseQuery := `
		SELECT 
			job_id, type, status, agent_id, 
			created_at, updated_at, error_message, 
			parameters, result
		FROM jobs
	`

	// WHERE koÅŸullarÄ± oluÅŸtur
	paramCount := 1 // PostgreSQL parametreleri $1, $2, ... ÅŸeklinde baÅŸlar

	// Agent ID filtresi ekle
	if req.AgentId != "" {
		conditions = append(conditions, fmt.Sprintf("agent_id = $%d", paramCount))
		queryParams = append(queryParams, req.AgentId)
		paramCount++
	}

	// Status filtresi ekle
	if req.Status != pb.JobStatus_JOB_STATUS_UNKNOWN {
		conditions = append(conditions, fmt.Sprintf("status = $%d", paramCount))
		queryParams = append(queryParams, req.Status.String())
		paramCount++
	}

	// Type filtresi ekle
	if req.Type != pb.JobType_JOB_TYPE_UNKNOWN {
		conditions = append(conditions, fmt.Sprintf("type = $%d", paramCount))
		queryParams = append(queryParams, req.Type.String())
		paramCount++
	}

	// WHERE koÅŸullarÄ±nÄ± SQL sorgusuna ekle
	query := baseQuery
	if len(conditions) > 0 {
		query += " WHERE " + strings.Join(conditions, " AND ")
	}

	// Toplam kayÄ±t sayÄ±sÄ±nÄ± almak iÃ§in COUNT sorgusu
	countQuery := "SELECT COUNT(*) FROM jobs"
	if len(conditions) > 0 {
		countQuery += " WHERE " + strings.Join(conditions, " AND ")
	}

	// Ã–nce toplam kayÄ±t sayÄ±sÄ±nÄ± al
	var total int32
	err := s.db.QueryRowContext(ctx, countQuery, queryParams...).Scan(&total)
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Msg("Job sayÄ±sÄ± alÄ±namadÄ±")
		return nil, fmt.Errorf("job sayÄ±sÄ± alÄ±namadÄ±: %v", err)
	}
	logger.Debug().
		Int64("total_jobs", int64(total)).
		Msg("Filtrelere gÃ¶re toplam job sayÄ±sÄ±")

	// SÄ±ralama ve limit ekle
	query += " ORDER BY created_at DESC"

	// VarsayÄ±lan deÄŸerler
	limit := req.Limit
	if limit <= 0 {
		limit = 10 // VarsayÄ±lan limit
	}

	offset := req.Offset
	if offset < 0 {
		offset = 0
	}

	query += fmt.Sprintf(" LIMIT $%d OFFSET $%d", paramCount, paramCount+1)
	queryParams = append(queryParams, limit, offset)

	logger.Debug().
		Str("query", query).
		Interface("parameters", queryParams).
		Msg("SQL sorgusu hazÄ±rlandÄ±")

	// AsÄ±l sorguyu Ã§alÄ±ÅŸtÄ±r
	rows, err := s.db.QueryContext(ctx, query, queryParams...)
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Str("query", query).
			Interface("params", queryParams).
			Msg("Job sorgusu Ã§alÄ±ÅŸtÄ±rÄ±lamadÄ±")
		return nil, fmt.Errorf("job sorgusu Ã§alÄ±ÅŸtÄ±rÄ±lamadÄ±: %v", err)
	}
	defer rows.Close()

	// SonuÃ§larÄ± iÅŸle
	var jobs []*pb.Job
	for rows.Next() {
		var (
			jobID        string
			jobType      string
			jobStatus    string
			agentID      string
			createdAt    time.Time
			updatedAt    time.Time
			errorMessage sql.NullString
			parameters   []byte // JSON formatÄ±nda
			result       sql.NullString
		)

		// SatÄ±rÄ± oku
		if err := rows.Scan(
			&jobID, &jobType, &jobStatus, &agentID,
			&createdAt, &updatedAt, &errorMessage,
			&parameters, &result,
		); err != nil {
			logger.Error().
				Err(err).
				Msg("Job kaydÄ± okunamadÄ±")
			continue
		}

		// Parameters JSON'Ä± Ã§Ã¶zÃ¼mle
		var paramsMap map[string]string
		if err := json.Unmarshal(parameters, &paramsMap); err != nil {
			logger.Error().
				Err(err).
				Str("job_id", jobID).
				Msg("Job parametreleri Ã§Ã¶zÃ¼mlenemedi")
			paramsMap = make(map[string]string) // BoÅŸ map kullan
		}

		// JobType ve JobStatus enum deÄŸerlerini Ã§Ã¶zÃ¼mle
		jobTypeEnum, ok := pb.JobType_value[jobType]
		if !ok {
			logger.Warn().
				Str("job_type", jobType).
				Str("job_id", jobID).
				Msg("Bilinmeyen job tipi, varsayÄ±lan olarak JOB_TYPE_UNKNOWN kullanÄ±lÄ±yor")
			jobTypeEnum = int32(pb.JobType_JOB_TYPE_UNKNOWN)
		}

		jobStatusEnum, ok := pb.JobStatus_value[jobStatus]
		if !ok {
			logger.Warn().
				Str("job_status", jobStatus).
				Str("job_id", jobID).
				Msg("Bilinmeyen job durumu, varsayÄ±lan olarak JOB_STATUS_UNKNOWN kullanÄ±lÄ±yor")
			jobStatusEnum = int32(pb.JobStatus_JOB_STATUS_UNKNOWN)
		}

		// pb.Job nesnesi oluÅŸtur
		job := &pb.Job{
			JobId:      jobID,
			Type:       pb.JobType(jobTypeEnum),
			Status:     pb.JobStatus(jobStatusEnum),
			AgentId:    agentID,
			CreatedAt:  timestamppb.New(createdAt),
			UpdatedAt:  timestamppb.New(updatedAt),
			Parameters: paramsMap,
		}

		// Null olabilecek alanlarÄ± iÅŸle
		if errorMessage.Valid {
			job.ErrorMessage = errorMessage.String
		}

		if result.Valid {
			job.Result = result.String
		}

		// Job'Ä± listeye ekle
		jobs = append(jobs, job)
	}

	// Rows.Err() kontrolÃ¼
	if err := rows.Err(); err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Msg("Job kayÄ±tlarÄ± okunurken hata")
		return nil, fmt.Errorf("job kayÄ±tlarÄ± okunurken hata: %v", err)
	}

	logger.Debug().
		Int("returned_jobs", len(jobs)).
		Int64("total_jobs", int64(total)).
		Str("agent_id", req.AgentId).
		Msg("VeritabanÄ±ndan job kayÄ±tlarÄ± dÃ¶ndÃ¼rÃ¼lÃ¼yor")

	return &pb.ListJobsResponse{
		Jobs:  jobs,
		Total: total,
	}, nil
}

// saveJobToDatabase, job'Ä± veritabanÄ±na kaydeder
func (s *Server) saveJobToDatabase(ctx context.Context, job *pb.Job) error {
	query := `
		INSERT INTO jobs (
			job_id,
			type,
			status,
			agent_id,
			created_at,
			updated_at,
			error_message,
			parameters,
			result
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
	`

	// map[string]string tipini JSON'a Ã§evir
	paramsJSON, err := json.Marshal(job.Parameters)
	if err != nil {
		return fmt.Errorf("parameters JSON'a Ã§evrilemedi: %v", err)
	}

	_, err = s.db.ExecContext(ctx, query,
		job.JobId,
		job.Type.String(),
		job.Status.String(),
		job.AgentId,
		job.CreatedAt.AsTime(),
		job.UpdatedAt.AsTime(),
		job.ErrorMessage,
		paramsJSON, // JSON formatÄ±nda parameterler
		job.Result,
	)

	return err
}

// updateJobInDatabase, job'Ä± veritabanÄ±nda gÃ¼nceller
func (s *Server) updateJobInDatabase(ctx context.Context, job *pb.Job) error {
	query := `
		UPDATE jobs SET
			status = $1,
			updated_at = $2,
			error_message = $3,
			result = $4
		WHERE job_id = $5
	`

	_, err := s.db.ExecContext(ctx, query,
		job.Status.String(),
		job.UpdatedAt.AsTime(),
		job.ErrorMessage,
		job.Result,
		job.JobId,
	)

	return err
}

// CreateJob, genel bir job oluÅŸturur ve veritabanÄ±na kaydeder
func (s *Server) CreateJob(ctx context.Context, job *pb.Job) error {
	// Job'Ä± memory'de kaydet
	s.jobMu.Lock()
	s.jobs[job.JobId] = job
	s.jobMu.Unlock()

	// VeritabanÄ±na kaydet
	if err := s.saveJobToDatabase(ctx, job); err != nil {
		return err
	}

	// Job tipi ve parametrelere gÃ¶re uygun iÅŸlemi baÅŸlat
	go func() {
		// Job durumunu Ã§alÄ±ÅŸÄ±yor olarak gÃ¼ncelle
		job.Status = pb.JobStatus_JOB_STATUS_RUNNING
		job.UpdatedAt = timestamppb.Now()
		s.updateJobInDatabase(context.Background(), job)

		// Ä°lgili agent'Ä± bul
		s.mu.RLock()
		agent, exists := s.agents[job.AgentId]
		s.mu.RUnlock()

		if !exists {
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = "Agent bulunamadÄ±"
			job.UpdatedAt = timestamppb.Now()
			s.updateJobInDatabase(context.Background(), job)
			return
		}

		var command string

		// Job tipine gÃ¶re iÅŸlemi belirle
		switch job.Type {
		case pb.JobType_JOB_TYPE_MONGO_PROMOTE_PRIMARY:
			nodeHostname := job.Parameters["node_hostname"]
			replicaSet := job.Parameters["replica_set"]
			nodeStatus := job.Parameters["node_status"]
			if nodeHostname == "" || replicaSet == "" {
				job.Status = pb.JobStatus_JOB_STATUS_FAILED
				job.ErrorMessage = "Eksik parametreler: node_hostname veya replica_set"
				break
			}

			// Node status'a gÃ¶re komutu belirle
			if nodeStatus == "primary" {
				// EÄŸer node zaten primary ise, farklÄ± bir komut gÃ¶nder
				command = fmt.Sprintf("db.isMaster() // Node %s zaten primary", nodeHostname)
			} else {
				// Secondary node'u primary'ye yÃ¼kselt
				command = fmt.Sprintf("rs.stepDown('%s')", nodeHostname)
			}

		case pb.JobType_JOB_TYPE_POSTGRES_PROMOTE_MASTER:
			// Operation type kontrol et - convert_to_slave ise farklÄ± iÅŸlem
			operationType := job.Parameters["operation_type"]
			if operationType == "convert_to_slave" {
				// PostgreSQL convert to slave iÅŸlemi
				newMasterHost := job.Parameters["new_master_host"]
				newMasterPort := job.Parameters["new_master_port"]
				dataDir := job.Parameters["data_directory"]
				replUser := job.Parameters["replication_user"]
				replPass := job.Parameters["replication_password"]

				if newMasterHost == "" || dataDir == "" || replUser == "" {
					job.Status = pb.JobStatus_JOB_STATUS_FAILED
					job.ErrorMessage = "Eksik parametreler: new_master_host, data_directory veya replication_user"
					break
				}

				if newMasterPort == "" {
					newMasterPort = "5432" // VarsayÄ±lan port
				}

				command = fmt.Sprintf("convert_postgres_to_slave|%s|%s|%s|%s|%s",
					newMasterHost, newMasterPort, dataDir, replUser, replPass)
			} else {
				// Normal promotion iÅŸlemi
				dataDir := job.Parameters["data_directory"]
				if dataDir == "" {
					job.Status = pb.JobStatus_JOB_STATUS_FAILED
					job.ErrorMessage = "Eksik parametre: data_directory"
					break
				}
				command = fmt.Sprintf("pg_ctl promote -D %s", dataDir)
			}

		case pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE:
			// PostgreSQL convert to slave iÅŸlemi
			newMasterHost := job.Parameters["new_master_host"]
			newMasterIp := job.Parameters["new_master_ip"]
			newMasterPort := job.Parameters["new_master_port"]
			dataDir := job.Parameters["data_directory"]
			coordinationJobId := job.Parameters["coordination_job_id"]
			oldMasterHost := job.Parameters["old_master_host"]

			if newMasterHost == "" || dataDir == "" {
				job.Status = pb.JobStatus_JOB_STATUS_FAILED
				job.ErrorMessage = "Eksik parametreler: new_master_host veya data_directory"
				break
			}

			if newMasterPort == "" {
				newMasterPort = "5432" // VarsayÄ±lan port
			}
			if newMasterIp == "" {
				newMasterIp = newMasterHost // Fallback: hostname'i IP olarak kullan
			}

			// Yeni komut formatÄ±
			// NOT: Replication bilgileri artÄ±k agent config'inden alÄ±nacak (gÃ¼venlik)
			command = fmt.Sprintf("convert_postgres_to_slave|%s|%s|%s|%s|%s|%s",
				newMasterHost,     // new_master_host
				newMasterIp,       // new_master_ip
				newMasterPort,     // new_master_port (STRING)
				dataDir,           // data_dir
				coordinationJobId, // coordination_job_id (opsiyonel)
				oldMasterHost)     // old_master_host (opsiyonel)

		default:
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = fmt.Sprintf("Desteklenmeyen job tipi: %s", job.Type.String())
		}

		if job.Status == pb.JobStatus_JOB_STATUS_FAILED {
			job.UpdatedAt = timestamppb.Now()
			s.updateJobInDatabase(context.Background(), job)
			return
		}

		// ðŸ”§ FIX: Timeout ile Stream.Send() Ã§aÄŸrÄ±sÄ±
		sendCtx, sendCancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer sendCancel()

		// Channel kullanarak timeout kontrollÃ¼ gÃ¶nderme
		sendDone := make(chan error, 1)
		go func() {
			sendErr := agent.Stream.Send(&pb.ServerMessage{
				Payload: &pb.ServerMessage_Query{
					Query: &pb.Query{
						QueryId: job.JobId,
						Command: command,
					},
				},
			})
			sendDone <- sendErr
		}()

		// Timeout veya baÅŸarÄ± durumunu bekle
		select {
		case err := <-sendDone:
			if err != nil {
				job.Status = pb.JobStatus_JOB_STATUS_FAILED
				job.ErrorMessage = fmt.Sprintf("Agent'a istek gÃ¶nderilemedi: %v", err)
			} else {
				job.Status = pb.JobStatus_JOB_STATUS_COMPLETED
				job.Result = "Job request sent successfully"
			}
		case <-sendCtx.Done():
			logger.Error().
				Str("job_id", job.JobId).
				Str("agent_id", job.AgentId).
				Dur("timeout", 10*time.Second).
				Msg("Timeout: CreateJob agent'a gÃ¶nderilemedi")
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = "Timeout: Agent'a istek gÃ¶nderilemedi (10s timeout)"
		}

		job.UpdatedAt = timestamppb.Now()

		// Database update iÃ§in de timeout context kullan
		dbCtx, dbCancel := context.WithTimeout(context.Background(), 5*time.Second)
		s.updateJobInDatabase(dbCtx, job)
		dbCancel()
	}()

	return nil
}

// FreezeMongoSecondary, MongoDB secondary node'larÄ±nÄ± belirli bir sÃ¼re iÃ§in dondurur
func (s *Server) FreezeMongoSecondary(ctx context.Context, req *pb.MongoFreezeSecondaryRequest) (*pb.MongoFreezeSecondaryResponse, error) {
	// Job oluÅŸtur
	job := &pb.Job{
		JobId:     req.JobId,
		Type:      pb.JobType_JOB_TYPE_MONGO_FREEZE_SECONDARY,
		Status:    pb.JobStatus_JOB_STATUS_PENDING,
		AgentId:   req.AgentId,
		CreatedAt: timestamppb.Now(),
		UpdatedAt: timestamppb.Now(),
		Parameters: map[string]string{
			"node_hostname": req.NodeHostname,
			"port":          fmt.Sprintf("%d", req.Port),
			"replica_set":   req.ReplicaSet,
			"seconds":       fmt.Sprintf("%d", req.Seconds),
		},
	}

	// Job'Ä± kaydet
	s.jobMu.Lock()
	s.jobs[job.JobId] = job
	s.jobMu.Unlock()

	// Ä°lgili agent'Ä± bul
	s.mu.RLock()
	agent, exists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !exists {
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = "Agent bulunamadÄ±"
		job.UpdatedAt = timestamppb.Now()
		return &pb.MongoFreezeSecondaryResponse{
			JobId:        job.JobId,
			Status:       job.Status,
			ErrorMessage: job.ErrorMessage,
		}, fmt.Errorf("agent bulunamadÄ±: %s", req.AgentId)
	}

	// Job'Ä± veritabanÄ±na kaydet
	if err := s.saveJobToDatabase(ctx, job); err != nil {
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = fmt.Sprintf("Job veritabanÄ±na kaydedilemedi: %v", err)
		job.UpdatedAt = timestamppb.Now()
		return &pb.MongoFreezeSecondaryResponse{
			JobId:        job.JobId,
			Status:       job.Status,
			ErrorMessage: job.ErrorMessage,
		}, err
	}

	// Job'Ä± Ã§alÄ±ÅŸtÄ±r
	go func() {
		job.Status = pb.JobStatus_JOB_STATUS_RUNNING
		job.UpdatedAt = timestamppb.Now()
		s.updateJobInDatabase(context.Background(), job)

		// Seconds parametresini kontrol et, varsayÄ±lan 60
		seconds := 60
		if req.Seconds > 0 {
			seconds = int(req.Seconds)
		}

		// rs.freeze() komutu oluÅŸtur
		command := fmt.Sprintf("rs.freeze(%d)", seconds)

		// ðŸ”§ FIX: Timeout ile Stream.Send() Ã§aÄŸrÄ±sÄ±
		sendCtx, sendCancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer sendCancel()

		// Channel kullanarak timeout kontrollÃ¼ gÃ¶nderme
		sendDone := make(chan error, 1)
		go func() {
			sendErr := agent.Stream.Send(&pb.ServerMessage{
				Payload: &pb.ServerMessage_Query{
					Query: &pb.Query{
						QueryId: job.JobId,
						Command: command,
					},
				},
			})
			sendDone <- sendErr
		}()

		// Timeout veya baÅŸarÄ± durumunu bekle
		select {
		case err := <-sendDone:
			if err != nil {
				job.Status = pb.JobStatus_JOB_STATUS_FAILED
				job.ErrorMessage = fmt.Sprintf("Agent'a istek gÃ¶nderilemedi: %v", err)
			} else {
				job.Status = pb.JobStatus_JOB_STATUS_COMPLETED
				job.Result = fmt.Sprintf("MongoDB node %s successfully frozen for %d seconds", req.NodeHostname, seconds)
			}
		case <-sendCtx.Done():
			logger.Error().
				Str("job_id", job.JobId).
				Str("node_hostname", req.NodeHostname).
				Dur("timeout", 10*time.Second).
				Msg("Timeout: FreezeMongoSecondary agent'a gÃ¶nderilemedi")
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = "Timeout: Agent'a istek gÃ¶nderilemedi (10s timeout)"
		}

		job.UpdatedAt = timestamppb.Now()

		// Database update iÃ§in de timeout context kullan
		dbCtx, dbCancel := context.WithTimeout(context.Background(), 5*time.Second)
		s.updateJobInDatabase(dbCtx, job)
		dbCancel()
	}()

	return &pb.MongoFreezeSecondaryResponse{
		JobId:  job.JobId,
		Status: pb.JobStatus_JOB_STATUS_PENDING,
	}, nil
}

// ExplainQuery, PostgreSQL sorgu planÄ±nÄ± EXPLAIN ANALYZE kullanarak getirir
func (s *Server) ExplainQuery(ctx context.Context, req *pb.ExplainQueryRequest) (*pb.ExplainQueryResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("database", req.Database).
		Msg("ExplainQuery metodu Ã§aÄŸrÄ±ldÄ±")

	// Agent ID'yi kontrol et
	agentID := req.AgentId
	if agentID == "" {
		return &pb.ExplainQueryResponse{
			Status:       "error",
			ErrorMessage: "agent_id boÅŸ olamaz",
		}, fmt.Errorf("agent_id boÅŸ olamaz")
	}

	// Sorguyu kontrol et
	query := req.Query
	if query == "" {
		return &pb.ExplainQueryResponse{
			Status:       "error",
			ErrorMessage: "sorgu boÅŸ olamaz",
		}, fmt.Errorf("sorgu boÅŸ olamaz")
	}

	// EXPLAIN ANALYZE ile sorguyu Ã§evrele
	explainQuery := fmt.Sprintf("EXPLAIN (ANALYZE true, BUFFERS true, COSTS true, TIMING true, VERBOSE true, FORMAT TEXT) %s", query)

	// Unique bir sorgu ID'si oluÅŸtur
	queryID := fmt.Sprintf("explain_%d", time.Now().UnixNano())

	// Agent'a sorguyu gÃ¶nder ve cevabÄ± al
	result, err := s.SendQuery(ctx, agentID, queryID, explainQuery, req.Database)
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Str("database", req.Database).
			Str("query_id", queryID).
			Msg("Sorgu planÄ± alÄ±nÄ±rken hata")
		return &pb.ExplainQueryResponse{
			Status:       "error",
			ErrorMessage: fmt.Sprintf("Sorgu planÄ± alÄ±nÄ±rken hata: %v", err),
		}, err
	}

	// Sorgu sonucunu dÃ¶ndÃ¼r
	if result.Result != nil {
		logger.Debug().
			Str("type_url", result.Result.TypeUrl).
			Str("query_id", queryID).
			Msg("Sorgu sonucu alÄ±ndÄ±")

		// Sonucu okunabilir formata dÃ¶nÃ¼ÅŸtÃ¼r
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Debug().
				Err(err).
				Str("query_id", queryID).
				Msg("SonuÃ§ structpb.Struct'a dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼rken hata")

			// FarklÄ± bir yÃ¶ntem deneyelim - doÄŸrudan JSON string'e Ã§evirmeyi deneyelim
			resultStr := string(result.Result.Value)
			if len(resultStr) > 0 {
				logger.Debug().
					Int("result_size", len(resultStr)).
					Str("query_id", queryID).
					Msg("Result.Value doÄŸrudan string olarak kullanÄ±lÄ±yor")
				return &pb.ExplainQueryResponse{
					Status: "success",
					Plan:   resultStr,
				}, nil
			}

			return &pb.ExplainQueryResponse{
				Status:       "error",
				ErrorMessage: fmt.Sprintf("SonuÃ§ dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼rken hata: %v", err),
			}, err
		}

		// Struct'Ä± doÄŸrudan JSON string'e dÃ¶nÃ¼ÅŸtÃ¼r
		resultMap := resultStruct.AsMap()
		resultBytes, err := json.Marshal(resultMap)
		if err != nil {
			logger.Error().
				Err(err).
				Str("agent_id", req.AgentId).
				Str("database", req.Database).
				Msg("ExplainQuery JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
			return &pb.ExplainQueryResponse{
				Status:       "error",
				ErrorMessage: fmt.Sprintf("JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err),
			}, err
		}

		// EÄŸer resultMap iÃ§inde Ã¶zellikle "result" veya "explain" anahtarÄ± varsa,
		// bunlarÄ± kullanarak okunabilir bir aÃ§Ä±klama formatÄ± oluÅŸtur
		var planText string
		if explainResult, ok := resultMap["result"]; ok {
			// SonuÃ§ "result" anahtarÄ±nda ise
			if explainStr, ok := explainResult.(string); ok {
				planText = explainStr
			} else {
				// EÄŸer doÄŸrudan string deÄŸilse JSON olarak dÃ¶nÃ¼ÅŸtÃ¼r
				if explainBytes, err := json.MarshalIndent(explainResult, "", "  "); err == nil {
					planText = string(explainBytes)
				}
			}
		} else if explainResult, ok := resultMap["explain"]; ok {
			// SonuÃ§ "explain" anahtarÄ±nda ise
			if explainStr, ok := explainResult.(string); ok {
				planText = explainStr
			} else {
				// EÄŸer doÄŸrudan string deÄŸilse JSON olarak dÃ¶nÃ¼ÅŸtÃ¼r
				if explainBytes, err := json.MarshalIndent(explainResult, "", "  "); err == nil {
					planText = string(explainBytes)
				}
			}
		}

		// EÄŸer Ã¶zel format bulunamadÄ±ysa, tÃ¼m JSON formatÄ±nÄ± kullan
		if planText == "" {
			planText = string(resultBytes)
		}

		// JSON string'i doÄŸrudan plan alanÄ±nda kullan
		return &pb.ExplainQueryResponse{
			Status: "success",
			Plan:   planText,
		}, nil
	}

	// SonuÃ§ boÅŸ ise hata dÃ¶ndÃ¼r
	return &pb.ExplainQueryResponse{
		Status:       "error",
		ErrorMessage: "Sorgu planÄ± alÄ±namadÄ±",
	}, fmt.Errorf("sorgu planÄ± alÄ±namadÄ±")
}

// ExplainMongoQuery, MongoDB sorgu planÄ±nÄ± explain() kullanarak getirir
func (s *Server) ExplainMongoQuery(ctx context.Context, req *pb.ExplainQueryRequest) (*pb.ExplainQueryResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("database", req.Database).
		Msg("ExplainMongoQuery Ã§aÄŸrÄ±ldÄ±")

	// Ham sorguyu loglayalÄ±m
	logger.Debug().
		Str("raw_query", req.Query).
		Msg("Ham sorgu (JSON)")

	// Agent ID'yi kontrol et
	agentID := req.AgentId
	if agentID == "" {
		return &pb.ExplainQueryResponse{
			Status:       "error",
			ErrorMessage: "agent_id boÅŸ olamaz",
		}, fmt.Errorf("agent_id boÅŸ olamaz")
	}

	// Sorguyu kontrol et
	query := req.Query
	if query == "" {
		return &pb.ExplainQueryResponse{
			Status:       "error",
			ErrorMessage: "sorgu boÅŸ olamaz",
		}, fmt.Errorf("sorgu boÅŸ olamaz")
	}

	// VeritabanÄ± adÄ±nÄ± kontrol et
	database := req.Database
	if database == "" {
		return &pb.ExplainQueryResponse{
			Status:       "error",
			ErrorMessage: "database boÅŸ olamaz",
		}, fmt.Errorf("database boÅŸ olamaz")
	}

	// Sorguyu MongoDB explain komutu olarak gÃ¶nder
	// Yeni protokol formatÄ±: MONGO_EXPLAIN|<database>|<query_json>
	explainCommand := fmt.Sprintf("MONGO_EXPLAIN|%s|%s", database, query)

	logger.Debug().
		Str("protocol_format", "MONGO_EXPLAIN|<database>|<query_json>").
		Msg("MongoDB Explain protokol formatÄ±")
	logger.Debug().
		Int("command_length", len(explainCommand)).
		Msg("HazÄ±rlanan sorgu komut uzunluÄŸu")

	// Sorgunun ilk kÄ±smÄ±nÄ± loglayalÄ±m, Ã§ok uzunsa sadece baÅŸÄ±nÄ±
	if len(query) > 500 {
		logger.Debug().
			Str("query_preview", query[:500]+"...").
			Int("total_length", len(query)).
			Msg("Sorgu (ilk 500 karakter)")
	} else {
		logger.Debug().
			Str("query", query).
			Msg("Sorgu")
	}

	// Unique bir sorgu ID'si oluÅŸtur
	queryID := fmt.Sprintf("mongo_explain_%d", time.Now().UnixNano())

	// Uzun iÅŸlem iÃ§in timeout sÃ¼resini artÄ±r (60 saniye)
	longCtx, cancel := context.WithTimeout(ctx, 60*time.Second)
	defer cancel()

	// Agent'a sorguyu gÃ¶nder ve cevabÄ± al
	logger.Debug().
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("MongoDB sorgusu agent'a gÃ¶nderiliyor")
	// Burada database parametresini boÅŸ geÃ§iyoruz Ã§Ã¼nkÃ¼ zaten explainCommand iÃ§inde belirttik
	result, err := s.SendQuery(longCtx, agentID, queryID, explainCommand, "")
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Str("query_id", queryID).
			Str("database", req.Database).
			Msg("MongoDB sorgu planÄ± alÄ±nÄ±rken hata")
		errMsg := err.Error()
		if err == context.DeadlineExceeded {
			errMsg = "sorgu zaman aÅŸÄ±mÄ±na uÄŸradÄ±"
		}
		return &pb.ExplainQueryResponse{
			Status:       "error",
			ErrorMessage: fmt.Sprintf("MongoDB sorgu planÄ± alÄ±nÄ±rken bir hata oluÅŸtu: %s", errMsg),
		}, err
	}

	// Sorgu sonucunu dÃ¶ndÃ¼r
	if result.Result != nil {
		logger.Debug().
			Str("type_url", result.Result.TypeUrl).
			Str("query_id", queryID).
			Msg("MongoDB sorgu planÄ± alÄ±ndÄ±")

		// Agent'tan gelen ham veriyi direkt kullan
		var rawResult string

		// TypeUrl'e gÃ¶re iÅŸlem yap - direkt ham veriyi Ã§Ä±karmaya Ã§alÄ±ÅŸ
		if result.Result.TypeUrl == "type.googleapis.com/google.protobuf.Value" {
			// Value tipinde olanlar iÃ§in direkt string deÄŸerini kullan
			rawResult = string(result.Result.Value)
			logger.Debug().
				Int("result_length", len(rawResult)).
				Str("query_id", queryID).
				Msg("Value tipi veri direkt string olarak kullanÄ±ldÄ±")
		} else {
			// Struct veya diÄŸer tipler iÃ§in unmarshalling yapÄ±lmalÄ±
			var resultStruct structpb.Struct
			if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
				logger.Error().
					Err(err).
					Str("query_id", queryID).
					Msg("MongoDB sonucu ayrÄ±ÅŸtÄ±rÄ±lamadÄ±")
				return &pb.ExplainQueryResponse{
					Status:       "error",
					ErrorMessage: fmt.Sprintf("MongoDB sorgu planÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: %v", err),
				}, err
			}

			// Struct'Ä± JSON olarak serialize et
			resultMap := resultStruct.AsMap()
			resultBytes, err := json.Marshal(resultMap)
			if err != nil {
				logger.Error().
					Err(err).
					Str("query_id", queryID).
					Msg("MongoDB JSON serileÅŸtirilirken hata")
				return &pb.ExplainQueryResponse{
					Status:       "error",
					ErrorMessage: fmt.Sprintf("MongoDB JSON serileÅŸtirme hatasÄ±: %v", err),
				}, err
			}

			rawResult = string(resultBytes)
		}

		// YanÄ±tÄ± dÃ¶ndÃ¼r
		return &pb.ExplainQueryResponse{
			Status: "success",
			Plan:   rawResult,
		}, nil
	}

	// SonuÃ§ boÅŸ ise hata dÃ¶ndÃ¼r
	logger.Error().
		Str("query_id", queryID).
		Str("agent_id", agentID).
		Msg("MongoDB sorgu planÄ± boÅŸ sonuÃ§ dÃ¶ndÃ¼")
	return &pb.ExplainQueryResponse{
		Status:       "error",
		ErrorMessage: "MongoDB sorgu planÄ± alÄ±namadÄ±: BoÅŸ sonuÃ§",
	}, fmt.Errorf("MongoDB sorgu planÄ± alÄ±namadÄ±: BoÅŸ sonuÃ§")
}

// SendMSSQLInfo, agent'dan gelen MSSQL bilgilerini iÅŸler
func (s *Server) SendMSSQLInfo(ctx context.Context, req *pb.MSSQLInfoRequest) (*pb.MSSQLInfoResponse, error) {
	logger.Info().
		Str("cluster", req.MssqlInfo.ClusterName).
		Str("hostname", req.MssqlInfo.Hostname).
		Bool("ha_enabled", req.MssqlInfo.IsHaEnabled).
		Msg("SendMSSQLInfo metodu Ã§aÄŸrÄ±ldÄ±")

	// Gelen MSSQL bilgilerini logla
	mssqlInfo := req.MssqlInfo
	
	// Agent ID'sini metrik verilerinden Ã§Ä±kar ve agent durumunu active yap
	agentID := fmt.Sprintf("agent_%s", mssqlInfo.Hostname)
	s.UpdateAgentConnectedStatus(agentID)

	// AlwaysOn bilgilerini logla
	if mssqlInfo.IsHaEnabled && mssqlInfo.AlwaysOnMetrics != nil {
		alwaysOn := mssqlInfo.AlwaysOnMetrics
		logger.Info().
			Str("cluster_name", alwaysOn.ClusterName).
			Str("health_state", alwaysOn.HealthState).
			Str("operational_state", alwaysOn.OperationalState).
			Str("primary_replica", alwaysOn.PrimaryReplica).
			Str("local_role", alwaysOn.LocalRole).
			Str("sync_mode", alwaysOn.SynchronizationMode).
			Int64("replication_lag_ms", alwaysOn.ReplicationLagMs).
			Int64("log_send_queue_kb", alwaysOn.LogSendQueueKb).
			Int64("redo_queue_kb", alwaysOn.RedoQueueKb).
			Msg("AlwaysOn Cluster bilgileri")

		if len(alwaysOn.Replicas) > 0 {
			logger.Debug().
				Int("replica_count", len(alwaysOn.Replicas)).
				Msg("AlwaysOn Replicas")
			for i, replica := range alwaysOn.Replicas {
				logger.Debug().
					Int("replica_index", i+1).
					Str("replica_name", replica.ReplicaName).
					Str("role", replica.Role).
					Str("connection_state", replica.ConnectionState).
					Msg("AlwaysOn Replica")
			}
		}

		if len(alwaysOn.Listeners) > 0 {
			logger.Debug().
				Int("listener_count", len(alwaysOn.Listeners)).
				Msg("AlwaysOn Listeners")
			for i, listener := range alwaysOn.Listeners {
				logger.Debug().
					Int("listener_index", i+1).
					Str("listener_name", listener.ListenerName).
					Int32("port", listener.Port).
					Str("state", listener.ListenerState).
					Msg("AlwaysOn Listener")
			}
		}
	} else if mssqlInfo.IsHaEnabled {
		logger.Warn().
			Str("hostname", mssqlInfo.Hostname).
			Msg("HA enabled but AlwaysOn metrics not available")
	}

	// VeritabanÄ±na kaydetme iÅŸlemi
	err := s.saveMSSQLInfoToDatabase(ctx, mssqlInfo)
	if err != nil {
		logger.Error().
			Err(err).
			Str("cluster", mssqlInfo.ClusterName).
			Str("hostname", mssqlInfo.Hostname).
			Msg("MSSQL bilgileri veritabanÄ±na kaydedilemedi")
		return &pb.MSSQLInfoResponse{
			Status: "error",
		}, nil
	}

	logger.Info().
		Str("cluster", mssqlInfo.ClusterName).
		Str("hostname", mssqlInfo.Hostname).
		Msg("MSSQL bilgileri baÅŸarÄ±yla iÅŸlendi ve kaydedildi")

	return &pb.MSSQLInfoResponse{
		Status: "success",
	}, nil
}

// MSSQL bilgilerini veritabanÄ±na kaydetmek iÃ§in yardÄ±mcÄ± fonksiyon
func (s *Server) saveMSSQLInfoToDatabase(ctx context.Context, mssqlInfo *pb.MSSQLInfo) error {
	// Ã–nce mevcut kaydÄ± kontrol et
	var existingData []byte
	var id int

	checkQuery := `
		SELECT id, jsondata FROM public.mssql_data 
		WHERE clustername = $1 
		ORDER BY id DESC LIMIT 1
	`

	err := s.db.QueryRowContext(ctx, checkQuery, mssqlInfo.ClusterName).Scan(&id, &existingData)

	// Yeni node verisi
	mssqlData := map[string]interface{}{
		"ClusterName": mssqlInfo.ClusterName,
		"Location":    mssqlInfo.Location,
		"FDPercent":   mssqlInfo.FdPercent,
		"FreeDisk":    mssqlInfo.FreeDisk,
		"Hostname":    mssqlInfo.Hostname,
		"IP":          mssqlInfo.Ip,
		"NodeStatus":  mssqlInfo.NodeStatus,
		"Status":      mssqlInfo.Status,
		"Version":     mssqlInfo.Version,
		"Instance":    mssqlInfo.Instance,
		"Port":        mssqlInfo.Port,
		"TotalVCPU":   mssqlInfo.TotalVcpu,
		"TotalMemory": mssqlInfo.TotalMemory,
		"ConfigPath":  mssqlInfo.ConfigPath,
		"Database":    mssqlInfo.Database,
		"IsHAEnabled": mssqlInfo.IsHaEnabled,
		"HARole":      mssqlInfo.HaRole,
		"Edition":     mssqlInfo.Edition,
	}

	// AlwaysOn bilgilerini ekle (eÄŸer mevcutsa)
	if mssqlInfo.IsHaEnabled && mssqlInfo.AlwaysOnMetrics != nil {
		alwaysOnData := map[string]interface{}{
			"ClusterName":         mssqlInfo.AlwaysOnMetrics.ClusterName,
			"HealthState":         mssqlInfo.AlwaysOnMetrics.HealthState,
			"OperationalState":    mssqlInfo.AlwaysOnMetrics.OperationalState,
			"SynchronizationMode": mssqlInfo.AlwaysOnMetrics.SynchronizationMode,
			"FailoverMode":        mssqlInfo.AlwaysOnMetrics.FailoverMode,
			"PrimaryReplica":      mssqlInfo.AlwaysOnMetrics.PrimaryReplica,
			"LocalRole":           mssqlInfo.AlwaysOnMetrics.LocalRole,
			"LastFailoverTime":    mssqlInfo.AlwaysOnMetrics.LastFailoverTime,
			"ReplicationLagMs":    mssqlInfo.AlwaysOnMetrics.ReplicationLagMs,
			"LogSendQueueKb":      mssqlInfo.AlwaysOnMetrics.LogSendQueueKb,
			"RedoQueueKb":         mssqlInfo.AlwaysOnMetrics.RedoQueueKb,
		}

		// Replica bilgilerini ekle
		if len(mssqlInfo.AlwaysOnMetrics.Replicas) > 0 {
			replicas := make([]map[string]interface{}, len(mssqlInfo.AlwaysOnMetrics.Replicas))
			for i, replica := range mssqlInfo.AlwaysOnMetrics.Replicas {
				replicas[i] = map[string]interface{}{
					"ReplicaName":         replica.ReplicaName,
					"Role":                replica.Role,
					"ConnectionState":     replica.ConnectionState,
					"SynchronizationMode": replica.SynchronizationMode,
					"FailoverMode":        replica.FailoverMode,
					"AvailabilityMode":    replica.AvailabilityMode,
					"JoinState":           replica.JoinState,
					"ConnectedState":      replica.ConnectedState,
					"SuspendReason":       replica.SuspendReason,
				}
			}
			alwaysOnData["Replicas"] = replicas
		}

		// Database bilgilerini ekle
		if len(mssqlInfo.AlwaysOnMetrics.Databases) > 0 {
			databases := make([]map[string]interface{}, len(mssqlInfo.AlwaysOnMetrics.Databases))
			for i, db := range mssqlInfo.AlwaysOnMetrics.Databases {
				databases[i] = map[string]interface{}{
					"DatabaseName":         db.DatabaseName,
					"ReplicaName":          db.ReplicaName,
					"SynchronizationState": db.SynchronizationState,
					"SuspendReason":        db.SuspendReason,
					"LastSentTime":         db.LastSentTime,
					"LastReceivedTime":     db.LastReceivedTime,
					"LastHardenedTime":     db.LastHardenedTime,
					"LastRedoneTime":       db.LastRedoneTime,
					"LogSendQueueKb":       db.LogSendQueueKb,
					"LogSendRateKbPerSec":  db.LogSendRateKbPerSec,
					"RedoQueueKb":          db.RedoQueueKb,
					"RedoRateKbPerSec":     db.RedoRateKbPerSec,
					"EndOfLogLsn":          db.EndOfLogLsn,
					"RecoveryLsn":          db.RecoveryLsn,
					"TruncationLsn":        db.TruncationLsn,
					"LastCommitLsn":        db.LastCommitLsn,
					"LastCommitTime":       db.LastCommitTime,
				}
			}
			alwaysOnData["Databases"] = databases
		}

		// Listener bilgilerini ekle
		if len(mssqlInfo.AlwaysOnMetrics.Listeners) > 0 {
			listeners := make([]map[string]interface{}, len(mssqlInfo.AlwaysOnMetrics.Listeners))
			for i, listener := range mssqlInfo.AlwaysOnMetrics.Listeners {
				listeners[i] = map[string]interface{}{
					"ListenerName":  listener.ListenerName,
					"IpAddresses":   listener.IpAddresses,
					"Port":          listener.Port,
					"SubnetMask":    listener.SubnetMask,
					"ListenerState": listener.ListenerState,
					"DnsName":       listener.DnsName,
				}
			}
			alwaysOnData["Listeners"] = listeners
		}

		mssqlData["AlwaysOnMetrics"] = alwaysOnData
		logger.Debug().
			Str("hostname", mssqlInfo.Hostname).
			Str("cluster", mssqlInfo.ClusterName).
			Msg("AlwaysOn metrics veritabanÄ±na kaydedildi")
	}

	var jsonData []byte

	// Hata kontrolÃ¼nÃ¼ dÃ¼zgÃ¼n yap
	if err == nil {
		// Mevcut kayÄ±t var, gÃ¼ncelle
		logger.Debug().
			Str("cluster", mssqlInfo.ClusterName).
			Int("id", id).
			Msg("MSSQL cluster iÃ§in mevcut kayÄ±t bulundu, gÃ¼ncelleniyor")

		var existingJSON map[string][]interface{}
		if err := json.Unmarshal(existingData, &existingJSON); err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mssqlInfo.ClusterName).
				Msg("MSSQL mevcut JSON ayrÄ±ÅŸtÄ±rma hatasÄ±")
			return err
		}

		// Cluster array'ini al
		clusterData, ok := existingJSON[mssqlInfo.ClusterName]
		if !ok {
			// EÄŸer cluster verisi yoksa yeni oluÅŸtur
			clusterData = []interface{}{}
		}

		// Node'u bul ve gÃ¼ncelle
		nodeFound := false
		nodeChanged := false
		for i, node := range clusterData {
			nodeMap, ok := node.(map[string]interface{})
			if !ok {
				continue
			}

			// Hostname ve IP ile node eÅŸleÅŸmesi kontrol et
			if nodeMap["Hostname"] == mssqlInfo.Hostname && nodeMap["IP"] == mssqlInfo.Ip {
				// Sadece deÄŸiÅŸen alanlarÄ± gÃ¼ncelle
				nodeFound = true

				// DeÄŸiÅŸiklikleri takip et
				for key, newValue := range mssqlData {
					currentValue, exists := nodeMap[key]
					var hasChanged bool

					if !exists {
						// DeÄŸer mevcut deÄŸil, yeni alan ekleniyor
						hasChanged = true
						logger.Debug().
							Str("hostname", mssqlInfo.Hostname).
							Str("field", key).
							Msg("MSSQL node'da yeni alan eklendi")
					} else {
						// Mevcut deÄŸer ile yeni deÄŸeri karÅŸÄ±laÅŸtÄ±r
						// Numeric deÄŸerler iÃ§in Ã¶zel karÅŸÄ±laÅŸtÄ±rma yap
						hasChanged = !compareValues(currentValue, newValue)
						if hasChanged {
							if key == "AlwaysOnMetrics" {
								logger.Debug().
									Str("hostname", mssqlInfo.Hostname).
									Msg("MSSQL node'da AlwaysOn metrics gÃ¼ncellendi")
							} else {
								logger.Debug().
									Str("hostname", mssqlInfo.Hostname).
									Str("field", key).
									Interface("old_value", currentValue).
									Interface("new_value", newValue).
									Msg("MSSQL node'da deÄŸiÅŸiklik tespit edildi")
							}
						}
					}

					if hasChanged {
						nodeMap[key] = newValue
						// HARole, Status, AlwaysOnMetrics gibi Ã¶nemli bir deÄŸiÅŸiklik varsa iÅŸaretle
						if key == "HARole" || key == "Status" || key == "NodeStatus" || key == "FreeDisk" || key == "AlwaysOnMetrics" {
							nodeChanged = true
						}
					}
				}

				clusterData[i] = nodeMap
				break
			}
		}

		// EÄŸer node bulunamadÄ±ysa yeni ekle
		if !nodeFound {
			clusterData = append(clusterData, mssqlData)
			nodeChanged = true
			logger.Info().
				Str("hostname", mssqlInfo.Hostname).
				Str("cluster", mssqlInfo.ClusterName).
				Msg("Yeni MSSQL node eklendi")
		}

		// EÄŸer Ã¶nemli bir deÄŸiÅŸiklik yoksa veritabanÄ±nÄ± gÃ¼ncelleme
		if !nodeChanged {
			logger.Debug().
				Str("hostname", mssqlInfo.Hostname).
				Str("cluster", mssqlInfo.ClusterName).
				Msg("MSSQL node'da Ã¶nemli bir deÄŸiÅŸiklik yok, gÃ¼ncelleme yapÄ±lmadÄ±")
			return nil
		}

		existingJSON[mssqlInfo.ClusterName] = clusterData

		// JSON'Ä± gÃ¼ncelle
		jsonData, err = json.Marshal(existingJSON)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mssqlInfo.ClusterName).
				Msg("MSSQL JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
			return err
		}

		// VeritabanÄ±nÄ± gÃ¼ncelle
		updateQuery := `
			UPDATE public.mssql_data 
			SET jsondata = $1, updated_at = CURRENT_TIMESTAMP
			WHERE id = $2
		`

		_, err = s.db.ExecContext(ctx, updateQuery, jsonData, id)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mssqlInfo.ClusterName).
				Int("id", id).
				Msg("MSSQL veritabanÄ± gÃ¼ncelleme hatasÄ±")
			return err
		}

		logger.Info().
			Str("hostname", mssqlInfo.Hostname).
			Str("cluster", mssqlInfo.ClusterName).
			Int("record_id", id).
			Msg("MSSQL node bilgileri baÅŸarÄ±yla gÃ¼ncellendi (Ã¶nemli deÄŸiÅŸiklik nedeniyle)")
	} else if err == sql.ErrNoRows {
		// KayÄ±t bulunamadÄ±, yeni kayÄ±t oluÅŸtur
		logger.Info().
			Str("cluster", mssqlInfo.ClusterName).
			Str("hostname", mssqlInfo.Hostname).
			Msg("MSSQL cluster iÃ§in kayÄ±t bulunamadÄ±, yeni kayÄ±t oluÅŸturuluyor")

		outerJSON := map[string][]interface{}{
			mssqlInfo.ClusterName: {mssqlData},
		}

		jsonData, err = json.Marshal(outerJSON)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mssqlInfo.ClusterName).
				Msg("MSSQL yeni kayÄ±t JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
			return err
		}

		insertQuery := `
			INSERT INTO public.mssql_data (
				jsondata, clustername, created_at, updated_at
			) VALUES ($1, $2, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
			ON CONFLICT (clustername) DO UPDATE SET
				jsondata = EXCLUDED.jsondata,
				updated_at = CURRENT_TIMESTAMP
		`

		_, err = s.db.ExecContext(ctx, insertQuery, jsonData, mssqlInfo.ClusterName)
		if err != nil {
			logger.Error().
				Err(err).
				Str("cluster", mssqlInfo.ClusterName).
				Str("hostname", mssqlInfo.Hostname).
				Msg("MSSQL veritabanÄ± ekleme hatasÄ±")
			return err
		}

		logger.Info().
			Str("cluster", mssqlInfo.ClusterName).
			Str("hostname", mssqlInfo.Hostname).
			Msg("MSSQL node bilgileri baÅŸarÄ±yla veritabanÄ±na kaydedildi (yeni kayÄ±t)")
	} else {
		// BaÅŸka bir veritabanÄ± hatasÄ± oluÅŸtu
		logger.Error().
			Err(err).
			Str("cluster", mssqlInfo.ClusterName).
			Str("hostname", mssqlInfo.Hostname).
			Msg("MSSQL cluster kayÄ±t kontrolÃ¼ sÄ±rasÄ±nda hata")
		return fmt.Errorf("veritabanÄ± kontrol hatasÄ±: %v", err)
	}

	return nil
}

// GetStatusMSSQL, MSSQL veritabanÄ±ndan durum bilgilerini Ã§eker
func (s *Server) GetStatusMSSQL(ctx context.Context, _ *structpb.Struct) (*structpb.Value, error) {
	rows, err := s.db.QueryContext(ctx, "SELECT json_agg(sub.jsondata) FROM (SELECT jsondata FROM mssql_data ORDER BY id) AS sub")
	if err != nil {
		return nil, status.Errorf(codes.Internal, "VeritabanÄ± sorgusu baÅŸarÄ±sÄ±z: %v", err)
	}
	defer rows.Close()

	var jsonData []byte
	if rows.Next() {
		err := rows.Scan(&jsonData)
		if err != nil {
			return nil, status.Errorf(codes.Internal, "Veri okuma hatasÄ±: %v", err)
		}
	}

	// JSON verisini structpb.Value'ya dÃ¶nÃ¼ÅŸtÃ¼r
	var jsonValue interface{}
	if err := json.Unmarshal(jsonData, &jsonValue); err != nil {
		return nil, status.Errorf(codes.Internal, "JSON ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
	}

	value, err := structpb.NewValue(jsonValue)
	if err != nil {
		return nil, status.Errorf(codes.Internal, "Veri dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
	}

	return value, nil
}

// GetMSSQLBestPracticesAnalysis, MSSQL best practice analizi gerÃ§ekleÅŸtirir
func (s *Server) GetMSSQLBestPracticesAnalysis(ctx context.Context, agentID, database string) (*pb.BestPracticesAnalysisResponse, error) {
	logger.Info().
		Str("agent_id", agentID).
		Str("database", database).
		Msg("GetMSSQLBestPracticesAnalysis Ã§aÄŸrÄ±ldÄ±")

	// Agent ID'yi kontrol et
	if agentID == "" {
		return nil, fmt.Errorf("agent_id boÅŸ olamaz")
	}

	// BestPracticesAnalysisRequest oluÅŸtur
	req := &pb.BestPracticesAnalysisRequest{
		AgentId:      agentID,
		DatabaseName: database,
	}

	// RPC Ã§aÄŸrÄ±sÄ±nÄ± yap
	return s.GetBestPracticesAnalysis(ctx, req)
}

// GetBestPracticesAnalysis, MSSQL best practice analizi iÃ§in gRPC Ã§aÄŸrÄ±sÄ±nÄ± yapar
func (s *Server) GetBestPracticesAnalysis(ctx context.Context, req *pb.BestPracticesAnalysisRequest) (*pb.BestPracticesAnalysisResponse, error) {
	agentID := req.AgentId

	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// Unique bir sorgu ID'si oluÅŸtur
	queryID := fmt.Sprintf("mssql_bpa_%d", time.Now().UnixNano())

	// MSSQL_BPA formatÄ±nda komut oluÅŸtur (database parametresi opsiyonel)
	command := "MSSQL_BPA"
	if req.DatabaseName != "" {
		command = fmt.Sprintf("MSSQL_BPA|%s", req.DatabaseName)
	}
	if req.ServerName != "" {
		command = fmt.Sprintf("%s|%s", command, req.ServerName)
	}

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Msg("MSSQL Best Practices Analysis komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	})
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", agentID).
			Str("query_id", queryID).
			Msg("MSSQL Best Practices Analysis sorgusu gÃ¶nderilemedi")
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Msg("MSSQL Best Practices Analysis sorgusu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			logger.Error().
				Str("query_id", queryID).
				Str("agent_id", agentID).
				Msg("Best Practices Analysis null sonuÃ§ alÄ±ndÄ±")
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Str("agent_id", agentID).
			Msg("Best Practices Analysis yanÄ±t alÄ±ndÄ±")

		// Sonucu BestPracticesAnalysisResponse'a dÃ¶nÃ¼ÅŸtÃ¼r
		response := &pb.BestPracticesAnalysisResponse{
			Status:            "success",
			AnalysisId:        queryID,
			AnalysisTimestamp: time.Now().Format(time.RFC3339),
		}

		// Any tipini Ã§Ã¶zÃ¼mle
		if result.Result.TypeUrl == "type.googleapis.com/google.protobuf.Value" {
			// Value tipinde ise, doÄŸrudan JSON string olarak kullan
			response.AnalysisResults = result.Result.Value
			logger.Debug().
				Int("result_size", len(response.AnalysisResults)).
				Str("type", "JSON string").
				Msg("Best Practices Analysis sonucu alÄ±ndÄ±")
		} else {
			// Struct olarak Ã§Ã¶zÃ¼mle
			var resultStruct structpb.Struct
			if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
				logger.Error().
					Err(err).
					Msg("Struct Ã§Ã¶zÃ¼mleme hatasÄ±")
				return nil, fmt.Errorf("sonuÃ§ Ã§Ã¶zÃ¼mleme hatasÄ±: %v", err)
			}

			// Struct'tan JSON string oluÅŸtur
			resultBytes, err := json.Marshal(resultStruct.AsMap())
			if err != nil {
				logger.Error().
					Err(err).
					Msg("JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±")
				return nil, fmt.Errorf("JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
			}

			response.AnalysisResults = resultBytes
			logger.Debug().
				Int("result_size", len(response.AnalysisResults)).
				Str("type", "Struct->JSON").
				Msg("Best Practices Analysis sonucu alÄ±ndÄ±")
		}

		return response, nil

	case <-ctx.Done():
		// Context iptal edildi veya zaman aÅŸÄ±mÄ±na uÄŸradÄ±
		logger.Error().
			Err(ctx.Err()).
			Msg("Best Practices Analysis sonucu beklerken timeout/iptal")
		return nil, ctx.Err()
	}
}

// ReportProcessLogs, agent'lardan gelen iÅŸlem loglarÄ±nÄ± iÅŸler
func (s *Server) ReportProcessLogs(ctx context.Context, req *pb.ProcessLogRequest) (*pb.ProcessLogResponse, error) {
	logger.Info().
		Str("agent_id", req.LogUpdate.AgentId).
		Str("process_id", req.LogUpdate.ProcessId).
		Str("process_type", req.LogUpdate.ProcessType).
		Msg("ReportProcessLogs metodu Ã§aÄŸrÄ±ldÄ±")

	// Gelen log mesajlarÄ±nÄ± logla
	for _, msg := range req.LogUpdate.LogMessages {
		logger.Debug().
			Str("process_id", req.LogUpdate.ProcessId).
			Str("status", req.LogUpdate.Status).
			Str("message", msg).
			Msg("Process Log")
	}

	// DEBUG: Metadata iÃ§eriÄŸini tamamen logla
	if metadata := req.LogUpdate.Metadata; metadata != nil {
		logger.Debug().
			Str("agent_id", req.LogUpdate.AgentId).
			Int("metadata_count", len(metadata)).
			Interface("metadata", metadata).
			Msg("ProcessLoggerHandler metadata iÃ§eriÄŸi")
	} else {
		logger.Debug().
			Str("agent_id", req.LogUpdate.AgentId).
			Msg("ProcessLoggerHandler - metadata yok")
	}

	// ProcessLoggerHandler'da metadata kontrolÃ¼ ekle
	if metadata := req.LogUpdate.Metadata; metadata != nil {
		// 1. Failover Coordination Request (Yeni master'dan gelen)
		if isFailoverRequest, exists := metadata["failover_coordination_request"]; exists && isFailoverRequest == "true" {
			logger.Info().
				Str("agent_id", req.LogUpdate.AgentId).
				Str("process_id", req.LogUpdate.ProcessId).
				Int("metadata_count", len(metadata)).
				Msg("Failover koordinasyon talebi algÄ±landÄ±")
			logger.Debug().
				Str("agent_id", req.LogUpdate.AgentId).
				Interface("metadata", metadata).
				Msg("Coordination metadata detaylarÄ±")

			// ðŸ”§ FIX: AkÄ±llÄ± duplicate coordination prevention check
			coordinationKey := fmt.Sprintf("%s_%s_%s_%s",
				req.LogUpdate.ProcessId,
				metadata["old_master_host"],
				metadata["new_master_host"],
				metadata["action"])

			s.coordinationMu.Lock()
			lastProcessed, alreadyProcessed := s.processedCoordinations[coordinationKey]

			// ðŸ”§ FIX: Reduced timeout from 2 minutes to 30 seconds for faster retry
			// This allows users to retry promotion sooner if the first attempt fails
			if alreadyProcessed && time.Since(lastProcessed) < 30*time.Second {
				s.coordinationMu.Unlock()
				logger.Warn().
					Str("coordination_key", coordinationKey).
					Dur("last_processed_ago", time.Since(lastProcessed).Round(time.Second)).
					Msg("Duplicate coordination request skipped (within 30 seconds)")
				return &pb.ProcessLogResponse{
					Status:  "ok",
					Message: "Process loglarÄ± baÅŸarÄ±yla alÄ±ndÄ± (duplicate coordination skipped)",
				}, nil
			}

			// Bu coordination'Ä± iÅŸlenmiÅŸ olarak iÅŸaretle
			s.processedCoordinations[coordinationKey] = time.Now()

			// ðŸ”§ FIX: More aggressive cleanup - every coordination request triggers cleanup
			// This helps prevent memory leaks and stuck coordination states
			go s.cleanupOldCoordinations()

			s.coordinationMu.Unlock()

			logger.Info().
				Str("coordination_key", coordinationKey).
				Bool("was_previously_processed", alreadyProcessed).
				Msg("Coordination request accepted")

			// Koordinasyon iÅŸlemini baÅŸlat
			logger.Info().
				Str("coordination_key", coordinationKey).
				Str("agent_id", req.LogUpdate.AgentId).
				Msg("Coordination iÅŸlemi goroutine'de baÅŸlatÄ±lÄ±yor")
			go s.handleFailoverCoordination(req.LogUpdate, req.LogUpdate.AgentId)
		}

		// 2. Coordination Completion (Eski master'dan gelen)
		logger.Debug().Msg("Checking for coordination_completion metadata...")
		if coordinationCompletionValue, exists := metadata["coordination_completion"]; exists {
			logger.Debug().
				Str("completion_value", coordinationCompletionValue).
				Bool("exists", exists).
				Msg("coordination_completion metadata found")
			if coordinationCompletionValue == "true" {
				logger.Info().
					Str("agent_id", req.LogUpdate.AgentId).
					Str("process_id", req.LogUpdate.ProcessId).
					Int("metadata_count", len(metadata)).
					Msg("Coordination completion bildirimi algÄ±landÄ±")
				logger.Debug().
					Str("agent_id", req.LogUpdate.AgentId).
					Interface("metadata", metadata).
					Msg("Completion metadata detaylarÄ±")

				// Coordination completion iÅŸlemini handle et
				logger.Debug().Msg("handleCoordinationCompletion fonksiyonu Ã§aÄŸrÄ±lÄ±yor")
				go s.handleCoordinationCompletion(req.LogUpdate, req.LogUpdate.AgentId)
			} else {
				logger.Debug().
					Str("completion_value", coordinationCompletionValue).
					Msg("coordination_completion metadata var ama 'true' deÄŸil")
			}
		} else {
			logger.Debug().Msg("coordination_completion metadata bulunamadÄ±")
		}

		// 3. Coordination Rollback Request (Rollback sÄ±rasÄ±nda gelen)
		if rollbackRequest, exists := metadata["coordination_rollback_request"]; exists && rollbackRequest == "true" {
			logger.Info().
				Str("agent_id", req.LogUpdate.AgentId).
				Str("process_id", req.LogUpdate.ProcessId).
				Msg("Coordination rollback request detected")

			// Rollback parametrelerini al
			targetHost := metadata["rollback_target_host"]
			targetIP := metadata["rollback_target_ip"]
			rollbackAction := metadata["rollback_action"]
			rollbackReason := metadata["rollback_reason"]
			originalMasterHost := metadata["original_master_host"]
			originalMasterIP := metadata["original_master_ip"]

			logger.Info().
				Str("target_host", targetHost).
				Str("target_ip", targetIP).
				Str("rollback_action", rollbackAction).
				Str("rollback_reason", rollbackReason).
				Str("original_master_host", originalMasterHost).
				Str("original_master_ip", originalMasterIP).
				Msg("Coordination rollback parameters")

			// Asenkron olarak koordinasyon rollback'i baÅŸlat
			go s.handleCoordinationRollback(targetHost, targetIP, originalMasterHost, originalMasterIP, rollbackReason)
		}
	}

	// Normal process log iÅŸleme devam et...
	err := s.saveProcessLogs(ctx, req.LogUpdate)
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.LogUpdate.AgentId).
			Str("process_id", req.LogUpdate.ProcessId).
			Msg("Process loglarÄ± veritabanÄ±na kaydedilemedi")
		return &pb.ProcessLogResponse{
			Status:  "error",
			Message: fmt.Sprintf("VeritabanÄ± hatasÄ±: %v", err),
		}, nil
	}

	return &pb.ProcessLogResponse{
		Status:  "ok",
		Message: "Process loglarÄ± baÅŸarÄ±yla alÄ±ndÄ±",
	}, nil
}

// saveProcessLogs, iÅŸlem loglarÄ±nÄ± veritabanÄ±na kaydeder
func (s *Server) saveProcessLogs(ctx context.Context, logUpdate *pb.ProcessLogUpdate) error {
	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// Metadata'yÄ± JSON formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
	metadataJSON, err := json.Marshal(logUpdate.Metadata)
	if err != nil {
		return fmt.Errorf("metadata JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
	}

	// Log mesajlarÄ±nÄ± JSON formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
	logMessagesJSON, err := json.Marshal(logUpdate.LogMessages)
	if err != nil {
		return fmt.Errorf("log mesajlarÄ± JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
	}

	// Ä°ÅŸlemin var olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	var processExists bool
	err = s.db.QueryRowContext(ctx, `
		SELECT EXISTS (
			SELECT 1 FROM process_logs 
			WHERE process_id = $1 AND agent_id = $2
		)
	`, logUpdate.ProcessId, logUpdate.AgentId).Scan(&processExists)

	if err != nil {
		return fmt.Errorf("iÅŸlem kaydÄ± kontrolÃ¼ sÄ±rasÄ±nda hata: %v", err)
	}

	if !processExists {
		// Ä°ÅŸlem ilk kez kaydediliyor
		logger.Info().
			Str("process_id", logUpdate.ProcessId).
			Str("agent_id", logUpdate.AgentId).
			Msg("Yeni iÅŸlem kaydÄ± oluÅŸturuluyor")

		insertQuery := `
			INSERT INTO process_logs (
				process_id,
				agent_id,
				process_type,
				status,
				log_messages,
				elapsed_time_s,
				metadata,
				created_at,
				updated_at
			) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $8)
		`

		_, err = s.db.ExecContext(ctx, insertQuery,
			logUpdate.ProcessId,
			logUpdate.AgentId,
			logUpdate.ProcessType,
			logUpdate.Status,
			logMessagesJSON,
			logUpdate.ElapsedTimeS,
			metadataJSON,
			time.Now(),
		)

		if err != nil {
			return fmt.Errorf("yeni iÅŸlem kaydÄ± eklenirken hata: %v", err)
		}
	} else {
		// Mevcut iÅŸlem gÃ¼ncelleniyor
		logger.Debug().
			Str("process_id", logUpdate.ProcessId).
			Str("agent_id", logUpdate.AgentId).
			Msg("Mevcut iÅŸlem kaydÄ± gÃ¼ncelleniyor")

		// Mevcut log mesajlarÄ±nÄ± ve status'u al
		var existingLogs []byte
		var currentStatus string
		err = s.db.QueryRowContext(ctx, `
    SELECT log_messages, status FROM process_logs 
    WHERE process_id = $1 AND agent_id = $2
`, logUpdate.ProcessId, logUpdate.AgentId).Scan(&existingLogs, &currentStatus)

		if err != nil {
			return fmt.Errorf("mevcut log mesajlarÄ± alÄ±nÄ±rken hata: %v", err)
		}

		// ðŸ”§ FIX: EÄŸer process zaten 'completed' veya 'failed' durumundaysa,
		// sadece log mesajlarÄ±nÄ± ekle, status'u deÄŸiÅŸtirme (race condition korumasÄ±)
		finalStatus := logUpdate.Status
		if currentStatus == "completed" || currentStatus == "failed" {
			logger.Debug().
				Str("process_id", logUpdate.ProcessId).
				Str("current_status", currentStatus).
				Str("incoming_status", logUpdate.Status).
				Msg("Process zaten final durumda, status korunuyor")
			finalStatus = currentStatus // Mevcut final status'u koru
		}

		// Mevcut log mesajlarÄ±nÄ± Ã§Ã¶zÃ¼mle
		var existingLogMessages []string
		if err := json.Unmarshal(existingLogs, &existingLogMessages); err != nil {
			return fmt.Errorf("mevcut log mesajlarÄ± ayrÄ±ÅŸtÄ±rÄ±lÄ±rken hata: %v", err)
		}

		// Yeni log mesajlarÄ±nÄ± ekle
		combinedLogMessages := append(existingLogMessages, logUpdate.LogMessages...)

		// BirleÅŸtirilmiÅŸ log mesajlarÄ±nÄ± JSON'a dÃ¶nÃ¼ÅŸtÃ¼r
		combinedLogsJSON, err := json.Marshal(combinedLogMessages)
		if err != nil {
			return fmt.Errorf("birleÅŸtirilmiÅŸ log mesajlarÄ± JSON dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: %v", err)
		}

		// GÃ¼ncelleme sorgusunu Ã§alÄ±ÅŸtÄ±r
		updateQuery := `
			UPDATE process_logs SET 
				status = $1,
				log_messages = $2,
				elapsed_time_s = $3,
				metadata = $4,
				updated_at = $5
			WHERE process_id = $6 AND agent_id = $7
		`

		_, err = s.db.ExecContext(ctx, updateQuery,
			finalStatus,
			combinedLogsJSON,
			logUpdate.ElapsedTimeS,
			metadataJSON,
			time.Now(),
			logUpdate.ProcessId,
			logUpdate.AgentId,
		)

		if err != nil {
			return fmt.Errorf("iÅŸlem kaydÄ± gÃ¼ncellenirken hata: %v", err)
		}
	}

	// EÄŸer iÅŸlem tamamlandÄ±ysa veya baÅŸarÄ±sÄ±z olduysa, job durumunu da gÃ¼ncelle
	if logUpdate.Status == "completed" || logUpdate.Status == "failed" {
		logger.Info().
			Str("process_id", logUpdate.ProcessId).
			Str("status", logUpdate.Status).
			Str("agent_id", logUpdate.AgentId).
			Msg("Process tamamlandÄ±, job durumu gÃ¼ncelleniyor")

		// Coordination job'larÄ± iÃ§in Ã¶zel kontrol - handleCoordinationCompletion zaten hallediyor
		if logUpdate.Metadata != nil {
			if coordinationCompletion, exists := logUpdate.Metadata["coordination_completion"]; exists && coordinationCompletion == "true" {
				logger.Debug().
					Str("process_id", logUpdate.ProcessId).
					Msg("Process coordination job olduÄŸu iÃ§in saveProcessLogs'da job update SKIP edildi (handleCoordinationCompletion halletti)")
				return nil
			}
		}

		// ðŸ”§ FIX: Find and update job without holding lock while calling updateJobInDatabase
		s.jobMu.RLock()
		job, exists := s.jobs[logUpdate.ProcessId]

		// Create a copy if job exists
		var jobCopy *pb.Job
		if exists {
			jobCopy = &pb.Job{
				JobId:        job.JobId,
				AgentId:      job.AgentId,
				Type:         job.Type,
				Status:       job.Status,
				Result:       job.Result,
				ErrorMessage: job.ErrorMessage,
				CreatedAt:    job.CreatedAt,
				UpdatedAt:    job.UpdatedAt,
				Parameters:   make(map[string]string),
			}
			// Copy parameters
			for k, v := range job.Parameters {
				jobCopy.Parameters[k] = v
			}
		}
		s.jobMu.RUnlock()

		if exists {
			// Job durumunu gÃ¼ncelle
			if logUpdate.Status == "completed" {
				jobCopy.Status = pb.JobStatus_JOB_STATUS_COMPLETED
				jobCopy.Result = "Job completed successfully by agent process logger"

				// EÄŸer bu bir coordination job ise, Ã¶zel iÅŸlem yap
				if jobCopy.Type == pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE {
					logger.Info().
						Str("job_id", jobCopy.JobId).
						Msg("Coordination job tamamlandÄ±")
					jobCopy.Result = "PostgreSQL convert to slave coordination completed successfully"
				}
			} else {
				jobCopy.Status = pb.JobStatus_JOB_STATUS_FAILED
				jobCopy.ErrorMessage = "Job failed as reported by agent process logger"

				// EÄŸer bu bir coordination job ise, Ã¶zel hata iÅŸlemi yap
				if jobCopy.Type == pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE {
					logger.Error().
						Str("job_id", jobCopy.JobId).
						Msg("Coordination job baÅŸarÄ±sÄ±z")
					jobCopy.ErrorMessage = "PostgreSQL convert to slave coordination failed"
				}
			}

			jobCopy.UpdatedAt = timestamppb.Now()

			// Update in memory (short lock)
			s.jobMu.Lock()
			s.jobs[logUpdate.ProcessId] = jobCopy
			s.jobMu.Unlock()

			// ðŸ”§ FIX: Database update without holding any locks
			err = s.updateJobInDatabase(context.Background(), jobCopy)
			if err != nil {
				logger.Error().
					Err(err).
					Str("job_id", jobCopy.JobId).
					Str("process_id", logUpdate.ProcessId).
					Msg("Job durumu veritabanÄ±nda gÃ¼ncellenirken hata")
			} else {
				if jobCopy.Type == pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE {
					logger.Info().
						Str("process_id", logUpdate.ProcessId).
						Str("job_status", jobCopy.Status.String()).
						Msg("Coordination job veritabanÄ±nda gÃ¼ncellendi")
				} else {
					logger.Info().
						Str("process_id", logUpdate.ProcessId).
						Str("job_status", jobCopy.Status.String()).
						Msg("Job durumu baÅŸarÄ±yla gÃ¼ncellendi")
				}
			}
		} else {
			logger.Warn().
				Str("process_id", logUpdate.ProcessId).
				Str("agent_id", logUpdate.AgentId).
				Str("status", logUpdate.Status).
				Msg("Process ID'ye karÅŸÄ±lÄ±k gelen job bulunamadÄ±")
		}
	}

	logger.Debug().
		Str("process_id", logUpdate.ProcessId).
		Str("agent_id", logUpdate.AgentId).
		Str("status", logUpdate.Status).
		Msg("Process loglarÄ± baÅŸarÄ±yla kaydedildi")
	return nil
}

// GetProcessStatus, belirli bir iÅŸlemin durumunu sorgular
func (s *Server) GetProcessStatus(ctx context.Context, req *pb.ProcessStatusRequest) (*pb.ProcessStatusResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("process_id", req.ProcessId).
		Msg("GetProcessStatus metodu Ã§aÄŸrÄ±ldÄ±")

	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return nil, fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// SQL sorgusu hazÄ±rla
	var query string
	var args []interface{}

	if req.AgentId != "" {
		// Agent ID verilmiÅŸse hem process_id hem agent_id ile sorgula
		query = `
			SELECT process_id, agent_id, process_type, status, log_messages, elapsed_time_s, metadata, created_at, updated_at
			FROM process_logs
			WHERE process_id = $1 AND agent_id = $2
		`
		args = []interface{}{req.ProcessId, req.AgentId}
	} else {
		// Agent ID verilmemiÅŸse sadece process_id ile sorgula
		query = `
			SELECT process_id, agent_id, process_type, status, log_messages, elapsed_time_s, metadata, created_at, updated_at
			FROM process_logs
			WHERE process_id = $1
		`
		args = []interface{}{req.ProcessId}
	}

	logger.Debug().
		Str("sql_query", query).
		Interface("parameters", args).
		Str("agent_id", req.AgentId).
		Str("process_id", req.ProcessId).
		Msg("Process sorgusu")

	var (
		processID       string
		agentID         string
		processType     string
		status          string
		logMessagesJSON []byte
		elapsedTimeS    float32
		metadataJSON    []byte
		createdAt       time.Time
		updatedAt       time.Time
	)

	// Sorguyu Ã§alÄ±ÅŸtÄ±r
	err := s.db.QueryRowContext(ctx, query, args...).Scan(
		&processID,
		&agentID,
		&processType,
		&status,
		&logMessagesJSON,
		&elapsedTimeS,
		&metadataJSON,
		&createdAt,
		&updatedAt,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			logger.Warn().
				Str("process_id", req.ProcessId).
				Str("agent_id", req.AgentId).
				Msg("Process bulunamadÄ±")
			return nil, fmt.Errorf("process bulunamadÄ±: %s", req.ProcessId)
		}
		logger.Error().
			Err(err).
			Str("process_id", req.ProcessId).
			Str("agent_id", req.AgentId).
			Msg("Process bilgileri alÄ±nÄ±rken hata")
		return nil, fmt.Errorf("process bilgileri alÄ±nÄ±rken hata: %v", err)
	}

	// Log mesajlarÄ±nÄ± Ã§Ã¶zÃ¼mle
	var logMessages []string
	if err := json.Unmarshal(logMessagesJSON, &logMessages); err != nil {
		logger.Error().
			Err(err).
			Str("process_id", req.ProcessId).
			Str("agent_id", req.AgentId).
			Msg("Log mesajlarÄ± ayrÄ±ÅŸtÄ±rÄ±lÄ±rken hata")
		return nil, fmt.Errorf("log mesajlarÄ± ayrÄ±ÅŸtÄ±rÄ±lÄ±rken hata: %v", err)
	}

	// Metadata'yÄ± Ã§Ã¶zÃ¼mle
	var metadata map[string]string
	if err := json.Unmarshal(metadataJSON, &metadata); err != nil {
		logger.Error().
			Err(err).
			Str("process_id", req.ProcessId).
			Str("agent_id", req.AgentId).
			Msg("Metadata ayrÄ±ÅŸtÄ±rÄ±lÄ±rken hata")
		return nil, fmt.Errorf("metadata ayrÄ±ÅŸtÄ±rÄ±lÄ±rken hata: %v", err)
	}

	logger.Debug().
		Str("process_id", processID).
		Str("process_type", processType).
		Str("status", status).
		Int("log_count", len(logMessages)).
		Str("agent_id", req.AgentId).
		Msg("Process bilgileri baÅŸarÄ±yla alÄ±ndÄ±")

	return &pb.ProcessStatusResponse{
		ProcessId:    processID,
		ProcessType:  processType,
		Status:       status,
		ElapsedTimeS: elapsedTimeS,
		LogMessages:  logMessages,
		CreatedAt:    createdAt.Format(time.RFC3339),
		UpdatedAt:    updatedAt.Format(time.RFC3339),
		Metadata:     metadata,
	}, nil
}

// handleFailoverCoordination failover koordinasyon iÅŸlemini yÃ¶netir
func (s *Server) handleFailoverCoordination(update *pb.ProcessLogUpdate, requestingAgentId string) {
	metadata := update.Metadata

	oldMasterHost := metadata["old_master_host"]
	newMasterHost := metadata["new_master_host"]
	newMasterIp := metadata["new_master_ip"] // âœ… YENÄ°: new_master_ip'yi metadata'dan al
	dataDirectory := metadata["data_directory"]
	replUser := metadata["replication_user"]
	replPass := metadata["replication_pass"]
	if replPass == "" {
		// Agent'Ä±n gÃ¶nderdiÄŸi alternatif anahtar adÄ±nÄ± kontrol et
		replPass = metadata["replication_password"]
	}
	action := metadata["action"]

	logger.Info().
		Str("old_master_host", oldMasterHost).
		Str("new_master_host", newMasterHost).
		Str("new_master_ip", newMasterIp).
		Str("requesting_agent_id", requestingAgentId).
		Str("action", action).
		Str("data_directory", dataDirectory).
		Str("replication_user", replUser).
		Bool("replication_password_provided", replPass != "").
		Msg("Failover koordinasyon iÅŸlemi baÅŸlatÄ±lÄ±yor")

	if action != "convert_master_to_slave" || oldMasterHost == "" {
		logger.Error().
			Str("action", action).
			Str("old_master_host", oldMasterHost).
			Msg("GeÃ§ersiz failover koordinasyon talebi")
		return
	}

	// Replication bilgilerini kontrol et
	if replUser == "" || replPass == "" {
		logger.Error().
			Str("replication_user", replUser).
			Bool("replication_password_provided", replPass != "").
			Msg("Replication bilgileri eksik")
		return
	}

	// GÃ¼venlik kontrolÃ¼: Talep eden agent'Ä±n yeni master olduÄŸunu doÄŸrula
	expectedRequestingAgent := fmt.Sprintf("agent_%s", newMasterHost)
	if requestingAgentId != expectedRequestingAgent {
		logger.Warn().
			Str("expected_agent", expectedRequestingAgent).
			Str("requesting_agent_id", requestingAgentId).
			Str("new_master_host", newMasterHost).
			Msg("GÃ¼venlik uyarÄ±sÄ±: Koordinasyon talebi beklenmeyen agent'dan geldi")
		// Ä°steÄŸe baÄŸlÄ±: Bu durumda iÅŸlemi durdurabilirsin
		// return
	} else {
		logger.Info().
			Str("requesting_agent_id", requestingAgentId).
			Str("expected_agent", expectedRequestingAgent).
			Msg("GÃ¼venlik kontrolÃ¼ baÅŸarÄ±lÄ±: Talep eden agent doÄŸru")
	}

	// ðŸ”§ FIX: Lock sÄ±rasÄ±nÄ± tutarlÄ± hale getir - Ã–NCE agents, SONRA jobs
	// Eski master agent'Ä±nÄ± bul
	oldMasterAgentId := fmt.Sprintf("agent_%s", oldMasterHost)
	logger.Info().
		Str("old_master_agent_id", oldMasterAgentId).
		Str("old_master_host", oldMasterHost).
		Msg("Eski master agent aranÄ±yor")

	s.mu.RLock()
	oldMasterAgent, exists := s.agents[oldMasterAgentId]
	agentCount := len(s.agents)

	// Mevcut agent'larÄ± da al
	var availableAgents []string
	for agentId := range s.agents {
		availableAgents = append(availableAgents, agentId)
	}
	// Agent bilgisini kopyala (lock'u erken bÄ±rakmak iÃ§in)
	var agentStreamCopy pb.AgentService_ConnectServer
	if exists {
		agentStreamCopy = oldMasterAgent.Stream
	}
	s.mu.RUnlock()

	logger.Debug().
		Int("total_agents", agentCount).
		Str("old_master_agent_id", oldMasterAgentId).
		Bool("agent_found", exists).
		Strs("available_agents", availableAgents).
		Msg("Agent durumu")

	if !exists {
		logger.Error().
			Str("old_master_agent_id", oldMasterAgentId).
			Strs("available_agents", availableAgents).
			Int("total_agents", agentCount).
			Msg("Eski master agent bulunamadÄ±")
		return
	}

	logger.Info().
		Str("old_master_agent_id", oldMasterAgentId).
		Msg("Eski master agent bulundu")

	// Job oluÅŸtur
	jobID := fmt.Sprintf("coord_%d", time.Now().UnixNano())
	logger.Info().
		Str("job_id", jobID).
		Str("old_master_host", oldMasterHost).
		Msg("Coordination job oluÅŸturuluyor")

	job := &pb.Job{
		JobId:     jobID,
		Type:      pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE,
		Status:    pb.JobStatus_JOB_STATUS_PENDING,
		AgentId:   oldMasterAgentId,
		CreatedAt: timestamppb.Now(),
		UpdatedAt: timestamppb.Now(),
		Parameters: map[string]string{
			"old_master_host":      oldMasterHost,
			"new_master_host":      newMasterHost,
			"data_directory":       dataDirectory,
			"replication_user":     replUser,
			"replication_password": replPass,
			"requesting_agent_id":  requestingAgentId, // Koordinasyon talep eden agent
		},
	}

	// Job'Ä± kaydet (lock sequence: agents -> jobs)
	s.jobMu.Lock()
	s.jobs[jobID] = job
	s.jobMu.Unlock()
	logger.Info().
		Str("job_id", jobID).
		Msg("Coordination job kaydedildi")

	// ðŸ”§ FIX: Coordination job ID'sini promotion metadata'sÄ±na ekle (job oluÅŸturulduktan SONRA)
	go s.addCoordinationJobIdToPromotionMetadata(requestingAgentId, newMasterHost, jobID)

	// ConvertPostgresToSlave komutunu gÃ¶nder
	// Format: convert_postgres_to_slave|new_master_host|new_master_ip|port|data_dir|coordination_job_id|old_master_host
	// NOT: Replication bilgileri artÄ±k agent config'inden alÄ±nacak (gÃ¼venlik)

	// âœ… FIX: newMasterIp artÄ±k metadata'dan alÄ±nÄ±yor, fallback gerekirse uygula
	if newMasterIp == "" {
		newMasterIp = newMasterHost // Fallback: hostname'i IP olarak kullan
		logger.Warn().
			Str("new_master_host", newMasterHost).
			Msg("new_master_ip metadata'da boÅŸ, hostname fallback kullanÄ±lÄ±yor")
	}

	command := fmt.Sprintf("convert_postgres_to_slave|%s|%s|5432|%s|%s|%s",
		newMasterHost, newMasterIp, dataDirectory, jobID, oldMasterHost)

	logger.Debug().
		Str("command", command).
		Str("old_master_agent_id", oldMasterAgentId).
		Str("job_id", jobID).
		Msg("Convert komutu hazÄ±rlandÄ±")
	logger.Info().
		Str("old_master_agent_id", oldMasterAgentId).
		Str("old_master_host", oldMasterHost).
		Msg("Convert komutu agent'a gÃ¶nderiliyor")

	// ðŸ”§ FIX: Timeout ile Stream.Send() Ã§aÄŸrÄ±sÄ±
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	// Channel kullanarak timeout kontrollÃ¼ gÃ¶nderme
	sendDone := make(chan error, 1)
	go func() {
		err := agentStreamCopy.Send(&pb.ServerMessage{
			Payload: &pb.ServerMessage_Query{
				Query: &pb.Query{
					QueryId: jobID,
					Command: command,
				},
			},
		})
		sendDone <- err
	}()

	// Timeout veya baÅŸarÄ± durumunu bekle
	select {
	case err := <-sendDone:
		if err != nil {
			logger.Error().
				Err(err).
				Str("requesting_agent_id", requestingAgentId).
				Str("old_master_agent_id", oldMasterAgentId).
				Str("job_id", jobID).
				Msg("Eski master'a convert komutu gÃ¶nderilemedi")
			job.Status = pb.JobStatus_JOB_STATUS_FAILED
			job.ErrorMessage = fmt.Sprintf("Agent'a komut gÃ¶nderilemedi: %v", err)
			job.UpdatedAt = timestamppb.Now()
		} else {
			logger.Info().
				Str("old_master_agent_id", oldMasterAgentId).
				Str("old_master_host", oldMasterHost).
				Str("new_master_host", newMasterHost).
				Str("job_id", jobID).
				Str("requesting_agent_id", requestingAgentId).
				Msg("Eski master'a convert komutu baÅŸarÄ±yla gÃ¶nderildi")
			job.Status = pb.JobStatus_JOB_STATUS_RUNNING
			job.UpdatedAt = timestamppb.Now()

			// ðŸŒ‰ PROCESS LOG BRIDGE: Coordination baÅŸlatÄ±ldÄ±ÄŸÄ±nÄ± promotion process logs'una ekle
			go s.bridgeCoordinationLogToPromotion(requestingAgentId, newMasterHost,
				fmt.Sprintf("[%s] Coordination baÅŸlatÄ±ldÄ±: Eski master (%s) slave'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...",
					time.Now().Format("15:04:05"), oldMasterHost))

			// ðŸš€ YENÄ°: DiÄŸer slave node'larÄ± iÃ§in reconfiguration komutlarÄ± gÃ¶nder
			go s.handleSlaveReconfiguration(metadata, newMasterHost, newMasterIp, requestingAgentId, jobID)
		}
	case <-ctx.Done():
		logger.Error().
			Str("old_master_agent_id", oldMasterAgentId).
			Str("job_id", jobID).
			Msg("Timeout: Eski master'a convert komutu gÃ¶nderilemedi (10s timeout)")
		job.Status = pb.JobStatus_JOB_STATUS_FAILED
		job.ErrorMessage = "Timeout: Agent'a komut gÃ¶nderilemedi (10s timeout)"
		job.UpdatedAt = timestamppb.Now()
	}

	// ðŸ”§ FIX: Database update iÃ§in de timeout context kullan
	dbCtx, dbCancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer dbCancel()

	if err := s.updateJobInDatabase(dbCtx, job); err != nil {
		logger.Error().
			Err(err).
			Str("job_id", jobID).
			Msg("Job veritabanÄ±nda gÃ¼ncellenirken hata")
	} else {
		logger.Info().
			Str("job_id", jobID).
			Str("job_status", job.Status.String()).
			Msg("Coordination job durumu gÃ¼ncellendi")
	}
}

// handleCoordinationCompletion coordination completion iÅŸlemini yÃ¶netir
func (s *Server) handleCoordinationCompletion(update *pb.ProcessLogUpdate, reportingAgentId string) {
	logger.Info().
		Str("reporting_agent_id", reportingAgentId).
		Str("process_id", update.ProcessId).
		Msg("handleCoordinationCompletion fonksiyonu baÅŸladÄ±")

	metadata := update.Metadata
	if metadata == nil {
		logger.Error().Msg("Metadata nil! handleCoordinationCompletion'dan Ã§Ä±kÄ±lÄ±yor")
		return
	}

	coordinationJobId := metadata["coordination_job_id"]
	oldMasterHost := metadata["old_master_host"]
	newMasterHost := metadata["new_master_host"]
	action := metadata["action"]
	status := metadata["completion_status"]              // "success" or "failed"
	requestingAgentId := metadata["requesting_agent_id"] // AsÄ±l requesting agent (promotion yapan)

	logger.Info().
		Str("coordination_job_id", coordinationJobId).
		Str("old_master_host", oldMasterHost).
		Str("new_master_host", newMasterHost).
		Str("action", action).
		Str("completion_status", status).
		Str("reporting_agent_id", reportingAgentId).
		Str("requesting_agent_id", requestingAgentId).
		Msg("Coordination completion iÅŸlemi baÅŸlatÄ±lÄ±yor")

	if coordinationJobId == "" {
		logger.Error().Msg("Coordination job ID eksik, completion iÅŸlemi yapÄ±lamÄ±yor")
		return
	}

	// ðŸ”§ FIX: Find and update job without holding lock while calling other functions
	logger.Debug().
		Str("coordination_job_id", coordinationJobId).
		Msg("Job aranÄ±yor")

	// First find the job
	s.jobMu.RLock()
	var existingJobIds []string
	var existingJobTypes []string
	for jobId, job := range s.jobs {
		existingJobIds = append(existingJobIds, jobId)
		existingJobTypes = append(existingJobTypes, job.Type.String())
	}

	logger.Debug().
		Int("total_jobs", len(s.jobs)).
		Strs("job_ids", existingJobIds).
		Strs("job_types", existingJobTypes).
		Msg("Mevcut job'lar")

	job, exists := s.jobs[coordinationJobId]
	// Create a copy to avoid pointer sharing
	var jobCopy *pb.Job
	if exists {
		jobCopy = &pb.Job{
			JobId:        job.JobId,
			AgentId:      job.AgentId,
			Type:         job.Type,
			Status:       job.Status,
			Result:       job.Result,
			ErrorMessage: job.ErrorMessage,
			CreatedAt:    job.CreatedAt,
			UpdatedAt:    job.UpdatedAt,
			Parameters:   make(map[string]string),
		}
		// Copy parameters
		for k, v := range job.Parameters {
			jobCopy.Parameters[k] = v
		}
	}
	s.jobMu.RUnlock()

	if !exists {
		logger.Error().
			Str("coordination_job_id", coordinationJobId).
			Strs("available_job_ids", existingJobIds).
			Msg("Coordination job bulunamadÄ±")
		return
	}

	logger.Info().
		Str("coordination_job_id", coordinationJobId).
		Str("job_status", jobCopy.Status.String()).
		Msg("Coordination job bulundu")

	// Job durumunu gÃ¼ncelle
	logger.Info().
		Str("coordination_job_id", coordinationJobId).
		Str("incoming_status", status).
		Msg("Job status gÃ¼ncelleniyor")

	oldStatus := jobCopy.Status.String()
	if status == "success" || status == "completed" {
		jobCopy.Status = pb.JobStatus_JOB_STATUS_COMPLETED
		jobCopy.Result = fmt.Sprintf("Coordination completed successfully: %s converted to slave by %s", oldMasterHost, reportingAgentId)
		// FIX: Completed job'Ä± active coordination jobs map'inden sil
		s.jobMu.Lock()
		delete(s.jobs, coordinationJobId)
		s.jobMu.Unlock()

		// FIX: Completed job'Ä±n processed coordination key'ini de temizle
		s.coordinationMu.Lock()
		coordinationKey := fmt.Sprintf("%s_%s_%s_convert_master_to_slave", coordinationJobId, oldMasterHost, newMasterHost)
		delete(s.processedCoordinations, coordinationKey)
		s.coordinationMu.Unlock()

		logger.Info().
			Str("coordination_job_id", coordinationJobId).
			Str("old_master_host", oldMasterHost).
			Str("reporting_agent_id", reportingAgentId).
			Msg("Coordination job baÅŸarÄ±yla tamamlandÄ± ve temizlendi")
	} else {
		jobCopy.Status = pb.JobStatus_JOB_STATUS_FAILED
		jobCopy.ErrorMessage = fmt.Sprintf("Coordination failed: %s could not be converted to slave", oldMasterHost)
		logger.Error().
			Str("coordination_job_id", coordinationJobId).
			Str("old_master_host", oldMasterHost).
			Msg("Coordination job baÅŸarÄ±sÄ±z")
	}

	jobCopy.UpdatedAt = timestamppb.Now()

	// Update in memory (short lock) - only if not completed
	if jobCopy.Status != pb.JobStatus_JOB_STATUS_COMPLETED {
		s.jobMu.Lock()
		s.jobs[coordinationJobId] = jobCopy
		s.jobMu.Unlock()

		logger.Info().
			Str("coordination_job_id", coordinationJobId).
			Str("old_status", oldStatus).
			Str("new_status", jobCopy.Status.String()).
			Msg("Job status deÄŸiÅŸimi")
	}

	// ðŸ”§ FIX: Database update without holding any locks
	logger.Debug().
		Str("coordination_job_id", coordinationJobId).
		Msg("VeritabanÄ±nda job gÃ¼ncelleniyor")
	err := s.updateJobInDatabase(context.Background(), jobCopy)
	if err != nil {
		logger.Error().
			Err(err).
			Str("coordination_job_id", coordinationJobId).
			Msg("Job veritabanÄ±nda gÃ¼ncellenirken hata")
	} else {
		logger.Info().
			Str("coordination_job_id", coordinationJobId).
			Str("job_status", jobCopy.Status.String()).
			Msg("Coordination job veritabanÄ±nda gÃ¼ncellendi")
	}

	// ðŸš€ BONUS: Ä°lgili promotion job'unu da complete et
	// EÄŸer coordination baÅŸarÄ±lÄ± olduysa, requesting agent'Ä±n promotion job'unu da tamamla
	if status == "success" || status == "completed" {
		logger.Info().
			Str("coordination_job_id", coordinationJobId).
			Str("requesting_agent_id", requestingAgentId).
			Msg("Ä°lgili promotion job'u aranÄ±yor ve complete ediliyor")

		// ðŸ”§ FIX: Bridge iÃ§in doÄŸru agent ID'sini belirle (deadlock'u Ã¶nlemek iÃ§in)
		bridgeAgentId := requestingAgentId
		if bridgeAgentId == "" {
			logger.Debug().
				Str("new_master_host", newMasterHost).
				Msg("requesting_agent_id bulunamadÄ±, new_master_host ile agent aranÄ±yor")

			// Agent'larÄ± ayrÄ± scope'da ara (deadlock'u Ã¶nlemek iÃ§in)
			foundAgentId := ""
			s.mu.RLock()
			for agentId := range s.agents {
				// Agent ID formatÄ± genellikle "agent_hostname" ÅŸeklinde
				// agent_skadi -> skadi Ã§Ä±kar
				if strings.HasPrefix(agentId, "agent_") {
					hostname := strings.TrimPrefix(agentId, "agent_")
					if hostname == newMasterHost {
						foundAgentId = agentId
						break
					}
				}
			}
			s.mu.RUnlock()

			if foundAgentId != "" {
				bridgeAgentId = foundAgentId
				logger.Info().
					Str("new_master_host", newMasterHost).
					Str("bridge_agent_id", bridgeAgentId).
					Msg("new_master_host ile eÅŸleÅŸen agent bulundu")
			} else {
				logger.Warn().
					Str("new_master_host", newMasterHost).
					Str("fallback_agent_id", reportingAgentId).
					Msg("new_master_host ile eÅŸleÅŸen agent bulunamadÄ±, fallback olarak reportingAgentId kullanÄ±lÄ±yor")
				bridgeAgentId = reportingAgentId
			}
		}

		// ðŸŒ‰ BRIDGE: Coordination completion logunu promotion process'e ekle
		displayOldMaster := oldMasterHost
		if displayOldMaster == "" {
			displayOldMaster = reportingAgentId // veya "bilinmeyen"
		}
		completionMessage := fmt.Sprintf("[%s] âœ… Coordination tamamlandÄ±: Eski master (%s) baÅŸarÄ±yla slave'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼!",
			time.Now().Format("15:04:05"), displayOldMaster)
		go s.bridgeCoordinationLogToPromotion(bridgeAgentId, newMasterHost, completionMessage)

		// ðŸ”§ NEW: Coordination completion'Ä± promotion metadata'sÄ±na ekle
		go s.updateCoordinationStatusInPromotionMetadata(bridgeAgentId, newMasterHost, coordinationJobId, "completed")

		// Final completion message
		finalMessage := fmt.Sprintf("[%s] ðŸŽ‰ PostgreSQL Failover baÅŸarÄ±yla tamamlandÄ±! (%s -> %s)",
			time.Now().Format("15:04:05"), oldMasterHost, newMasterHost)
		go s.bridgeCoordinationLogToPromotion(bridgeAgentId, newMasterHost, finalMessage)

		// Auto-complete related promotion job
		s.completeRelatedPromotionJob(bridgeAgentId, newMasterHost)
	} else {
		// ðŸ”§ FIX: Bridge iÃ§in doÄŸru agent ID'sini belirle (failure case - deadlock'u Ã¶nlemek iÃ§in)
		bridgeAgentId := requestingAgentId
		if bridgeAgentId == "" {
			logger.Debug().
				Str("new_master_host", newMasterHost).
				Msg("requesting_agent_id bulunamadÄ± (failure case), new_master_host ile agent aranÄ±yor")

			// Agent'larÄ± ayrÄ± scope'da ara (deadlock'u Ã¶nlemek iÃ§in)
			foundAgentId := ""
			s.mu.RLock()
			for agentId := range s.agents {
				// Agent ID formatÄ± genellikle "agent_hostname" ÅŸeklinde
				if strings.HasPrefix(agentId, "agent_") {
					hostname := strings.TrimPrefix(agentId, "agent_")
					if hostname == newMasterHost {
						foundAgentId = agentId
						break
					}
				}
			}
			s.mu.RUnlock()

			if foundAgentId != "" {
				bridgeAgentId = foundAgentId
				logger.Info().
					Str("new_master_host", newMasterHost).
					Str("bridge_agent_id", bridgeAgentId).
					Msg("new_master_host ile eÅŸleÅŸen agent bulundu (failure case)")
			} else {
				logger.Warn().
					Str("new_master_host", newMasterHost).
					Str("fallback_agent_id", reportingAgentId).
					Msg("new_master_host ile eÅŸleÅŸen agent bulunamadÄ± (failure case), fallback olarak reportingAgentId kullanÄ±lÄ±yor")
				bridgeAgentId = reportingAgentId
			}
		}

		// ðŸŒ‰ BRIDGE: Coordination failure logunu promotion process'e ekle
		failureMessage := fmt.Sprintf("[%s] âŒ Coordination baÅŸarÄ±sÄ±z: Eski master (%s) slave'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lemedi!",
			time.Now().Format("15:04:05"), oldMasterHost)
		go s.bridgeCoordinationLogToPromotion(bridgeAgentId, newMasterHost, failureMessage)

		// ðŸ”§ NEW: Coordination failure'Ä± promotion metadata'sÄ±na ekle
		go s.updateCoordinationStatusInPromotionMetadata(bridgeAgentId, newMasterHost, coordinationJobId, "failed")
	}

	// Coordination completion sonrasÄ±nda duplicate prevention key'ini temizle
	s.cleanupCompletedCoordination(coordinationJobId, oldMasterHost, newMasterHost)

	logger.Info().
		Str("coordination_job_id", coordinationJobId).
		Str("completion_status", status).
		Msg("Coordination completion iÅŸlemi tamamlandÄ±")
}

// handleSlaveReconfiguration diÄŸer slave node'larÄ± iÃ§in reconfiguration komutlarÄ± gÃ¶nderir
func (s *Server) handleSlaveReconfiguration(metadata map[string]string, newMasterHost, newMasterIp, requestingAgentId, parentJobId string) {
	logger.Info().
		Str("new_master_host", newMasterHost).
		Str("new_master_ip", newMasterIp).
		Str("requesting_agent_id", requestingAgentId).
		Str("parent_job_id", parentJobId).
		Msg("Slave reconfiguration iÅŸlemi baÅŸlatÄ±lÄ±yor")

	// Metadata'dan slave bilgilerini al
	slaveCount := 0
	if slaveCountStr, exists := metadata["slave_count"]; exists {
		if count, err := strconv.Atoi(slaveCountStr); err == nil {
			slaveCount = count
		}
	}

	logger.Info().
		Int("slave_count", slaveCount).
		Msg("Reconfigure edilecek slave sayÄ±sÄ±")

	if slaveCount == 0 {
		logger.Info().Msg("Reconfigure edilecek slave node yok")
		return
	}

	// Her slave iÃ§in reconfiguration komutu gÃ¶nder
	for i := 0; i < slaveCount; i++ {
		slaveHostnameKey := fmt.Sprintf("slave_%d_hostname", i)
		slaveIpKey := fmt.Sprintf("slave_%d_ip", i)

		slaveHostname, hostnameExists := metadata[slaveHostnameKey]
		slaveIp, ipExists := metadata[slaveIpKey]

		if !hostnameExists || !ipExists || slaveHostname == "" {
			logger.Warn().
				Int("slave_index", i).
				Str("hostname_key", slaveHostnameKey).
				Str("ip_key", slaveIpKey).
				Bool("hostname_exists", hostnameExists).
				Bool("ip_exists", ipExists).
				Str("hostname", slaveHostname).
				Msg("Slave bilgileri eksik, atlanÄ±yor")
			continue
		}

		logger.Info().
			Int("slave_index", i).
			Str("slave_hostname", slaveHostname).
			Str("slave_ip", slaveIp).
			Msg("Slave reconfiguration komutu gÃ¶nderiliyor")

		// Slave agent'Ä±nÄ± bul
		slaveAgentId := fmt.Sprintf("agent_%s", slaveHostname)

		s.mu.RLock()
		slaveAgent, exists := s.agents[slaveAgentId]
		var slaveStreamCopy pb.AgentService_ConnectServer
		if exists {
			slaveStreamCopy = slaveAgent.Stream
		}
		s.mu.RUnlock()

		if !exists {
			logger.Error().
				Str("slave_agent_id", slaveAgentId).
				Str("slave_hostname", slaveHostname).
				Msg("Slave agent bulunamadÄ±, reconfiguration atlanÄ±yor")

			// Bridge log gÃ¶nder
			go s.bridgeCoordinationLogToPromotion(requestingAgentId, newMasterHost,
				fmt.Sprintf("[%s] âš ï¸ Slave (%s) agent bulunamadÄ±, reconfiguration atlandÄ±",
					time.Now().Format("15:04:05"), slaveHostname))
			continue
		}

		// Reconfiguration komutu oluÅŸtur
		// Format: reconfigure_slave_master|new_master_host|new_master_ip|new_master_port|parent_job_id
		reconfigCommand := fmt.Sprintf("reconfigure_slave_master|%s|%s|5432|%s",
			newMasterHost, newMasterIp, parentJobId)

		logger.Debug().
			Str("reconfigure_command", reconfigCommand).
			Str("slave_agent_id", slaveAgentId).
			Str("slave_hostname", slaveHostname).
			Msg("Reconfiguration komutu hazÄ±rlandÄ±")

		// Unique query ID oluÅŸtur
		queryID := fmt.Sprintf("reconfig_%s_%d", parentJobId, i)

		// Timeout ile komut gÃ¶nder
		ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)

		// Channel kullanarak timeout kontrollÃ¼ gÃ¶nderme
		sendDone := make(chan error, 1)
		go func(agentStream pb.AgentService_ConnectServer, qID, cmd string) {
			err := agentStream.Send(&pb.ServerMessage{
				Payload: &pb.ServerMessage_Query{
					Query: &pb.Query{
						QueryId: qID,
						Command: cmd,
					},
				},
			})
			sendDone <- err
		}(slaveStreamCopy, queryID, reconfigCommand)

		// Timeout veya baÅŸarÄ± durumunu bekle
		select {
		case err := <-sendDone:
			if err != nil {
				logger.Error().
					Err(err).
					Str("slave_agent_id", slaveAgentId).
					Str("slave_hostname", slaveHostname).
					Str("query_id", queryID).
					Msg("Slave reconfiguration komutu gÃ¶nderilemedi")

				// Bridge log gÃ¶nder
				go s.bridgeCoordinationLogToPromotion(requestingAgentId, newMasterHost,
					fmt.Sprintf("[%s] âŒ Slave (%s) reconfiguration komutu gÃ¶nderilemedi: %v",
						time.Now().Format("15:04:05"), slaveHostname, err))
			} else {
				logger.Info().
					Str("slave_agent_id", slaveAgentId).
					Str("slave_hostname", slaveHostname).
					Str("query_id", queryID).
					Str("new_master_host", newMasterHost).
					Str("new_master_ip", newMasterIp).
					Msg("Slave reconfiguration komutu baÅŸarÄ±yla gÃ¶nderildi")

				// Bridge log gÃ¶nder
				go s.bridgeCoordinationLogToPromotion(requestingAgentId, newMasterHost,
					fmt.Sprintf("[%s] ðŸ”„ Slave (%s) reconfiguration baÅŸlatÄ±ldÄ±: %s -> %s",
						time.Now().Format("15:04:05"), slaveHostname, slaveHostname, newMasterHost))
			}
		case <-ctx.Done():
			logger.Error().
				Str("slave_agent_id", slaveAgentId).
				Str("slave_hostname", slaveHostname).
				Str("query_id", queryID).
				Msg("Timeout: Slave reconfiguration komutu gÃ¶nderilemedi (10s timeout)")

			// Bridge log gÃ¶nder
			go s.bridgeCoordinationLogToPromotion(requestingAgentId, newMasterHost,
				fmt.Sprintf("[%s] â° Slave (%s) reconfiguration timeout (10s)",
					time.Now().Format("15:04:05"), slaveHostname))
		}

		cancel()
	}

	logger.Info().
		Int("slave_count", slaveCount).
		Str("new_master_host", newMasterHost).
		Msg("Slave reconfiguration komutlarÄ± gÃ¶nderildi")
}

// completeRelatedPromotionJob ilgili promotion job'unu complete eder
func (s *Server) completeRelatedPromotionJob(requestingAgentId, newMasterHost string) {
	logger.Info().
		Str("requesting_agent_id", requestingAgentId).
		Str("new_master_host", newMasterHost).
		Msg("Agent'Ä±nÄ±n promotion job'u aranÄ±yor")

	// ðŸ”§ FIX: Find the job first without holding the lock for too long
	var promotionJob *pb.Job
	var promotionJobId string

	s.jobMu.RLock()
	// Requesting agent'a ait RUNNING promotion job'larÄ±nÄ± ara
	for jobId, job := range s.jobs {
		// Promotion job kriterleri:
		// 1. Agent ID eÅŸleÅŸmeli
		// 2. Type postgresql_promotion olmalÄ±
		// 3. Status RUNNING olmalÄ±
		// 4. Node hostname new master ile eÅŸleÅŸmeli (opsiyonel kontrol)
		if job.AgentId == requestingAgentId &&
			job.Type == pb.JobType_JOB_TYPE_POSTGRES_PROMOTE_MASTER &&
			job.Status == pb.JobStatus_JOB_STATUS_RUNNING {

			// Node hostname kontrolÃ¼ (eÄŸer varsa)
			if nodeHostname, exists := job.Parameters["node_hostname"]; exists {
				if nodeHostname == newMasterHost {
					// Job'un kopyasÄ±nÄ± oluÅŸtur (pointer paylaÅŸÄ±mÄ±ndan kaÃ§Ä±nmak iÃ§in)
					promotionJob = &pb.Job{
						JobId:        job.JobId,
						AgentId:      job.AgentId,
						Type:         job.Type,
						Status:       job.Status,
						Result:       job.Result,
						ErrorMessage: job.ErrorMessage,
						CreatedAt:    job.CreatedAt,
						UpdatedAt:    job.UpdatedAt,
						Parameters:   make(map[string]string),
					}
					// Parameters'Ä± kopyala
					for k, v := range job.Parameters {
						promotionJob.Parameters[k] = v
					}
					promotionJobId = jobId
					logger.Info().
						Str("job_id", jobId).
						Str("node_hostname", nodeHostname).
						Str("new_master_host", newMasterHost).
						Msg("EÅŸleÅŸen promotion job bulundu")
					break
				} else {
					logger.Debug().
						Str("job_id", jobId).
						Str("node_hostname", nodeHostname).
						Str("new_master_host", newMasterHost).
						Msg("Promotion job bulundu ama node hostname eÅŸleÅŸmiyor")
				}
			} else {
				// Node hostname yoksa, agent ve type eÅŸleÅŸen ilk job'u al
				promotionJob = &pb.Job{
					JobId:        job.JobId,
					AgentId:      job.AgentId,
					Type:         job.Type,
					Status:       job.Status,
					Result:       job.Result,
					ErrorMessage: job.ErrorMessage,
					CreatedAt:    job.CreatedAt,
					UpdatedAt:    job.UpdatedAt,
					Parameters:   make(map[string]string),
				}
				// Parameters'Ä± kopyala
				for k, v := range job.Parameters {
					promotionJob.Parameters[k] = v
				}
				promotionJobId = jobId
				logger.Info().
					Str("job_id", jobId).
					Msg("Promotion job bulundu (node hostname bilgisi yok)")
				break
			}
		}
	}
	s.jobMu.RUnlock()

	if promotionJob == nil {
		logger.Warn().
			Str("requesting_agent_id", requestingAgentId).
			Str("new_master_host", newMasterHost).
			Msg("Agent iÃ§in RUNNING promotion job bulunamadÄ±")

		// Debug: Mevcut job'larÄ± listele (ayrÄ± lock ile)
		s.jobMu.RLock()
		var agentJobs []string
		for jobId, job := range s.jobs {
			if job.AgentId == requestingAgentId {
				agentJobs = append(agentJobs, fmt.Sprintf("%s:%s:%s", jobId, job.Type.String(), job.Status.String()))
			}
		}
		s.jobMu.RUnlock()

		logger.Debug().
			Str("requesting_agent_id", requestingAgentId).
			Strs("agent_jobs", agentJobs).
			Msg("Mevcut job'lar")
		return
	}

	// ðŸ”§ FIX: Update the job without holding lock for too long
	oldStatus := promotionJob.Status.String()
	promotionJob.Status = pb.JobStatus_JOB_STATUS_COMPLETED
	promotionJob.Result = fmt.Sprintf("PostgreSQL promotion completed successfully. Coordination also completed for old master conversion.")
	promotionJob.UpdatedAt = timestamppb.Now()

	// Now update in memory (short lock)
	s.jobMu.Lock()
	s.jobs[promotionJobId] = promotionJob
	s.jobMu.Unlock()

	logger.Info().
		Str("promotion_job_id", promotionJobId).
		Str("old_status", oldStatus).
		Str("new_status", promotionJob.Status.String()).
		Msg("Promotion job status deÄŸiÅŸimi")

	// ðŸ”§ FIX: Database and process logs updates without holding any locks
	// VeritabanÄ±nda da gÃ¼ncelle (no locks held)
	err := s.updateJobInDatabase(context.Background(), promotionJob)
	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_job_id", promotionJobId).
			Msg("Promotion job veritabanÄ±nda gÃ¼ncellenirken hata")
	} else {
		logger.Info().
			Str("promotion_job_id", promotionJobId).
			Str("job_status", promotionJob.Status.String()).
			Msg("Promotion job veritabanÄ±nda gÃ¼ncellendi")
	}

	// ðŸ”§ FIX: Mevcut metadata'yÄ± al ve completion bilgilerini ekle (override etme!)
	completionMetadata := make(map[string]string)

	// VeritabanÄ±ndan mevcut metadata'yÄ± al
	var completionMetadataJSON []byte
	completionDbErr := s.db.QueryRow(`
		SELECT metadata FROM process_logs 
		WHERE process_id = $1 AND agent_id = $2
	`, promotionJobId, requestingAgentId).Scan(&completionMetadataJSON)

	if completionDbErr == nil && len(completionMetadataJSON) > 0 {
		// Mevcut metadata'yÄ± parse et
		if parseErr := json.Unmarshal(completionMetadataJSON, &completionMetadata); parseErr != nil {
			logger.Warn().
				Err(parseErr).
				Str("promotion_job_id", promotionJobId).
				Msg("COMPLETION: Mevcut metadata parse edilemedi, yeni metadata oluÅŸturuluyor")
			completionMetadata = make(map[string]string)
		}
	}

	// Completion bilgilerini mevcut metadata'ya ekle (override etmeden)
	completionMetadata["auto_completed"] = "true"
	completionMetadata["completion_source"] = "coordination_system"
	completionMetadata["completed_at"] = time.Now().Format(time.RFC3339)

	// ðŸ”§ FIX: Process logs tablosunu da "completed" olarak gÃ¼ncelle (no locks held)
	completionLogUpdate := &pb.ProcessLogUpdate{
		AgentId:      requestingAgentId,
		ProcessId:    promotionJobId,
		ProcessType:  "postgresql_promotion",
		Status:       "completed", // Running'den completed'e gÃ¼ncelle
		LogMessages:  []string{fmt.Sprintf("[%s] ðŸŽ‰ PostgreSQL promotion baÅŸarÄ±yla tamamlandÄ±! (Auto-completed by coordination system)", time.Now().Format("15:04:05"))},
		ElapsedTimeS: 0,
		UpdatedAt:    time.Now().Format(time.RFC3339),
		Metadata:     completionMetadata, // Merged metadata kullan
	}

	err = s.saveProcessLogs(context.Background(), completionLogUpdate)
	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_job_id", promotionJobId).
			Msg("Promotion process logs gÃ¼ncellenirken hata")
	} else {
		logger.Info().
			Str("promotion_job_id", promotionJobId).
			Msg("Promotion process logs da 'completed' olarak gÃ¼ncellendi")
	}

	logger.Info().
		Str("promotion_job_id", promotionJobId).
		Str("requesting_agent_id", requestingAgentId).
		Msg("Ä°lgili promotion job baÅŸarÄ±yla complete edildi")
}

// addCoordinationJobIdToPromotionMetadata coordination job ID'sini promotion process metadata'sÄ±na ekler
func (s *Server) addCoordinationJobIdToPromotionMetadata(requestingAgentId, newMasterHost, coordinationJobId string) {
	logger.Debug().
		Str("requesting_agent_id", requestingAgentId).
		Str("new_master_host", newMasterHost).
		Str("coordination_job_id", coordinationJobId).
		Msg("Coordination job ID promotion metadata'sÄ±na ekleniyor")

	// ðŸ”§ FIX: Find promotion process ID without holding lock
	var promotionProcessId string

	s.jobMu.RLock()
	var availableJobs []string
	for jobId, job := range s.jobs {
		availableJobs = append(availableJobs, fmt.Sprintf("%s:%s:%s", jobId, job.Type.String(), job.AgentId))

		if job.AgentId == requestingAgentId &&
			job.Type == pb.JobType_JOB_TYPE_POSTGRES_PROMOTE_MASTER &&
			(job.Status == pb.JobStatus_JOB_STATUS_RUNNING || job.Status == pb.JobStatus_JOB_STATUS_COMPLETED) {

			// Node hostname kontrolÃ¼ (opsiyonel)
			if nodeHostname, exists := job.Parameters["node_hostname"]; exists {
				if nodeHostname == newMasterHost {
					promotionProcessId = jobId
					logger.Info().
						Str("job_id", jobId).
						Str("node_hostname", nodeHostname).
						Str("new_master_host", newMasterHost).
						Msg("âœ… Promotion process ID bulundu (hostname match)")
					break
				}
			} else {
				// Node hostname yoksa ilk eÅŸleÅŸen job'u al
				promotionProcessId = jobId
				logger.Info().
					Str("job_id", jobId).
					Msg("âœ… Promotion process ID bulundu (hostname yok)")
				break
			}
		}
	}
	s.jobMu.RUnlock()

	logger.Debug().
		Str("requesting_agent_id", requestingAgentId).
		Str("new_master_host", newMasterHost).
		Strs("available_jobs", availableJobs).
		Str("found_promotion_process_id", promotionProcessId).
		Msg("Job arama sonucu")

	if promotionProcessId == "" {
		logger.Warn().
			Str("requesting_agent_id", requestingAgentId).
			Str("new_master_host", newMasterHost).
			Msg("Promotion process bulunamadÄ±, coordination job ID eklenemedi")
		return
	}

	// Mevcut metadata'yÄ± al ve coordination job ID'sini ekle
	var existingMetadata map[string]string
	var existingMetadataJSON []byte

	err := s.db.QueryRow(`
		SELECT metadata FROM process_logs 
		WHERE process_id = $1 AND agent_id = $2
	`, promotionProcessId, requestingAgentId).Scan(&existingMetadataJSON)

	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Msg("Promotion process metadata alÄ±namadÄ±")
		return
	}

	// Mevcut metadata'yÄ± parse et
	if err := json.Unmarshal(existingMetadataJSON, &existingMetadata); err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Msg("Promotion process metadata parse edilemedi")
		return
	}

	// Coordination job ID'sini ekle
	existingMetadata["coordination_job_id"] = coordinationJobId
	existingMetadata["coordination_status"] = "started"
	existingMetadata["coordination_added_at"] = time.Now().Format(time.RFC3339)

	logger.Info().
		Str("promotion_process_id", promotionProcessId).
		Str("coordination_job_id", coordinationJobId).
		Interface("updated_metadata", existingMetadata).
		Msg("Metadata gÃ¼ncelleniyor")

	// GÃ¼ncellenmiÅŸ metadata'yÄ± JSON'a Ã§evir
	updatedMetadataJSON, err := json.Marshal(existingMetadata)
	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Msg("GÃ¼ncellenmiÅŸ metadata JSON'a Ã§evrilemedi")
		return
	}

	logger.Debug().
		Str("promotion_process_id", promotionProcessId).
		Str("requesting_agent_id", requestingAgentId).
		Str("updated_metadata_json", string(updatedMetadataJSON)).
		Msg("Database update SQL Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor")

	// VeritabanÄ±nda gÃ¼ncelle
	result, err := s.db.Exec(`
		UPDATE process_logs SET 
			metadata = $1,
			updated_at = $2
		WHERE process_id = $3 AND agent_id = $4
	`, updatedMetadataJSON, time.Now(), promotionProcessId, requestingAgentId)

	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Str("coordination_job_id", coordinationJobId).
			Msg("âŒ Promotion process metadata gÃ¼ncellenemedi")
	} else {
		// Etkilenen satÄ±r sayÄ±sÄ±nÄ± kontrol et
		rowsAffected, _ := result.RowsAffected()
		logger.Info().
			Str("promotion_process_id", promotionProcessId).
			Str("coordination_job_id", coordinationJobId).
			Int64("rows_affected", rowsAffected).
			Msg("âœ… Coordination job ID baÅŸarÄ±yla promotion metadata'sÄ±na eklendi")
	}
}

// updateCoordinationStatusInPromotionMetadata coordination status'unu promotion metadata'sÄ±nda gÃ¼nceller
func (s *Server) updateCoordinationStatusInPromotionMetadata(requestingAgentId, newMasterHost, coordinationJobId, status string) {
	logger.Debug().
		Str("requesting_agent_id", requestingAgentId).
		Str("new_master_host", newMasterHost).
		Str("coordination_job_id", coordinationJobId).
		Str("status", status).
		Msg("Coordination status promotion metadata'sÄ±nda gÃ¼ncelleniyor")

	// ðŸ”§ FIX: Find promotion process ID without holding lock
	var promotionProcessId string

	s.jobMu.RLock()
	for jobId, job := range s.jobs {
		if job.AgentId == requestingAgentId &&
			job.Type == pb.JobType_JOB_TYPE_POSTGRES_PROMOTE_MASTER {

			// Node hostname kontrolÃ¼ (opsiyonel)
			if nodeHostname, exists := job.Parameters["node_hostname"]; exists {
				if nodeHostname == newMasterHost {
					promotionProcessId = jobId
					break
				}
			} else {
				// Node hostname yoksa ilk eÅŸleÅŸen job'u al
				promotionProcessId = jobId
				break
			}
		}
	}
	s.jobMu.RUnlock()

	if promotionProcessId == "" {
		logger.Warn().
			Str("requesting_agent_id", requestingAgentId).
			Str("new_master_host", newMasterHost).
			Msg("Promotion process bulunamadÄ±, coordination status gÃ¼ncellenemedi")
		return
	}

	// Mevcut metadata'yÄ± al ve coordination status'unu gÃ¼ncelle
	var existingMetadata map[string]string
	var existingMetadataJSON []byte

	err := s.db.QueryRow(`
		SELECT metadata FROM process_logs 
		WHERE process_id = $1 AND agent_id = $2
	`, promotionProcessId, requestingAgentId).Scan(&existingMetadataJSON)

	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Msg("Promotion process metadata alÄ±namadÄ±")
		return
	}

	// Mevcut metadata'yÄ± parse et
	if err := json.Unmarshal(existingMetadataJSON, &existingMetadata); err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Msg("Promotion process metadata parse edilemedi")
		return
	}

	// Coordination status'unu gÃ¼ncelle
	existingMetadata["coordination_status"] = status
	if status == "completed" {
		existingMetadata["coordination_completed_at"] = time.Now().Format(time.RFC3339)
	} else if status == "failed" {
		existingMetadata["coordination_failed_at"] = time.Now().Format(time.RFC3339)
	}

	// GÃ¼ncellenmiÅŸ metadata'yÄ± JSON'a Ã§evir
	updatedMetadataJSON, err := json.Marshal(existingMetadata)
	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Msg("GÃ¼ncellenmiÅŸ metadata JSON'a Ã§evrilemedi")
		return
	}

	// VeritabanÄ±nda gÃ¼ncelle
	_, err = s.db.Exec(`
		UPDATE process_logs SET 
			metadata = $1,
			updated_at = $2
		WHERE process_id = $3 AND agent_id = $4
	`, updatedMetadataJSON, time.Now(), promotionProcessId, requestingAgentId)

	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Str("coordination_job_id", coordinationJobId).
			Str("status", status).
			Msg("Promotion process coordination status gÃ¼ncellenemedi")
	} else {
		logger.Info().
			Str("promotion_process_id", promotionProcessId).
			Str("coordination_job_id", coordinationJobId).
			Str("status", status).
			Msg("Coordination status baÅŸarÄ±yla promotion metadata'sÄ±nda gÃ¼ncellendi")
	}
}

// bridgeCoordinationLogToPromotion coordination loglarÄ±nÄ± promotion process logs'una bridge eder
func (s *Server) bridgeCoordinationLogToPromotion(requestingAgentId, newMasterHost, logMessage string) {
	logger.Debug().
		Str("requesting_agent_id", requestingAgentId).
		Str("new_master_host", newMasterHost).
		Str("log_message", logMessage).
		Msg("BRIDGE: Agent'Ä±nÄ±n promotion process'ine log ekleniyor")

	// ðŸ”§ FIX: Find promotion process ID without holding lock while calling saveProcessLogs
	var promotionProcessId string

	s.jobMu.RLock()
	for jobId, job := range s.jobs {
		if job.AgentId == requestingAgentId &&
			job.Type == pb.JobType_JOB_TYPE_POSTGRES_PROMOTE_MASTER &&
			(job.Status == pb.JobStatus_JOB_STATUS_RUNNING || job.Status == pb.JobStatus_JOB_STATUS_COMPLETED) {

			// Node hostname kontrolÃ¼ (opsiyonel)
			if nodeHostname, exists := job.Parameters["node_hostname"]; exists {
				if nodeHostname == newMasterHost {
					promotionProcessId = jobId
					logger.Debug().
						Str("job_id", jobId).
						Str("node_hostname", nodeHostname).
						Str("new_master_host", newMasterHost).
						Msg("BRIDGE: Promotion process ID bulundu")
					break
				}
			} else {
				// Node hostname yoksa ilk eÅŸleÅŸen job'u al
				promotionProcessId = jobId
				logger.Debug().
					Str("job_id", jobId).
					Msg("BRIDGE: Promotion process ID bulundu (hostname yok)")
				break
			}
		}
	}
	s.jobMu.RUnlock()

	if promotionProcessId == "" {
		logger.Warn().
			Str("requesting_agent_id", requestingAgentId).
			Str("new_master_host", newMasterHost).
			Msg("BRIDGE: Agent'Ä±nÄ±n promotion process'i bulunamadÄ±")
		return
	}

	// ðŸ”§ FIX: Mevcut metadata'yÄ± al ve bridge bilgilerini ekle (override etme!)
	existingMetadata := make(map[string]string)

	// VeritabanÄ±ndan mevcut metadata'yÄ± al
	var metadataJSON []byte
	dbErr := s.db.QueryRow(`
		SELECT metadata FROM process_logs 
		WHERE process_id = $1 AND agent_id = $2
	`, promotionProcessId, requestingAgentId).Scan(&metadataJSON)

	if dbErr == nil && len(metadataJSON) > 0 {
		// Mevcut metadata'yÄ± parse et
		if parseErr := json.Unmarshal(metadataJSON, &existingMetadata); parseErr != nil {
			logger.Warn().
				Err(parseErr).
				Str("promotion_process_id", promotionProcessId).
				Msg("BRIDGE: Mevcut metadata parse edilemedi, yeni metadata oluÅŸturuluyor")
			existingMetadata = make(map[string]string)
		}
	}

	// Bridge bilgilerini mevcut metadata'ya ekle (override etmeden)
	existingMetadata["bridge_source"] = "coordination"
	existingMetadata["bridge_type"] = "coordination_update"
	existingMetadata["bridge_last_update"] = time.Now().Format(time.RFC3339)

	// ðŸ”§ FIX: Process log update oluÅŸtur ve kaydet (hiÃ§ lock tutmadan)
	logUpdate := &pb.ProcessLogUpdate{
		AgentId:      requestingAgentId,
		ProcessId:    promotionProcessId,
		ProcessType:  "postgresql_promotion",
		Status:       "running", // Hala devam ediyor
		LogMessages:  []string{logMessage},
		ElapsedTimeS: 0,
		UpdatedAt:    time.Now().Format(time.RFC3339),
		Metadata:     existingMetadata, // Merged metadata kullan
	}

	// Process logs'a kaydet (no locks held - prevents deadlock)
	err := s.saveProcessLogs(context.Background(), logUpdate)
	if err != nil {
		logger.Error().
			Err(err).
			Str("promotion_process_id", promotionProcessId).
			Str("requesting_agent_id", requestingAgentId).
			Msg("BRIDGE: Log kaydedilirken hata")
	} else {
		logger.Debug().
			Str("promotion_process_id", promotionProcessId).
			Str("requesting_agent_id", requestingAgentId).
			Msg("BRIDGE: Coordination log baÅŸarÄ±yla promotion process'e eklendi")
	}
}

// cleanupOldCoordinations, eski coordination kayÄ±tlarÄ±nÄ± temizler (memory leak prevention)
func (s *Server) cleanupOldCoordinations() {
	s.coordinationMu.Lock()
	defer s.coordinationMu.Unlock()

	cutoff := time.Now().Add(-5 * time.Minute) // ðŸ”§ FIX: 5 dakikadan eski olanlarÄ± temizle (daha agresif)
	keysToDelete := make([]string, 0)

	for key, timestamp := range s.processedCoordinations {
		if timestamp.Before(cutoff) {
			keysToDelete = append(keysToDelete, key)
		}
	}

	for _, key := range keysToDelete {
		delete(s.processedCoordinations, key)
	}

	if len(keysToDelete) > 0 {
		logger.Info().
			Int("cleaned_records", len(keysToDelete)).
			Dur("older_than", 5*time.Minute).
			Msg("Cleaned up old coordination records (aggressive cleanup)")
	}
}

// cleanupCompletedCoordination, tamamlanan coordination'Ä±n duplicate prevention key'ini temizler
func (s *Server) cleanupCompletedCoordination(coordinationJobId, oldMasterHost, newMasterHost string) {
	s.coordinationMu.Lock()
	defer s.coordinationMu.Unlock()

	// Coordination key formatÄ±nÄ± oluÅŸtur (aynÄ± format kullanÄ±lmalÄ±)
	// Format: {process_id}_{old_master_host}_{new_master_host}_{action}
	coordinationKey := fmt.Sprintf("%s_%s_%s_convert_master_to_slave",
		coordinationJobId,
		oldMasterHost,
		newMasterHost)

	// EÄŸer bu key varsa, temizle
	if _, exists := s.processedCoordinations[coordinationKey]; exists {
		delete(s.processedCoordinations, coordinationKey)
		logger.Info().
			Str("coordination_key", coordinationKey).
			Str("coordination_job_id", coordinationJobId).
			Msg("Completed coordination key cleaned up from duplicate prevention")
	}

	// Fallback: Job ID'si prefix olan tÃ¼m key'leri temizle (gÃ¼venlik Ã¶nlemi)
	keysToDelete := make([]string, 0)
	for key := range s.processedCoordinations {
		if strings.HasPrefix(key, coordinationJobId+"_") {
			keysToDelete = append(keysToDelete, key)
		}
	}

	for _, key := range keysToDelete {
		delete(s.processedCoordinations, key)
		logger.Debug().
			Str("fallback_key", key).
			Str("coordination_job_id", coordinationJobId).
			Msg("Fallback cleanup: removed coordination key with job ID prefix")
	}
}

// GetCoordinationStatus, mevcut coordination state'ini dÃ¶ndÃ¼rÃ¼r
func (s *Server) GetCoordinationStatus() (map[string]time.Time, map[string]*pb.Job) {
	// Coordination keys'i ayrÄ± lock ile al
	coordinationKeys := make(map[string]time.Time)
	s.coordinationMu.RLock()
	for k, v := range s.processedCoordinations {
		coordinationKeys[k] = v
	}
	s.coordinationMu.RUnlock()

	// Active coordination jobs'larÄ± ayrÄ± lock ile al (deadlock'u Ã¶nlemek iÃ§in)
	activeJobs := make(map[string]*pb.Job)
	s.jobMu.RLock()
	for jobId, job := range s.jobs {
		// Sadece CONVERT_TO_SLAVE tipindeki ve COMPLETED olmayan job'larÄ± al
		if job.Type == pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE &&
			job.Status != pb.JobStatus_JOB_STATUS_COMPLETED {
			// Job'un kopyasÄ±nÄ± oluÅŸtur (pointer referansÄ±ndan kaÃ§Ä±nmak iÃ§in)
			jobCopy := &pb.Job{
				JobId:        job.JobId,
				AgentId:      job.AgentId,
				Type:         job.Type,
				Status:       job.Status,
				Result:       job.Result,
				ErrorMessage: job.ErrorMessage,
				CreatedAt:    job.CreatedAt,
				UpdatedAt:    job.UpdatedAt,
				Parameters:   make(map[string]string),
			}
			// Parameters'Ä± kopyala
			for k, v := range job.Parameters {
				jobCopy.Parameters[k] = v
			}
			activeJobs[jobId] = jobCopy
		}
	}
	s.jobMu.RUnlock()

	return coordinationKeys, activeJobs
}

// CleanupAllCoordination, tÃ¼m coordination state'ini temizler
func (s *Server) CleanupAllCoordination() int {
	s.coordinationMu.Lock()
	defer s.coordinationMu.Unlock()

	count := len(s.processedCoordinations)
	s.processedCoordinations = make(map[string]time.Time)

	logger.Info().
		Int("cleaned_count", count).
		Msg("All coordination state cleaned up")

	return count
}

// CleanupOldCoordination, eski coordination kayÄ±tlarÄ±nÄ± temizler
func (s *Server) CleanupOldCoordination() int {
	s.coordinationMu.Lock()
	defer s.coordinationMu.Unlock()

	cutoff := time.Now().Add(-5 * time.Minute) // ðŸ”§ FIX: TutarlÄ± 5 dakika timeout
	keysToDelete := make([]string, 0)

	for key, timestamp := range s.processedCoordinations {
		if timestamp.Before(cutoff) {
			keysToDelete = append(keysToDelete, key)
		}
	}

	for _, key := range keysToDelete {
		delete(s.processedCoordinations, key)
	}

	if len(keysToDelete) > 0 {
		logger.Info().
			Int("cleaned_count", len(keysToDelete)).
			Dur("older_than", 5*time.Minute).
			Msg("Old coordination records cleaned up (API request)")
	}

	return len(keysToDelete)
}

// CleanupCoordinationKey, belirli bir coordination key'ini temizler
func (s *Server) CleanupCoordinationKey(key string) bool {
	s.coordinationMu.Lock()
	defer s.coordinationMu.Unlock()

	if _, exists := s.processedCoordinations[key]; exists {
		delete(s.processedCoordinations, key)
		logger.Info().
			Str("coordination_key", key).
			Msg("Specific coordination key cleaned up")
		return true
	}

	return false
}

// ðŸ”§ NEW: CleanupStuckCoordinationJobs cleans up coordination jobs that are stuck
func (s *Server) CleanupStuckCoordinationJobs() int {
	s.jobMu.Lock()
	defer s.jobMu.Unlock()

	cutoff := time.Now().Add(-10 * time.Minute) // Jobs stuck for more than 10 minutes
	stuckJobsCount := 0

	for jobId, job := range s.jobs {
		if job.Type == pb.JobType_JOB_TYPE_POSTGRES_CONVERT_TO_SLAVE {
			// Check if job is stuck in PENDING or RUNNING state for too long
			if (job.Status == pb.JobStatus_JOB_STATUS_PENDING || job.Status == pb.JobStatus_JOB_STATUS_RUNNING) &&
				job.CreatedAt.AsTime().Before(cutoff) {

				logger.Warn().
					Str("job_id", jobId).
					Str("job_status", job.Status.String()).
					Dur("age", time.Since(job.CreatedAt.AsTime())).
					Msg("Cleaning up stuck coordination job")

				// Mark job as failed
				job.Status = pb.JobStatus_JOB_STATUS_FAILED
				job.ErrorMessage = "Job timed out and was automatically cleaned up"
				job.UpdatedAt = timestamppb.Now()

				s.jobs[jobId] = job
				stuckJobsCount++

				// Also update in database
				go func(j *pb.Job) {
					ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
					defer cancel()
					if err := s.updateJobInDatabase(ctx, j); err != nil {
						logger.Error().
							Err(err).
							Str("job_id", j.JobId).
							Msg("Failed to update stuck job in database")
					}
				}(job)
			}
		}
	}

	if stuckJobsCount > 0 {
		logger.Info().
			Int("cleaned_jobs", stuckJobsCount).
			Msg("Cleaned up stuck coordination jobs")
	}

	return stuckJobsCount
}

// ðŸš¨ EMERGENCY: CleanupEmergencyDeadlock cleans up any potential deadlock situation
func (s *Server) CleanupEmergencyDeadlock() map[string]int {
	logger.Warn().Msg("ðŸš¨ EMERGENCY DEADLOCK CLEANUP BAÅžLADI")

	result := make(map[string]int)

	// 1. Clean all coordination state
	result["coordination_keys"] = s.CleanupAllCoordination()

	// 2. Clean stuck coordination jobs
	result["stuck_jobs"] = s.CleanupStuckCoordinationJobs()

	// 3. Clean ALL promotion jobs that are stuck (more aggressive)
	s.jobMu.Lock()
	promotionStuckCount := 0
	cutoff := time.Now().Add(-5 * time.Minute) // More aggressive - 5 minutes

	for jobId, job := range s.jobs {
		if job.Type == pb.JobType_JOB_TYPE_POSTGRES_PROMOTE_MASTER {
			if (job.Status == pb.JobStatus_JOB_STATUS_PENDING || job.Status == pb.JobStatus_JOB_STATUS_RUNNING) &&
				job.CreatedAt.AsTime().Before(cutoff) {

				logger.Warn().
					Str("job_id", jobId).
					Str("job_type", job.Type.String()).
					Str("job_status", job.Status.String()).
					Dur("age", time.Since(job.CreatedAt.AsTime())).
					Msg("ðŸš¨ EMERGENCY: Cleaning up stuck promotion job")

				job.Status = pb.JobStatus_JOB_STATUS_FAILED
				job.ErrorMessage = "Emergency cleanup: Job was stuck and cleaned up"
				job.UpdatedAt = timestamppb.Now()
				s.jobs[jobId] = job
				promotionStuckCount++
			}
		}
	}
	s.jobMu.Unlock()

	result["stuck_promotions"] = promotionStuckCount

	logger.Warn().
		Int("coordination_keys", result["coordination_keys"]).
		Int("stuck_jobs", result["stuck_jobs"]).
		Int("stuck_promotions", result["stuck_promotions"]).
		Msg("ðŸš¨ EMERGENCY CLEANUP TAMAMLANDI")

	return result
}

// ðŸ”§ NEW: ForceCompleteProcess manually completes a stuck process
func (s *Server) ForceCompleteProcess(processId string) bool {
	logger.Warn().
		Str("process_id", processId).
		Msg("ðŸ”§ FORCE COMPLETING STUCK PROCESS")

		// Check if process exists in database first
	var currentStatus string
	var agentId string
	var processType string

	err := s.db.QueryRow(`
		SELECT status, agent_id, process_type
		FROM process_logs 
		WHERE process_id = $1
	`, processId).Scan(&currentStatus, &agentId, &processType)

	if err != nil {
		logger.Error().
			Err(err).
			Str("process_id", processId).
			Msg("Process bulunamadÄ±")
		return false
	}
	logger.Info().
		Str("process_id", processId).
		Str("current_status", currentStatus).
		Str("agent_id", agentId).
		Str("process_type", processType).
		Msg("Process bulundu, zorla tamamlanÄ±yor")

	if currentStatus == "completed" {
		logger.Info().
			Str("process_id", processId).
			Msg("Process zaten completed durumunda")
		return true
	}

	// Force update process logs to completed
	completionTime := time.Now()

	// Get existing logs first
	var existingLogsJSON []byte
	err = s.db.QueryRow(`
		SELECT log_messages FROM process_logs WHERE process_id = $1
	`, processId).Scan(&existingLogsJSON)

	if err != nil {
		logger.Error().
			Err(err).
			Str("process_id", processId).
			Msg("Existing logs alÄ±namadÄ±")
		return false
	}

	// Parse existing logs
	var existingLogs []string
	if err := json.Unmarshal(existingLogsJSON, &existingLogs); err != nil {
		logger.Error().
			Err(err).
			Str("process_id", processId).
			Msg("Existing logs parse edilemedi")
		return false
	}

	// Add completion message
	completionMessage := fmt.Sprintf("[%s] ðŸ”§ FORCE COMPLETED: Process manually completed via API",
		completionTime.Format("15:04:05"))
	existingLogs = append(existingLogs, completionMessage)

	// Convert back to JSON
	updatedLogsJSON, err := json.Marshal(existingLogs)
	if err != nil {
		logger.Error().
			Err(err).
			Str("process_id", processId).
			Msg("Updated logs JSON'a Ã§evrilemedi")
		return false
	}

	// Update metadata to include force completion info
	metadata := map[string]string{
		"force_completed":   "true",
		"completion_source": "manual_api",
		"completion_time":   completionTime.Format(time.RFC3339),
	}
	metadataJSON, _ := json.Marshal(metadata)

	// Force update in database
	_, err = s.db.Exec(`
		UPDATE process_logs SET 
			status = 'completed',
			log_messages = $1,
			metadata = $2,
			updated_at = $3
		WHERE process_id = $4
	`, updatedLogsJSON, metadataJSON, completionTime, processId)

	if err != nil {
		logger.Error().
			Err(err).
			Str("process_id", processId).
			Msg("Process force completion veritabanÄ±nda gÃ¼ncellenemedi")
		return false
	}

	// Also update in-memory job if it exists
	s.jobMu.Lock()
	if job, jobExists := s.jobs[processId]; jobExists {
		job.Status = pb.JobStatus_JOB_STATUS_COMPLETED
		job.Result = "Process force completed via API"
		job.UpdatedAt = timestamppb.Now()
		s.jobs[processId] = job
	}
	s.jobMu.Unlock()

	logger.Warn().
		Str("process_id", processId).
		Str("agent_id", agentId).
		Str("process_type", processType).
		Msg("ðŸ”§ PROCESS FORCE COMPLETED SUCCESSFULLY")

	return true
}

// RollbackPostgresFailover PostgreSQL failover iÅŸlemini geri alÄ±r
func (s *Server) RollbackPostgresFailover(ctx context.Context, req *pb.PostgresRollbackRequest) (*pb.PostgresRollbackResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("job_id", req.JobId).
		Str("reason", req.Reason).
		Msg("PostgreSQL rollback iÅŸlemi baÅŸlatÄ±lÄ±yor")

	// Agent'Ä± bul
	s.mu.RLock()
	agent, exists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !exists {
		logger.Error().
			Str("agent_id", req.AgentId).
			Msg("Rollback iÃ§in agent bulunamadÄ±")
		return &pb.PostgresRollbackResponse{
			JobId:        req.JobId,
			Status:       pb.JobStatus_JOB_STATUS_FAILED,
			ErrorMessage: "Agent bulunamadÄ±",
		}, nil
	}

	// Rollback komutunu gÃ¶nder
	command := fmt.Sprintf("rollback_postgres_failover|%s|%s", req.JobId, req.Reason)

	logger.Debug().
		Str("agent_id", req.AgentId).
		Str("command", command).
		Msg("Rollback komutu agent'a gÃ¶nderiliyor")

	// Timeout ile komut gÃ¶nder
	ctx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	sendDone := make(chan error, 1)
	go func() {
		err := agent.Stream.Send(&pb.ServerMessage{
			Payload: &pb.ServerMessage_Query{
				Query: &pb.Query{
					QueryId: req.JobId,
					Command: command,
				},
			},
		})
		sendDone <- err
	}()

	select {
	case err := <-sendDone:
		if err != nil {
			logger.Error().
				Err(err).
				Str("agent_id", req.AgentId).
				Str("job_id", req.JobId).
				Msg("Rollback komutu gÃ¶nderilemedi")
			return &pb.PostgresRollbackResponse{
				JobId:        req.JobId,
				Status:       pb.JobStatus_JOB_STATUS_FAILED,
				ErrorMessage: fmt.Sprintf("Rollback komutu gÃ¶nderilemedi: %v", err),
			}, nil
		}
	case <-ctx.Done():
		logger.Error().
			Str("agent_id", req.AgentId).
			Str("job_id", req.JobId).
			Msg("Rollback komutu gÃ¶nderme timeout")
		return &pb.PostgresRollbackResponse{
			JobId:        req.JobId,
			Status:       pb.JobStatus_JOB_STATUS_FAILED,
			ErrorMessage: "Rollback komutu gÃ¶nderme timeout (10s)",
		}, nil
	}

	logger.Info().
		Str("agent_id", req.AgentId).
		Str("job_id", req.JobId).
		Msg("Rollback komutu baÅŸarÄ±yla gÃ¶nderildi")

	return &pb.PostgresRollbackResponse{
		JobId:  req.JobId,
		Status: pb.JobStatus_JOB_STATUS_PENDING,
		Result: "Rollback iÅŸlemi baÅŸlatÄ±ldÄ±",
	}, nil
}

// GetPostgresRollbackInfo PostgreSQL rollback durumunu sorgular
func (s *Server) GetPostgresRollbackInfo(ctx context.Context, req *pb.PostgresRollbackInfoRequest) (*pb.PostgresRollbackInfoResponse, error) {
	logger.Debug().
		Str("agent_id", req.AgentId).
		Msg("PostgreSQL rollback durumu sorgulanÄ±yor")

	// Agent'Ä± bul
	s.mu.RLock()
	agent, exists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !exists {
		logger.Warn().
			Str("agent_id", req.AgentId).
			Msg("Rollback info iÃ§in agent bulunamadÄ±")
		return &pb.PostgresRollbackInfoResponse{
			HasState: false,
		}, nil
	}

	// Rollback durumu sorgu komutunu gÃ¶nder
	command := "get_postgres_rollback_info"
	queryId := fmt.Sprintf("rollback_info_%d", time.Now().Unix())

	logger.Debug().
		Str("agent_id", req.AgentId).
		Str("query_id", queryId).
		Str("command", command).
		Msg("Rollback info komutu agent'a gÃ¶nderiliyor")

	// Timeout ile komut gÃ¶nder
	ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
	defer cancel()

	sendDone := make(chan error, 1)
	go func() {
		err := agent.Stream.Send(&pb.ServerMessage{
			Payload: &pb.ServerMessage_Query{
				Query: &pb.Query{
					QueryId: queryId,
					Command: command,
				},
			},
		})
		sendDone <- err
	}()

	select {
	case err := <-sendDone:
		if err != nil {
			logger.Error().
				Err(err).
				Str("agent_id", req.AgentId).
				Msg("Rollback info komutu gÃ¶nderilemedi")
			return &pb.PostgresRollbackInfoResponse{
				HasState: false,
			}, nil
		}
	case <-ctx.Done():
		logger.Error().
			Str("agent_id", req.AgentId).
			Msg("Rollback info komutu gÃ¶nderme timeout")
		return &pb.PostgresRollbackInfoResponse{
			HasState: false,
		}, nil
	}

	// TODO: Agent'tan gelen response'u beklemek iÃ§in async handling gerekebilir
	// Åžimdilik basic response dÃ¶ndÃ¼rÃ¼yoruz
	logger.Info().
		Str("agent_id", req.AgentId).
		Msg("Rollback info komutu baÅŸarÄ±yla gÃ¶nderildi")

	return &pb.PostgresRollbackInfoResponse{
		HasState: true,
		// DiÄŸer alanlar agent'tan gelen yanÄ±ta gÃ¶re doldurulacak
		// Bu kÄ±sÄ±m agent response handling sistemi ile tamamlanacak
	}, nil
}

// GetRecentAlarms, dashboard iÃ§in optimize edilmiÅŸ son alarmlarÄ± getirir
func (s *Server) GetRecentAlarms(ctx context.Context, limit int, onlyUnacknowledged bool) ([]map[string]interface{}, error) {
	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return nil, fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// VarsayÄ±lan limit kontrolÃ¼ (dashboard iÃ§in max 50)
	if limit <= 0 || limit > 50 {
		limit = 20 // Dashboard iÃ§in varsayÄ±lan 20 kayÄ±t
	}

	// Optimize edilmiÅŸ sorgu - sadece gerekli alanlar ve index kullanÄ±mÄ±
	query := `
		SELECT 
			alarm_id,
			event_id,
			agent_id,
			status,
			metric_name,
			metric_value,
			message,
			severity,
			created_at,
			acknowledged
		FROM alarms`

	var queryParams []interface{}
	if onlyUnacknowledged {
		query += ` WHERE acknowledged = false`
	}

	query += ` ORDER BY created_at DESC LIMIT $1`
	queryParams = append(queryParams, limit)

	logger.Debug().
		Int("limit", limit).
		Bool("only_unacknowledged", onlyUnacknowledged).
		Msg("Recent alarms sorgusu")

	// Sorguyu Ã§alÄ±ÅŸtÄ±r
	rows, err := s.db.QueryContext(ctx, query, queryParams...)
	if err != nil {
		return nil, fmt.Errorf("recent alarm verileri Ã§ekilemedi: %v", err)
	}
	defer rows.Close()

	// SonuÃ§larÄ± topla (hafif veri yapÄ±sÄ±)
	alarms := make([]map[string]interface{}, 0)
	for rows.Next() {
		var (
			alarmID      string
			eventID      string
			agentID      string
			status       string
			metricName   string
			metricValue  string
			message      string
			severity     string
			createdAt    time.Time
			acknowledged bool
		)

		// SatÄ±rÄ± oku
		err := rows.Scan(
			&alarmID,
			&eventID,
			&agentID,
			&status,
			&metricName,
			&metricValue,
			&message,
			&severity,
			&createdAt,
			&acknowledged,
		)
		if err != nil {
			return nil, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
		}

		// Hafif alarm objesi (dashboard iÃ§in gerekli minimum alanlar)
		alarm := map[string]interface{}{
			"alarm_id":     alarmID,
			"event_id":     eventID,
			"agent_id":     agentID,
			"status":       status,
			"metric_name":  metricName,
			"metric_value": metricValue,
			"message":      message,
			"severity":     severity,
			"created_at":   createdAt.Format(time.RFC3339),
			"acknowledged": acknowledged,
		}

		alarms = append(alarms, alarm)
	}

	// SatÄ±r okuma hatasÄ± kontrolÃ¼
	if err = rows.Err(); err != nil {
		return nil, fmt.Errorf("satÄ±r okuma hatasÄ±: %v", err)
	}

	return alarms, nil
}

// GetAlarmFilterValues, frontend filtreleme iÃ§in kullanÄ±lacak unique deÄŸerleri getirir
func (s *Server) GetAlarmFilterValues(ctx context.Context) (map[string][]string, error) {
	// VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kontrol et
	if err := s.checkDatabaseConnection(); err != nil {
		return nil, fmt.Errorf("veritabanÄ± baÄŸlantÄ± hatasÄ±: %v", err)
	}

	// Filtrelenebilir kolonlar
	filterableColumns := []string{
		"agent_id",
		"status",
		"metric_name",
		"severity",
		"database",
	}

	result := make(map[string][]string)

	// Her kolon iÃ§in unique deÄŸerleri Ã§ek
	for _, column := range filterableColumns {
		// NULL deÄŸerleri hariÃ§ tutarak unique deÄŸerleri al
		var query string
		if column == "database" {
			// Database kolonu NULL olabilir, NULL'larÄ± hariÃ§ tut
			query = fmt.Sprintf(`
				SELECT DISTINCT %s 
				FROM alarms 
				WHERE %s IS NOT NULL AND %s != ''
				ORDER BY %s ASC
			`, column, column, column, column)
		} else {
			// DiÄŸer kolonlar NOT NULL
			query = fmt.Sprintf(`
				SELECT DISTINCT %s 
				FROM alarms 
				WHERE %s IS NOT NULL AND %s != ''
				ORDER BY %s ASC
			`, column, column, column, column)
		}

		rows, err := s.db.QueryContext(ctx, query)
		if err != nil {
			logger.Error().
				Err(err).
				Str("column", column).
				Msg("Alarm filter deÄŸerleri Ã§ekilemedi")
			continue // Bu kolon iÃ§in hata olsa da diÄŸerleri iÃ§in devam et
		}

		var values []string
		for rows.Next() {
			var value string
			if err := rows.Scan(&value); err != nil {
				logger.Error().
					Err(err).
					Str("column", column).
					Msg("SatÄ±r okuma hatasÄ±")
				continue
			}

			// BoÅŸ olmayan deÄŸerleri ekle
			if strings.TrimSpace(value) != "" {
				values = append(values, value)
			}
		}
		rows.Close()

		// SatÄ±r okuma hatasÄ± kontrolÃ¼
		if err = rows.Err(); err != nil {
			logger.Error().
				Err(err).
				Str("column", column).
				Msg("SatÄ±r okuma hatasÄ±")
			continue
		}

		result[column] = values

		logger.Debug().
			Str("column", column).
			Int("value_count", len(values)).
			Msg("Alarm filter deÄŸerleri baÅŸarÄ±yla Ã§ekildi")
	}

	// AyrÄ±ca alarm_id'lerin unique deÄŸerlerini de ekle (Ã¶zel case)
	alarmIDQuery := `
		SELECT DISTINCT alarm_id 
		FROM alarms 
		WHERE alarm_id IS NOT NULL AND alarm_id != ''
		ORDER BY alarm_id ASC
		LIMIT 1000
	`

	rows, err := s.db.QueryContext(ctx, alarmIDQuery)
	if err == nil {
		var alarmIDs []string
		for rows.Next() {
			var alarmID string
			if err := rows.Scan(&alarmID); err != nil {
				continue
			}
			if strings.TrimSpace(alarmID) != "" {
				alarmIDs = append(alarmIDs, alarmID)
			}
		}
		rows.Close()
		result["alarm_id"] = alarmIDs

		logger.Debug().
			Int("alarm_id_count", len(alarmIDs)).
			Msg("Alarm ID filter deÄŸerleri baÅŸarÄ±yla Ã§ekildi")
	}

	logger.Info().
		Int("filter_categories", len(result)).
		Msg("Alarm filter deÄŸerleri baÅŸarÄ±yla tamamlandÄ±")

	return result, nil
}

func compareValues(a, b interface{}) bool {
	// Nil kontrolÃ¼
	if a == nil && b == nil {
		return true
	}
	if a == nil || b == nil {
		return false
	}

	// Numeric deÄŸerler iÃ§in tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yaparak karÅŸÄ±laÅŸtÄ±r
	aFloat, aIsNumeric := convertToFloat64(a)
	bFloat, bIsNumeric := convertToFloat64(b)

	if aIsNumeric && bIsNumeric {
		// Float karÅŸÄ±laÅŸtÄ±rmasÄ± iÃ§in tolerans kullan (floating point precision)
		return math.Abs(aFloat-bFloat) < 1e-9
	}

	// String karÅŸÄ±laÅŸtÄ±rmasÄ±
	if aStr, ok := a.(string); ok {
		if bStr, ok := b.(string); ok {
			return aStr == bStr
		}
		return false
	}

	// Boolean karÅŸÄ±laÅŸtÄ±rmasÄ±
	if aBool, ok := a.(bool); ok {
		if bBool, ok := b.(bool); ok {
			return aBool == bBool
		}
		return false
	}

	// Complex types (maps, slices) iÃ§in reflect.DeepEqual kullan
	return reflect.DeepEqual(a, b)
}

// convertToFloat64, farklÄ± numeric tiplerini float64'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
func convertToFloat64(v interface{}) (float64, bool) {
	switch val := v.(type) {
	case float64:
		return val, true
	case float32:
		return float64(val), true
	case int:
		return float64(val), true
	case int32:
		return float64(val), true
	case int64:
		return float64(val), true
	case uint:
		return float64(val), true
	case uint32:
		return float64(val), true
	case uint64:
		return float64(val), true
	default:
		return 0, false
	}
}

// handleCoordinationRollback koordinasyon rollback iÅŸlemini yÃ¶netir
func (s *Server) handleCoordinationRollback(targetHost, targetIP, originalMasterHost, originalMasterIP, reason string) {
	logger.Info().
		Str("target_host", targetHost).
		Str("target_ip", targetIP).
		Str("original_master_host", originalMasterHost).
		Str("original_master_ip", originalMasterIP).
		Str("reason", reason).
		Msg("Starting coordination rollback")

	// 1. Target node'u (Fenrir) STANDBY'ye dÃ¶ndÃ¼r
	if err := s.revertNodeToStandby(targetHost, targetIP, originalMasterHost, originalMasterIP); err != nil {
		logger.Error().
			Err(err).
			Str("target_host", targetHost).
			Msg("Failed to revert node to standby")
		// Hata durumunda alarm gÃ¶nder
		s.sendSplitBrainAlert(targetHost, originalMasterHost, err)
		return
	}

	logger.Info().
		Str("target_host", targetHost).
		Msg("Coordination rollback completed successfully - node reverted to STANDBY")
}

// revertNodeToStandby bir node'u STANDBY moduna dÃ¶ndÃ¼rÃ¼r
func (s *Server) revertNodeToStandby(targetHost, targetIP, newMasterHost, newMasterIP string) error {
	logger.Info().
		Str("target_host", targetHost).
		Str("target_ip", targetIP).
		Str("new_master_host", newMasterHost).
		Str("new_master_ip", newMasterIP).
		Msg("Reverting node to standby mode")

	// Target node'a convert_postgres_to_slave komutu gÃ¶nder
	convertCommand := fmt.Sprintf("convert_postgres_to_slave|%s|%s|5432|/var/lib/postgresql/15/main|rollback_%d|%s",
		newMasterHost, newMasterIP, time.Now().Unix(), targetHost)

	logger.Info().
		Str("target_host", targetHost).
		Str("command", convertCommand).
		Msg("Sending coordination rollback command")

	// Query gÃ¶nderme sistemi ile komutu target node'a gÃ¶nder
	queryID := fmt.Sprintf("coordination_rollback_%d", time.Now().Unix())

	// Timeout context oluÅŸtur
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()

	// Target node'a komutu gÃ¶nder
	result, err := s.SendQuery(ctx, targetHost, queryID, convertCommand, "")
	if err != nil {
		return fmt.Errorf("failed to send rollback command to %s: %v", targetHost, err)
	}

	// Sonucu kontrol et - result.Result *anypb.Any tipinde olduÄŸu iÃ§in string'e Ã§evirmek gerekiyor
	var resultStr string
	if result.Result != nil {
		// Any tipini string'e Ã§evir
		var structValue structpb.Struct
		if err := result.Result.UnmarshalTo(&structValue); err == nil {
			if resultMap := structValue.AsMap(); resultMap != nil {
				if status, ok := resultMap["status"].(string); ok && status != "success" {
					return fmt.Errorf("rollback command failed on %s: status=%s", targetHost, status)
				}
				if resultBytes, err := json.Marshal(resultMap); err == nil {
					resultStr = string(resultBytes)
				}
			}
		}
	}

	logger.Info().
		Str("target_host", targetHost).
		Str("result", resultStr).
		Msg("Coordination rollback command completed successfully")

	return nil
}

// sendSplitBrainAlert split-brain durumu iÃ§in alarm gÃ¶nderir
func (s *Server) sendSplitBrainAlert(targetHost, originalMasterHost string, err error) {
	logger.Error().
		Str("target_host", targetHost).
		Str("original_master_host", originalMasterHost).
		Err(err).
		Msg("CRITICAL: Split-brain situation detected!")

	logger.Error().
		Str("target_node", targetHost+" (still PRIMARY)").
		Str("original_master", originalMasterHost+" (reverted to MASTER)").
		Str("rollback_error", err.Error()).
		Msg("MANUAL INTERVENTION REQUIRED!")

	// Koordinasyon rollback event'ini kaydet
	s.logCoordinationRollback(targetHost, originalMasterHost, "revert_to_standby", "failed")

	// Burada alarm sisteminize gÃ¶re kritik alarm gÃ¶nderebilirsiniz
	// Ã–rnek: Slack, email, monitoring system, vs.
}

// logCoordinationRollback koordinasyon rollback iÃ§in Ã¶zel logging
func (s *Server) logCoordinationRollback(targetHost, originalHost, action, status string) {
	logger.Info().
		Str("event_type", "coordination_rollback").
		Str("target_host", targetHost).
		Str("original_host", originalHost).
		Str("action", action).
		Str("status", status).
		Time("timestamp", time.Now()).
		Msg("Coordination rollback event logged")

	// Database'e kaydet veya monitoring sistemine gÃ¶nder
	// Bu kÄ±sÄ±m isteÄŸe baÄŸlÄ± olarak geniÅŸletilebilir
}

// ExecutePatroniCommand, Patroni komutunu Ã§alÄ±ÅŸtÄ±rÄ±r
func (s *Server) ExecutePatroniCommand(ctx context.Context, req *pb.PatroniCommandRequest) (*pb.PatroniCommandResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Str("command", req.Command).
		Str("job_id", req.JobId).
		Msg("ExecutePatroniCommand Ã§aÄŸrÄ±ldÄ±")

	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", req.AgentId)
	}

	// Unique bir sorgu ID'si oluÅŸtur
	queryID := fmt.Sprintf("patroni_cmd_%d", time.Now().UnixNano())

	// Patroni komutunu oluÅŸtur
	command := fmt.Sprintf("PATRONI_CMD|%s", req.Command)
	if len(req.Args) > 0 {
		for _, arg := range req.Args {
			command = fmt.Sprintf("%s|%s", command, arg)
		}
	}

	logger.Debug().
		Str("command", command).
		Str("agent_id", req.AgentId).
		Str("query_id", queryID).
		Msg("Patroni komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	})
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni komutu gÃ¶nderilemedi")
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", req.AgentId).
		Str("query_id", queryID).
		Msg("Patroni komutu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		if result == nil {
			logger.Error().
				Str("agent_id", req.AgentId).
				Str("query_id", queryID).
				Msg("BoÅŸ Patroni komut yanÄ±tÄ± alÄ±ndÄ±")
			return nil, fmt.Errorf("boÅŸ komut yanÄ±tÄ± alÄ±ndÄ±")
		}

		logger.Info().
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni komut yanÄ±tÄ± alÄ±ndÄ±")

		// Result'Ä± parse et
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Error().
				Err(err).
				Str("agent_id", req.AgentId).
				Str("query_id", queryID).
				Msg("Patroni komut yanÄ±tÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±")
			return nil, fmt.Errorf("komut yanÄ±tÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: %v", err)
		}

		// JSON string'e Ã§evir
		resultJSON, _ := json.Marshal(resultStruct.AsMap())

		return &pb.PatroniCommandResponse{
			JobId:      req.JobId,
			Status:     pb.JobStatus_JOB_STATUS_COMPLETED,
			Command:    req.Command,
			Args:       req.Args,
			Output:     string(resultJSON),
			ExecutedAt: time.Now().Unix(),
			Duration:   0, // Agent tarafÄ±ndan hesaplanacak
		}, nil

	case <-ctx.Done():
		logger.Error().
			Err(ctx.Err()).
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni komut yanÄ±tÄ± beklerken timeout")
		return nil, ctx.Err()
	}
}

// GetPatroniClusterStatus, Patroni cluster durumunu getirir
func (s *Server) GetPatroniClusterStatus(ctx context.Context, req *pb.PatroniStatusRequest) (*pb.PatroniStatusResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Msg("GetPatroniClusterStatus Ã§aÄŸrÄ±ldÄ±")

	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", req.AgentId)
	}

	// Unique bir sorgu ID'si oluÅŸtur
	queryID := fmt.Sprintf("patroni_status_%d", time.Now().UnixNano())

	// Patroni status komutunu oluÅŸtur
	command := "PATRONI_STATUS"

	logger.Debug().
		Str("command", command).
		Str("agent_id", req.AgentId).
		Str("query_id", queryID).
		Msg("Patroni status komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	})
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni status sorgusu gÃ¶nderilemedi")
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", req.AgentId).
		Str("query_id", queryID).
		Msg("Patroni status sorgusu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		if result == nil {
			logger.Error().
				Str("agent_id", req.AgentId).
				Str("query_id", queryID).
				Msg("BoÅŸ Patroni status yanÄ±tÄ± alÄ±ndÄ±")
			return nil, fmt.Errorf("boÅŸ status yanÄ±tÄ± alÄ±ndÄ±")
		}

		logger.Info().
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni status yanÄ±tÄ± alÄ±ndÄ±")

		// Result'Ä± parse et
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Error().
				Err(err).
				Str("agent_id", req.AgentId).
				Str("query_id", queryID).
				Msg("Patroni status yanÄ±tÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±")
			return nil, fmt.Errorf("status yanÄ±tÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: %v", err)
		}

		// JSON string'e Ã§evir
		resultJSON, _ := json.Marshal(resultStruct.AsMap())

		return &pb.PatroniStatusResponse{
			Status:      "success",
			ClusterName: "unknown", // Agent'dan gelecek
			NodeRole:    "unknown", // Agent'dan gelecek
			NodeState:   "unknown", // Agent'dan gelecek
			Output:      string(resultJSON),
			LastUpdated: time.Now().Unix(),
		}, nil

	case <-ctx.Done():
		logger.Error().
			Err(ctx.Err()).
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni status yanÄ±tÄ± beklerken timeout")
		return nil, ctx.Err()
	}
}

// GetPatroniAvailableCommands, kullanÄ±labilir Patroni komutlarÄ±nÄ± getirir
func (s *Server) GetPatroniAvailableCommands(ctx context.Context, req *pb.PatroniCommandsRequest) (*pb.PatroniCommandsResponse, error) {
	logger.Info().
		Str("agent_id", req.AgentId).
		Msg("GetPatroniAvailableCommands Ã§aÄŸrÄ±ldÄ±")

	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[req.AgentId]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", req.AgentId)
	}

	// Unique bir sorgu ID'si oluÅŸtur
	queryID := fmt.Sprintf("patroni_commands_%d", time.Now().UnixNano())

	// Patroni commands komutunu oluÅŸtur
	command := "PATRONI_COMMANDS"

	logger.Debug().
		Str("command", command).
		Str("agent_id", req.AgentId).
		Str("query_id", queryID).
		Msg("Patroni commands komut")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	})
	if err != nil {
		logger.Error().
			Err(err).
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni commands sorgusu gÃ¶nderilemedi")
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", req.AgentId).
		Str("query_id", queryID).
		Msg("Patroni commands sorgusu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		if result == nil {
			logger.Error().
				Str("agent_id", req.AgentId).
				Str("query_id", queryID).
				Msg("BoÅŸ Patroni commands yanÄ±tÄ± alÄ±ndÄ±")
			return nil, fmt.Errorf("boÅŸ commands yanÄ±tÄ± alÄ±ndÄ±")
		}

		logger.Info().
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni commands yanÄ±tÄ± alÄ±ndÄ±")

		// Result'Ä± parse et
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Error().
				Err(err).
				Str("agent_id", req.AgentId).
				Str("query_id", queryID).
				Msg("Patroni commands yanÄ±tÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±")
			return nil, fmt.Errorf("commands yanÄ±tÄ± ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: %v", err)
		}

		// VarsayÄ±lan komutlar listesi (agent'dan gelecek gerÃ§ek komutlar ile deÄŸiÅŸtirilecek)
		commands := []*pb.PatroniCommand{
			{
				Name:         "list",
				Command:      "patronictl list",
				Args:         []string{},
				Description:  "List all cluster members",
				Category:     "cluster",
				RequiresAuth: false,
			},
			{
				Name:         "switchover",
				Command:      "patronictl switchover",
				Args:         []string{"--cluster", "--master", "--candidate"},
				Description:  "Perform a switchover",
				Category:     "failover",
				RequiresAuth: true,
			},
			{
				Name:         "failover",
				Command:      "patronictl failover",
				Args:         []string{"--cluster", "--master", "--candidate"},
				Description:  "Perform a failover",
				Category:     "failover",
				RequiresAuth: true,
			},
			{
				Name:         "reinit",
				Command:      "patronictl reinit",
				Args:         []string{"--cluster", "--member"},
				Description:  "Reinitialize a cluster member",
				Category:     "maintenance",
				RequiresAuth: true,
			},
		}

		return &pb.PatroniCommandsResponse{
			Status:   "success",
			Commands: commands,
		}, nil

	case <-ctx.Done():
		logger.Error().
			Err(ctx.Err()).
			Str("agent_id", req.AgentId).
			Str("query_id", queryID).
			Msg("Patroni commands yanÄ±tÄ± beklerken timeout")
		return nil, ctx.Err()
	}
}

// ExecuteMongoCommand executes a generic MongoDB command on the agent
func (s *Server) ExecuteMongoCommand(ctx context.Context, agentID, commandName string, params map[string]interface{}) (map[string]interface{}, error) {
	// Agent'Ä±n baÄŸlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
	s.mu.RLock()
	agent, agentExists := s.agents[agentID]
	s.mu.RUnlock()

	if !agentExists || agent == nil {
		return nil, fmt.Errorf("agent bulunamadÄ± veya baÄŸlantÄ± kapalÄ±: %s", agentID)
	}

	// Command string oluÅŸtur
	command := commandName
	if params != nil {
		if paramsJSON, err := json.Marshal(params); err == nil {
			command = fmt.Sprintf("%s|%s", commandName, string(paramsJSON))
		}
	}
	
	queryID := fmt.Sprintf("mongo_%s_%d", commandName, time.Now().UnixNano())

	logger.Debug().
		Str("command", command).
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Interface("params", params).
		Msg("MongoDB komutu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor")

	// SonuÃ§ kanalÄ± oluÅŸtur
	resultChan := make(chan *pb.QueryResult, 1)
	s.queryMu.Lock()
	s.queryResult[queryID] = &QueryResponse{
		ResultChan: resultChan,
	}
	s.queryMu.Unlock()

	// Context'in iptal durumunda kaynaklarÄ± temizle
	defer func() {
		s.queryMu.Lock()
		delete(s.queryResult, queryID)
		s.queryMu.Unlock()
		close(resultChan)
	}()

	// Sorguyu gÃ¶nder
	if err := agent.Stream.Send(&pb.ServerMessage{
		Payload: &pb.ServerMessage_Query{
			Query: &pb.Query{
				QueryId: queryID,
				Command: command,
			},
		},
	}); err != nil {
		return nil, fmt.Errorf("sorgu gÃ¶nderilemedi: %v", err)
	}

	logger.Info().
		Str("agent_id", agentID).
		Str("query_id", queryID).
		Str("command_name", commandName).
		Msg("MongoDB komutu iÃ§in sorgu gÃ¶nderildi")

	// CevabÄ± bekle
	select {
	case result := <-resultChan:
		// SonuÃ§ geldi
		if result == nil {
			return nil, fmt.Errorf("null sorgu sonucu alÄ±ndÄ±")
		}

		logger.Debug().
			Str("query_id", queryID).
			Str("type_url", result.Result.TypeUrl).
			Msg("Agent'tan yanÄ±t alÄ±ndÄ±")

		// Struct olarak ayrÄ±ÅŸtÄ±r
		var resultStruct structpb.Struct
		if err := result.Result.UnmarshalTo(&resultStruct); err != nil {
			logger.Error().
				Err(err).
				Str("query_id", queryID).
				Msg("Struct ayrÄ±ÅŸtÄ±rma hatasÄ±")
			return nil, fmt.Errorf("sonuÃ§ ayrÄ±ÅŸtÄ±rma hatasÄ±: %v", err)
		}

		// Debug: TÃ¼m field'larÄ± log'la
		logger.Debug().
			Str("query_id", queryID).
			Interface("fields", resultStruct.Fields).
			Msg("Agent'tan gelen response fields")

		// Status kontrolÃ¼
		statusValue, ok := resultStruct.Fields["status"]
		if !ok {
			logger.Error().
				Str("query_id", queryID).
				Msg("Status field bulunamadÄ±")
			return nil, fmt.Errorf("agent hatasÄ±: Status field yok")
		}
		
		statusStr := statusValue.GetStringValue()
		logger.Debug().
			Str("query_id", queryID).
			Str("status", statusStr).
			Msg("Agent status")
			
		if statusStr != "success" {
			errorMsg := "Bilinmeyen hata"
			if messageValue, exists := resultStruct.Fields["message"]; exists {
				errorMsg = messageValue.GetStringValue()
			}
			logger.Error().
				Str("query_id", queryID).
				Str("status", statusStr).
				Str("error", errorMsg).
				Msg("Agent baÅŸarÄ±sÄ±z status dÃ¶ndÃ¼rdÃ¼")
			return nil, fmt.Errorf("agent hatasÄ±: %s", errorMsg)
		}

		// Result field'Ä±ndan data'yÄ± Ã§Ä±kar
		var resultData map[string]interface{}
		if resultValue, exists := resultStruct.Fields["result"]; exists {
			if resultValue.GetStructValue() != nil {
				resultData = make(map[string]interface{})
				for k, v := range resultValue.GetStructValue().Fields {
					resultData[k] = v.AsInterface()
				}
			}
		}

		logger.Info().
			Str("query_id", queryID).
			Str("command_name", commandName).
			Msg("MongoDB komutu baÅŸarÄ±yla tamamlandÄ±")
		
		return resultData, nil

	case <-ctx.Done():
		logger.Error().
			Err(ctx.Err()).
			Str("agent_id", agentID).
			Str("query_id", queryID).
			Str("command_name", commandName).
			Msg("MongoDB komutu yanÄ±tÄ± beklerken timeout")
		return nil, ctx.Err()
	}
}
